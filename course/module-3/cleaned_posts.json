[{"instruction": 0, "content": "If anyone told you that ML or MLOps is easy, they were right.\nHere is a simple trick that I learned the hard way  \nIf you are in this domain, you already know that everything changes fast:\n- a new tool every month\n- a new model every week\n- a new project every day\nYou know what I did? I stopped caring about all these changes and switched my attention to the real gold.\nWhich is   \"Focus on the fundamentals.\"\n.\nLet me explain  \nWhen you constantly chase the latest models (aka FOMO), you will only have a shallow understanding of that new information (except if you are a genius or already deep into that niche).\nBut the joke's on you. In reality, most of what you think you need to know, you don't.\nSo you won't use what you learned and forget most of it after 1-2 months.\nWhat a waste of time, right?\n.\nBut...\nIf you master the fundamentals of the topic, you want to learn.\nFor example, for deep learning, you have to know:\n- how models are built\n- how they are trained\n- groundbreaking architectures (Resnet, UNet, Transformers, etc.)\n- parallel training\n- deploying a model, etc.\n...when in need (e.g., you just moved on to a new project), you can easily pick up the latest research.\nThus, after you have laid the foundation, it is straightforward to learn SoTA approaches when needed (if needed).\nMost importantly, what you learn will stick with you, and you will have the flexibility to jump from one project to another quickly.\n.\nI am also guilty. I used to FOMO into all kinds of topics until I was honest with myself and admitted I am no Leonardo Da Vinci.\nBut here is what I did and worked well:\n- building projects\n- replicating the implementations of famous papers\n- teaching the subject I want to learn\n... and most importantly, take my time to relax and internalize the information.\n.\nTo conclude:\n- learn ahead only the fundamentals\n- learn the latest trend only when needed\nWhat is your learning strategy? Let me know in the comments  \nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\n  Follow me for daily lessons about ML engineering and MLOps."}, {"instruction": "Create an LLM Twin that mimics your style and voice by enrolling in the 'LLM Twin' course. Learn to build your own AI writing assistant from data collection to deployment, and leverage MLOps best practices for experiment tracking, model management, prompt monitoring, and versioning.", "content": " Did you know that you can build your own LLM Twin that sounds just like you?\n  At a relatively recent sushi lunch, Paul Iusztin and I discussed the challenges of finding open-source resources for deploying large language models (LLMs) in production environments.\nI had a big AHA moment and I thought:\nWait, how awesome would it be if we were able to create an LLM Twin that would act like an AI writing assistant that can replicate your personality and tone of voice? That would be super helpful for the entire community of engineers that actively write specialized articles. \n  This conversation sparked the    :   -  .\n  \"LLM Twin\" course, sponsored by  , gives you the practical skills to build your own AI writing assistant that mimics your style and voice. \nThe best part of this course? You will learn how to architect and build a real-world LLM system from start to finish  from data collection to deployment.\nYou will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.\nHeres what this -  will teach you:\n   Data Collection Pipeline (AWS): Gather & clean your social media data using ETL pipelines.\n Feature Pipeline: Process data in real-time with Bytewax & Superlinked, then store it in a Qdrant vector DB.\n Training Pipeline: Train your LLM with QLoRA & Comet ML's experiment tracking, saving the best model.\n  Inference Pipeline: Deploy your fine-tuned LLM as a REST API & enhance prompts with RAG for creative content generation. Monitor performance with Comet ML.\n This course is designed for:\n- ML Engineers\n-Data Engineers\n-Data Scientists\n-Software Engineers\nProvided to you by the Decoding ML umbrella:\nPaul Iusztin\n| Senior ML & MLOps Engineer\nAlex Vesa\n| Senior AI Engineer\nAlexandru Razvant\n| Senior ML & MLOps Engineer\nWe couldnt create this course without the help and support of:\n \nBytewax\n \nQwak\n \nSuperlinked\n \nComet\n \nQdrant\nWith that being said, I hope you are all on board to join us on our Medium & Substack channels and stay tuned for a 11-lesson practical experience!\nWhos UP for this challenge?  [URL]"}, {"instruction": "Join the FREE course 'LLM Twin: Building Your Production-Ready AI Replica' by Decoding ML team to learn how to architect, train, and deploy production-ready LLM systems using LLMOps good practices and tools such as experiment trackers, model registries, and more. The course is designed for MLE, DE, DS, or SWE with intermediate level skills in Python, ML, and cloud. Access 11 hands-on written lessons and open-source code on GitHub for free.", "content": "Want to learn an end-to-end framework for production-ready LLM systems by building your LLM twin?\nThen you are in luck.\n \nThe Decoding ML team and I will release a FREE course called the LLM Twin: Building Your Production-Ready AI Replica in a few days.\nWithin the course, you will learn how to:\n- architect\n- train\n- deploy\n...a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices, such as:\n- experiment trackers\n- model registries\n- prompt monitoring\n- versioning\n- deploying LLMs\n...and more!\n.\nIt is an end-to-end LLM course where you will build a real-world LLM system:\n  from start to finish\n  from data collection to deployment\n.\nWhat is an LLM Twin? It is an AI character that learns to write like somebody by incorporating its style and personality into an LLM.\n.\nWho is thisfor?\nAudience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.\nLevel: intermediate\nPrerequisites: basic knowledge of Python, ML, and the cloud\nHow will youlearn?\nThe course contains 11 hands-on written lessons and the open-source code you can access on GitHub.\nYou can read everything at your own pace.\nCosts?\nThe articles and code are completely free. They will always remain free.\nThis time, the Medium articles won't be under any paid wall. I want to make them entirely available to everyone.\nMeet your teachers!\nThe course is created under the Decoding ML umbrella by:\nPaul Iusztin\n| Senior ML & MLOps Engineer\nAlex Vesa\n| Senior AI Engineer\nAlexandru Razvant\n| Senior ML & MLOps Engineer\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n         \n \n  LLM Twin: Building Your Production-Ready AI Replica:\n[URL]"}, {"instruction": "Create a post announcing SSIMA 2022: Medical Image Computing and Image-Assisted Robotic Surgery event to be held in Oradea City between 5th and 9th of September this year, with details on the participants, activities, and focus areas.", "content": "SSIMA Re:Imagine Healthcare & Oradea City Hall proudly announce SSIMA 2022: Medical Image Computing and Image-Assisted Robotic Surgery.\nIn close collaboration with our host, the City of Oradea, the SSIMA Organization Committee is inviting medical and engineering students, postgraduates and professionals to join us between the 5th and 9th of September this year at SSIMA 2022.\nIt will be an event hosted for the first time in the beautiful riverside City of Oradea, with its grand Art Nouveau architecture, parks and quaint town squares.\nThe perfect location for an academic event unparalleled in Eastern Europe, where top academics from the most renowned institutions from the USA, Israel, the Netherlands, UK and many more gather to share and discuss their latest discoveries during a weeklong schedule full of lectures, roundtable discussions, social events and practical hands-on sessions.\nThis years edition will focus on Medical Image Computing and Image-Assisted Robotic Surgery. However, there will also be enough room for discussion and interaction with our invited speakers on other topics related to the state-of-the-art on related methods and clinical applications, as well as on fundamental academic skills in scientific writing and presentations, and grant writing tips.\nThe SSIMA Organization Committee and the City of Oradea look forward to welcoming you on 5th of September and will be opening the application process soon at\n[URL]\n.\nSincerely,\nSSIMA Organization Committee\nProf.\nAlfred M. (Freddy) Bruckstein\n(\nTechnion - Israel Institute of Technology\n)\nProf.\nAlejandro F Frangi\n(\nUniversity of Leeds\n|\nKU Leuven\n)\nGeorge Haber\n(Crestafund Investments)\nDr. Elena Ovreiu (\nUniversity POLITEHNICA of Bucharest\n)\nProf.\nBart ter Haar Romeny\n(\nEindhoven University of Technology\n)\nProf.\nAlon Wolf\n(VP of\nTechnion - Israel Institute of Technology\n)\nOradea City Hall\nMayor of Oradea  Florin Birta\nCity Manager  Mihai Jurca"}, {"instruction": "Write a step-by-step tutorial on how to create a streaming retrieval system for RAG on social media data with a streaming ingestion pipeline and a retrieval client based on Python.", "content": "Batch systems are the past. Here is how to write a streaming retrieval system for RAG on social media data  \nWhy streaming over batch?\nIn environments where data evolves quickly (e.g., social media platforms), the system's response time is critical for your application's user experience.\nThat is why TikTok is so addicting. Its recommender system adapts in real-time based on your interaction with the app.\nHow would it be if the recommendations were updated daily or hourly?\nWell, it would work, but you would probably get bored of the app much faster.\nThe same applies to RAG for highly intensive data sources...\n  where you must sync your source and vector DB in real time for up-to-date retrievals.\n.\nI wrote a step-by-step tutorial on VectorHub by\nSuperlinked\n(the best place to learn about VectorDBs & RAG).\n    .\n \nThe retrieval system is based on 2 detached components:\n- the streaming ingestion pipeline\n- the retrieval client\nThe streaming ingestion pipeline runs 24/7 to keep the vector DB synced with the current raw LinkedIn posts data source.\nThe retrieval client is used in RAG applications to query the vector DB.\n  These 2 components are completely decoupled and communicate with each other through the vector DB.\n#1. The streaming ingestion pipeline\n  Implemented in\nBytewax\n- a streaming engine built in Rust (speed& reliability) that exposes a Python interface\nMain flow:\n- uses CDC to add changes from the source DB to a queue\n- listens to the queue for new events\n- cleans, chunks, and embeds the LI posts\n- loads them to a Qdrant vector DB\nand... everything in real-time!\n#2. The retrieval client\n  A standard Python module.\nThe goal is to retrieve similar posts using a variety of query types - e.g., posts, questions, sentences.\nMain flow:\n- preprocess user queries (the same way as they were ingested)\n- search the Qdrant vector DB for the most similar results\n- use rerank to improve the retrieval system's accuracy\n- visualize the results on a 2D plot using UMAP\n.\nWant to see the code? \nCheck it out on VectorHub...\n...along a step-by-step tutorial on how to build a full-fledged streaming pipeline using only Python.\n \n  A Real-time Retrieval System for Social Media Data:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Write a post highlighting the advancements in general purpose robot helpers and the potential future where having a robot helper in your house is normal.", "content": "The robots are coming.  \nIf youre not yet convinced that general purpose robot helpers will be normal in the not too distant future, I highly recommend this short demo.\nIts called Astribot, and it was just released by a Chinese research company. A lot is still unknown about their work, but the video really brings to life the incredible potential as robotic assistants come closer to the tipping points required to take off.\nThe race to develop general purpose robots has been heating up for years, but still with major limitations on speed, dexterity, and complexity of tasks.\n but this is changing.\nThe continued convergence of advanced robotics, computer vision, and AIs with multi-model understanding will undoubtedly unlock a new global industry.\nNot if. When.\nAmazon already has close to a million robotic unit deployed in their warehouses (up from zero just 12 years ago!), and a simple use case like robotic lawn mowers is already a multi billion-dollar industry on its own.\nImagine when the industry unlocks true general purpose at a reasonable price point.\nHow quickly will it then become normal to have a robot helper in your house?\nIts still early days, with many hurdles still to overcome.\n but theyre coming.\nFascinating future ahead.[URL]"}, {"instruction": "Celebrate the team's achievements with a proud shoutout to each member by name!", "content": "Proud of our team!\nAndreea M.\n,\nFabian Jichi\n,\nRares Istoc\n,\nAndreea Gabriela Oros\n,\nNickolas Filip\n,\nCristian Predescu\nyou are awesome ."}, {"instruction": "Integrate a pre-trained 'all-MiniLM-L6-v2' encoder-only model to embed the cleaned user input prompt, then query the Qdrant vector DB to extract the top 3 most similar financial news based on cosine similarity distance.", "content": " can you  &  an   for a real-time financial advisor with  powered by  &   while considering   ?\n.\nAs a quick reminder from previous posts, here is what we already have:\n- a\nQdrant\nvector DB populated with financial news (the output of the feature pipeline)\n- fine-tuned Falcon-7B LoRA weights stored in Comet's model registry (the output of the training pipeline)\nThe\nQdrant\nvectorDB is accessed through a Python client.\nA specific version of the Falcon-7B LoRA weights is downloaded from Comet's model registry and loaded in memory using QLoRA.\nThe goal of the inference pipeline is to use LangChain to glue the 2 components into a single `FinancialAssistant` entity.\n.\nThe `FinancialAssistant` entity is deployed in a request-response fashion under a RESTful API. We used Beam to deploy it quickly under a serverless web endpoint.\nTo deploy any model using Beam as a RESTful API is as easy as writing the following Python decorator:\n```\n@financial_bot. rest_api(keep_warm_seconds=300, loader=load_bot)\ndef run(**inputs):\n....\n```\n  Beam:\n[URL]\n '      ``  \n1. Clean the user's input prompt and use a pre-trained \"all-MiniLM-L6-v2\" encoder-only model to embed it (the same LM used to populate the vector DB).\n2. Using the embedded user input, query the Qdrant vector DB and extract the top 3 most similar financial news based on the cosine similarly distance\nThese 2 steps were necessary to do RAG.\n3. Build the final prompt using a \"PromptTemplate\" class (the same one used for training) that formats the following components:\n- a system prompt\n- the user's input prompt\n- the financial news context\n- the chat history\n4. Now that our prompt contains all the necessary data, we pass it to the fine-tuned Falcon-7B LLM for the final answer.\nThe input prompt and LLM answer will be logged and monitored by Comet LLMOps.\n5. You can get the answer in one shot or use the `TextIteratorStreamer` class (from HuggingFace) to stream it token-by-token.\n6. Store the user's input prompt and LLM answer in the chat history.\n7. Pass the final answer to the client.\nNote: You can use the `TextIteratorStreamer` class & wrap the `FinancialAssistant` under a WebSocket (instead of the RESTful API) to stream the answer of the bot token by token.\nSimilar to ChatGPT.\n  To see the full code, check out our \"Hands-on LLMs\" FREE course & support us with a :  \n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Discuss strategies for ensuring quality summaries when evaluating the output of a Private LLM Summary Task.", "content": "In today's tech-driven world, summarizing documents has become deceptively simple.\nJust a call to OpenAI's API, and voil  you have a summary. But what about when things get complex?\n  The Real Challenge Begins in Production-Ready Environments\nConsider these scenarios:\n1. No Access to OpenAI API: What if you're restricted from using it?\n2. Alternative LLMs: How do you adapt to private LLMs like Llama2-7b or Mixtral-7b?\n3. Context Window Limitations: What if your document exceeds the LLM's context window?\nThese are valid concerns, but let's talk about a crucial aspect:\n Evaluating the Output of a Private LLM Summary Task\nA common pitfall in LLM-based summarization is the tendency to \"hallucinate\"  introducing non-existent information. So, how do we mitigate this?\nStrategies for Ensuring Quality Summaries:\n1. Prompt Techniques: Fine-tuning prompts to align with desired outputs.\n2. Summary Metrics: Metrics to evaluate summary quality.\n3. LLM Evaluator: A larger LLM, Llama2-13B or Mixtral-13B instructed to evaluate the summaries based on Accuracy, Relevance, and Adherence.\n4. Human Feedback: Integrating human judgment for final validation.\n   In my professional journey, I've developed a composed summary metric focusing on three key aspects: Accuracy, Relevance, and Adherence.\n  Accuracy\nScore 0: We avoid summaries with significant inaccuracies or misleading information.\nScore 1: We aim to improve summaries that are not fully accurate for reliable information extraction.\nScore 2: Our goal is to achieve highly accurate summaries that capture the main points with only minor inaccuracies that don't impede understanding.\n  Relevancy\nScore 0: We discard summaries that are completely irrelevant and lack meaningful information.\nScore 1: We refine somewhat relevant summaries but miss some main themes of the document.\nScore 2: We strive for highly relevant summaries, encapsulating all main themes with precision.\n  Adherence\nScore 0: We reject summaries that completely disregard the document's structure and content.\nScore 1: We work on summaries with major adherence issues to reach a coherent structure.\nScore 2: We aspire to create summaries that perfectly adhere to the document's structure, mirroring its logical flow with precision.\nLLM evaluator and Human Feedback based on these summary metrics play a crucial role in assuring the quality of LLM summary tasks.\nFrom your experience, how do you evaluate the summary task of LLMs?[URL]"}, {"instruction": "Announce the release of 8 Medium lessons for the Hands-on LLMs course covering real-time streaming pipeline, fine-tuning pipeline, inference pipeline, and integration of serverless tools. Targeted at MLE, DE, DS, or SWE interested in learning about LLM systems using LLMOps principles. Encourage checking out the series for hands-on video lessons and open-source code on GitHub.", "content": "Want to build your first LLM project but don't know where to start?   If you want to learn in a structured way to build LLM systems using good LLMOps principles...\nWe want to announce that we just released 8 Medium lessons for the Hands-on LLMs course that will put you on the right track  \n.\nWithin the 8 Medium lessons, you will go step-by-step through the theory, system design, and code to learn how to build a:\n  real-time streaming pipeline (deployed on AWS) that uses Bytewax as the stream engine to listen to financial news, cleans & embeds the documents, and loads them to a vector DB\n  fine-tuning pipeline (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\n  inference pipeline built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news)\nWe will also show you how to integrate various serverless tools, such as:\n Comet ML as your ML Platform;\n Qdrant as your vector DB;\n Beam as your infrastructure.\n.\nWho is this for?\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM systems using LLMOps good principles.\nHow will you learn?\nThe series contains 4 hands-on video lessons and the open-source code you can access on GitHub.\nCurious?\nCheck out the 8 Medium lessons of the Hands-on LLMs course and start building your own LLMs system:\n  The Hands-on LLMs Medium Series:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps.[URL]"}, {"instruction": "Create a comprehensive documentation for the AI model, including a model card that describes the data used for training and also a data card to provide further insights into the data being utilized.", "content": "It was also about time to add a data card along with the model card that explains the data your model was trained on.\nSo excited to work at a company that innovates in the field of safety AI.\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelligence[URL]"}, {"instruction": "Explain the synergy between human life and technology and how Artificial Intelligence can make lives more human in the medical spectrum. Also, highlight NTT DATA Romania's vision for the Romania eAwards and express gratitude towards the team, medical body, and participants at MedTech events and competitions. Utilize hashtags #technology, #artificialintelligence, #medtech, #startup, and #radiotherapy in the post.", "content": " What is the synergy between human life and technology, and how can we use Artificial Intelligence to make our lives more human, in the medical spectrum?\nWe are glad to present our vision at\nNTT DATA Romania\nfor the Romania eAwards, and thank everybody who embraced it!\nOur participation at MedTech events and competitions shows how resilient we are in our development process. We are grateful for our team and medical body, who trust our work and perseverant state of mind!\nhashtag\n#\ntechnology\nhashtag\n#\nartificialintelligence\nhashtag\n#\nmedtech\nhashtag\n#\nstartup\nhashtag\n#\nradiotherapy"}, {"instruction": "Create a social media post providing an update on the ongoing crowdfunding campaign for your MedTech startup, highlighting the opportunity to invest a minimum of $100 to join the priority list. Include relevant hashtags and links to learn more about AIMINDED.", "content": "We are off to a great start! Our first investment is here!\n Now you have the chance to be on the PRIORITY LIST, in the\nhashtag\n#\ncrowdfunding\ncampaign that we started with\nRnin\n!\nAt the end of the week, the campaign will be LIVE and anyone will be able to invest in our startup, with the amount of min. 100!\n If you are interested in being part of the investors of a successful\nhashtag\n#\nMedTech\nbusiness, NOW IT'S THE RIGHT TIME! \n[URL]\n Learn more about AIMINDED:\n[URL]\nhashtag\n#\ncrowdfunding\nhashtag\n#\ncrowdfundingcampaign\nhashtag\n#\ninvestment\nhashtag\n#\ninvestmentopportunity\nhashtag\n#\ninvest\nhashtag\n#\ninvesting\nhashtag\n#\nstartup\nhashtag\n#\nartificialintelligence\nhashtag\n#\ntechnology[URL]"}, {"instruction": "Share your experience reducing the latency of PyTorch code by 82% using Python & PyTorch, without any fancy tools. Highlight the key steps taken and encourage others to suggest additional methods in the comments. Tag 'machinelearning', 'mlops', 'datascience', and include relevant URL hashtags.", "content": "This is how I reduced the latency of my PyTorch code by 82% using only Python & PyTorch. NO fancy tools involved!\n ?\nDuring inference, I am using 5 DL at ~25k images at once.\nThe script took around ~4 hours to run.\nThe problem is that this isn't a batch job that runs over the night...\nVarious people across the company required it to run in \"real-time\" multiple times a day.\n ?\nThe first thing that might come to your mind is to start using some fancy optimizer (e.g., TensorRT).\nEven though that should be done at some point...\nFirst, you should ask yourself:\n- I/O bottlenecks: reading & writing images\n- preprocessing & postprocessing - can it be parallelized?\n- are the CUDA cores used at their maximum potential?\n- is the bandwidth between the CPU & GPU throttled?\n- can we move more computation to the GPU?\nThat being said...\nHere is what I did I decreased the latency of the script by 82%\n \n1. Batched the inference samples\nBatching is not only valuable for training but also mighty in speeding up your inference time.\nOtherwise, you waste your GPU CUDA cores.\nInstead of passing through the models one sample at a time, I now process 64.\n2. Leveraged PyTorch's DataLoader\nThis has 2 main advantages:\n- parallel data loading & preprocessing on multiple processes (NOT threads)\n- copying your input images directly into the pinned memory (avoid a CPU -> CPU copy operation)\n3. Moved as much of the postprocessing on the GPU\nI saw that the tensor was moved too early on the CPU and mapped to a NumPy array.\nI refactored the code to keep it on the GPU as much as possible, which had 2 main advantages:\n- tensors are processed faster on the GPU\n- at the end of the logic, I had smaller tensors, resulting in smaller transfers between the CPU & GPU\n4. Multithreading for all my I/O write operations\nFor I/O bottlenecks, using Python threads is extremely powerful.\nI moved all my writes under a ThreadPoolExecutor, batching my write operations.\n.\nNote that I used only good old Python & PyTorch code.\n  When the code is poorly written, no tool can save you\nOnly now is the time to add fancy tooling, such as TensorRT.\n.\nSo remember...\nTo optimize the PyTorch code by 82%:\n1. Batched the inference samples\n2. Leveraged PyTorch's DataLoader\n3. Moved as much of the postprocessing on the GPU\n4. Multithreading for all my I/O write operations\nWhat other methods do you have in mind? Leave them in the comments  \nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Create a post sharing your personal journey and insights on entrepreneurship, emphasizing the importance of mindset, self-awareness, resilience, and embracing the entrepreneurial process. Highlight how entrepreneurship is more than just profit, but also about building communities and making a positive impact. Use relevant hashtags like #business, #startup, #ai, #leadership, and include a note of gratitude and a signature at the end of the post.", "content": "Ce nseamn antreprenoriatul pentru tine?\n Cu aceast ntrebare mi-am propus s provoc imaginaia i minile unor studeni extraordinari care au participat la concursul Startup Survivor. Am rmas plcut surprins de multitudinea de rspunsuri pe care le-am primit i m simt inspirat s mi adaug amprenta personal asupra acestei ntrebri.\n Suntem nvai s asociem antreprenoriatul cu capitalul, profitul i performana.\nAspectul care nu este predat n coli este ce mentalitate trebuie s ai pentru a ajunge antreprenor. Care este motivaia din spatele performanei tale?\nPentru mine, antreprenoriatul reprezint un stil de viat, un crez cu care te trezeti  cu care adormi. Nu cred c ne natem antreprenori i, n acelasi timp, nu cred ca antreprenoriatul se nva; nu exist limit de vrst sau educaie n aceast direcie.\nTrebuie ns s fii foarte prezent i conectat la ce se ntmpl n jurul tu. Astfel, dobndim maturitatea de a gsi soluii pentru probleme neexploatate.\nObtnd pentru o astfel de gndire, antreprenoriatul te pune n situaia inconfortabil de a te cunoate foarte bine pe tine nsui. Totul pornete de la cine eti tu.\n  Introspecia devine cea mai puternic arm pe care noi ca antreprenori o putem avea n procesul de gsire a unei soluii. Combinat cu puterea de a visa, introspecia ne dezvolt o trstur specific fiecrui antreprenor de succes: reziliena.\nPentru mine aceast cltorie se desfoar ca o bucl infinit, zi i noapte, n care cred orbete c pot gsi formula ideal pentru a avansa, dar asta este lumea mea interioar. Lumea exterioar m nva c nu e aa. Azi primeti un nu, maine primeti un alt nu, iar procesul continu.\nPe hrtie, destinaia final arat frumos, ns ca antreprenor consider c trebuie s te ndrgosteti de  acest proces, n care eti nevoit s lai pri din tine i s te deschizi, pentru a primi deschidere.\nAntreprenoriatul construiete comuniti i business-uri care pot crea un impact, att n lumea interioar a fiecrui om, dar i n exteriorul ce ne nconjoar. Totul devine un sistem mai puternic, n care valorile comune primeaz.\nSemnat, un antreprenor la nceput de drum.\nMultumesc\nAndra Cara\npentru inspiratie.\nhashtag\n#\nbusiness\nhashtag\n#\nstartup\nhashtag\n#\nai\nhashtag\n#\nleadership[URL]"}, {"instruction": "Ask your LinkedIn community for advice on choosing between Knowledge Graphs and Graph Databases by mentioning ML, Software, Data, and Engineers in a post and inviting those familiar to comment below.", "content": "Hey,\nhashtag\n#\nLinkedin\ncommunity. Your guy needs help!  \nI'm having a hard time choosing between Knowledge Graphs and Graph Databases.   \nIf there are any\nhashtag\n#\nML\nhashtag\n#\nSoftware\nhashtag\n#\nData\nhashtag\n#\nEngineers\nin my list that have worked with these concepts before, raise a hand in the comments below  , I'd be super appreciative if I could have a chat about my dilemma.\nThanks in advance    ![URL]"}, {"instruction": "Enroll in the FREE course 'LLM Twin: Building Your Production-Ready AI Replica' to learn how to build 4 Python microservices and integrate 3 serverless tools for production-ready LLM systems.", "content": "Want to learn to build production LLMs in a structured way? For FREE? Then you should take our NEW course on how to implement an end-to-end framework for production-ready LLM systems  \n Decoding ML and I are starting a new FREE course on learning how to architect and build a real-world LLM system by building an LLM Twin:\n  from start to finish-from\n  from data collection to deployment\n  production-ready\n  from NO MLOps to experiment trackers, model registries, prompt monitoring, and versioning\nThe course is called: LLM Twin: Building Your Production-Ready AI Replica\n...and here is what you will learn to build\n \n  4 Python microservices:\n  The data collection pipeline\n- Crawl your digital data from various social media platforms.\n- Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines.\n- Send database changes to a queue using the CDC pattern.\n Deployed on AWS.\n  The feature pipeline\n- Consume messages from a queue through a\nBytewax\nstreaming pipeline.\n- Every message will be cleaned, chunked, embedded (using\nSuperlinked\n), and loaded into a\nQdrant\nvector DB in real-time.\n Deployed on AWS.\n  The training pipeline\n- Create a custom dataset based on your digital data.\n- Fine-tune an LLM using QLoRA.\n- Use\nComet\nML's experiment tracker to monitor the experiments.\n- Evaluate and save the best model to Comet's model registry.\n Deployed on\nQwak\n.\n  The inference pipeline\n- Load and quantize the fine-tuned LLM from Comet's model registry.\n- Deploy it as a REST API\n- Enhance the prompts using RAG\n- Generate content using your LLM twin\n- Monitor the LLM using Comet's prompt monitoring dashboard\n Deployed on Qwak.\n.\nAlong the 4 microservices, you will learn to integrate 3 serverless tools:\n- Comet as your ML Platform\n- Qdrant as your vector DB\n- Qwak as your ML infrastructure\n.\nTo stay updated on LLM Twin: Building Your Production-Ready AI Replica course...\n         \n \n  LLM Twin: Building Your Production-Ready AI Replica:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Create a detailed guide on utilizing Hydra for organizing ML configuration files, including structuring the project, defining config files, using it in Python code, and leveraging its flexibility.", "content": "What is the best tool to organize and load your ML configuration files? Hydra      Here is a quick guide to get you started.\nThe problem?\nThe bigger the ML projects get, the harder it gets to keep them organized.\nThat's where the magic of proper configuration management comes in.\nConfiguration files are present in every stage of your ML pipelines:\n- data\n- features\n- training\n- inference\nManaging complex configurations is a must-have rather than a nice-to-have.\nFor complex projects, it is as critical as versioning your data or models.\nThe solution?\n  \n.\nAlexandru Razvant\ncompiled a quick guide for you on\nDecoding ML\nthat will get you started with Hydra on how to:\n- structure your project to use it\n- define your config files\n- use it in your Python code\n- leverage the flexibility of Hydra\nEverything you need to start trying it yourself in your projects  \n.\nTo find out more...\nCheck it out\n \n  Mastering ML Configurations by leveraging OmegaConf and Hydra:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "When evaluating OCR systems, ensure to use Language Model components like the LLM Evaluator and LLM Correct Text Extraction to enhance and correct the text extraction quality.", "content": " You dont need to check OCR system output quality.\nWhat a joke right?\nText Extraction is the core of any RAG application where you want to have the best experience in talking to your documents.\n  Ive seen several RAG applications where the text extraction quality is not checked.\n  When we talk about millions of documents per day, the text extraction quality becomes crucial\nIn my daily work, when I deal with OCR systems, I use 2 LLM components to evaluate and correct the text extraction:\n1 LLM Evaluator - this component is used to evaluate the text from deep-learning-based OCR systems like PyTesseract or EasyOCR\n2 LLM Correct Text Extraction - this component is used to correct the final text (grammatical error)\nHow these 2 components are integrated into an OCR system workflow?\nHeres my approach:\n  Starting with source documents\n  using Python PDF tools for initial extraction\n  conducting a readability check\n enhancing with OCR tools like PyTesseract/EasyOCR\n  reviewing OCR results and parameters\n employing a Language Model Evaluator for accuracy\n  iterating through a feedback loop for improvements\n fine-tuning OCR settings\n  double-checking with LLM post-OCR\n  producing a refined, accurate, and clear output, ready for application.\nIm curious - how are you ensuring accuracy in your text extraction processes?[URL]"}, {"instruction": "Guide readers on the importance of a well-engineered data pipeline for successful AI projects, mentioning various components like data collection process, ETL pipelines, ODM, NoSQL Database, and Feature Pipeline, along with the technologies used. Encourage them to stay tuned for the upcoming lesson on data pipelines in the LLMTwin course and provide links for further information and support.", "content": "            .  \n  Data is the lifeblood of any successful AI project, and a well-engineered data pipeline is the key to harnessing its power.\nYou may ask:  ,       ,      ?\nHear me out! A data pipeline is a series of automated steps that guide\nhashtag\n#\ndata\non a purpose.\nImagine this automated system acts as the engine, seamlessly moving data through various stages and transforming it from raw form into actionable insights.\n            ?\nIn  2 of the     by Decoding ML we are focusing on how to build an end to end data pipeline by building:\n  Data collection process -> Medium, Github, Substack & Linkedin crawlers\n  ETL pipelines -> data is cleaned and normalized\n  ODM (Object Document Mapping ) -> a technique that maps between an object model in an application and a document database\n  NoSQL Database (\nMongoDB\n) & CDC (Change Data Capture) pattern\n-> Tracks data changes log them, and queues messages for real-time system updates\n-> The clean data is stored in a NoSQL database\n  Feature Pipeline\n->A streaming ingestion pipeline is part of the feature pipeline that processes Articles, Posts, and Code.\n->Tools like\nBytewax\nand\nSuperlinked\nare used, likely for further data processing and transformation.\n->This processed data is then queued in\nRabbitMQ\n, a message broker that helps in asynchronous processing and communication between different services.\n  STAY TUNED, guys. On 23rd March, we unleash the second lesson of the\nhashtag\n#\nLLMTwin\ncourse by Decoding ML on\nMedium\n: \"The Importance of Data Pipelines in the Era of Generative AI\"!\nIf you want to learn more about the technologies used in this lesson check out:\n  :\n[URL]\n   :\n[URL]\n  :\n[URL]\n   :\n[URL]\n  Check out the code on GitHub and support us with a  ->\n[URL]"}, {"instruction": 20, "content": "To successfully use RAG in your LLM applications, your vector DB must constantly be updated with the latest data.\nHere is how you can implement a streaming pipeline to keep your vector DB in sync with your datasets  \n.\nRAG is a popular strategy when building LLMs to add context to your prompt about your private datasets.\nLeveraging your domain data using RAG provides 2 significant benefits:\n- you don't need to fine-tune your model as often (or at all)\n- avoid hallucinations\n.\nOn the bot side, to implement RAG, you have to:\n3. Embed the user's question using an embedding model (e.g., BERT). Use the embedding to query your vector DB and find the most similar vectors using a distance function (e.g., cos similarity).\n4. Get the top N closest vectors and their metadata.\n5. Attach the extracted top N vectors metadata + the chat history to the input prompt.\n6. Pass the prompt to the LLM.\n7. Insert the user question + assistant answer to the chat history.\n.\nBut the question is, how do you keep your vector DB up to date with the latest data?\n  You need a real-time streaming pipeline.\nHow do you implement it?\nYou need 2 components:\n  A streaming processing framework. For example, Bytewax is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need Java to implement real-time pipelines anymore.\n  Bytewax:\n[URL]\n  A vector DB. For example, Qdrant provides a rich set of features and a seamless experience.\n  Qdrant:\n[URL]\n.\nHere is an example of how to implement a streaming pipeline for financial news  \n#1. Financial news data source (e.g., Alpaca):\nTo populate your vector DB, you need a historical API (e.g., RESTful API) to add data to your vector DB in batch mode between a desired [start_date, end_date] range. You can tweak the number of workers to parallelize this step as much as possible.\n  You run this once in the beginning.\nYou need the data exposed under a web socket to ingest news in real-time. So, you'll be able to listen to the news and ingest it in your vector DB as soon as they are available.\n  Listens 24/7 for financial news.\n#2. Build the streaming pipeline using Bytewax:\nImplement 2 input connectors for the 2 different types of APIs: RESTful API & web socket.\nThe rest of the steps can be shared between both connectors  \n- Clean financial news documents.\n- Chunk the documents.\n- Embed the documents (e.g., using Bert).\n- Insert the embedded documents + their metadata to the vector DB (e.g., Qdrant).\n#3-7. When the users ask a financial question, you can leverage RAG with an up-to-date vector DB to search for the latest news in the industry.\nBytewax and Qdrant make this easy  \nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"}, {"instruction": "Encourage future young entrepreneurs to focus on resilience and passion as the fuel for long-term success.", "content": "          .\nThis was the advice I gave at a meetup organized by\n Andrei Cosmin Munteanu\nwithin the InnovationLabs incubator.\n The energy of young entrepreneurs is genuinely inspiring!\nThey're coming with ideas and a passion to make a difference.\n  However, during the discussion, I noticed a trend - many are drawn to the startup world because it's seen as \"sexy\" or the new \"cool.\"\nWhile chasing trends can be exciting, a strong foundation built on a clear \"why\" is crucial for long-term success.\n  Your \"why\" is the driving force that fuels your motivation, keeps you resilient through challenges and allows you to connect with your target audience truly.\nSo, for future young entrepreneurs,            .\nResilience and passion are the fuel that will keep you going when faced with challenges.\nPS: Honestly, these are also important reminders for me.  \nThanks,\n Andrei Cosmin Munteanu\n,\nRadu Ticiu\nand\nOvidiu Stegari\nfor helping the community by sharing your experiences.[URL]"}, {"instruction": "Discuss the implications of Jensen Huang's statement about the future of programming and the role of AI in closing the technology gap between programmers and non-programmers.", "content": "Computer science is dead. Do this instead.\nIn a recent talk, Jensen Huang, CEO of Nvidia, said that kids shouldn't learn programming anymore.\nHe said that until now, most of us thought that everyone should learn to program at some point.\nBut the actual opposite is the truth.\nWith the rise of AI, nobody should have or need to learn to program anymore.\nHe highlights that with AI tools, the technology divide between non-programmers and engineers is closing.\n.\nAs an engineer, my ego is hurt; my first reaction is to say it is stupid.\nBut after thinking about it more thoroughly, I tend to agree with him.\nAfter all, even now, almost anybody can work with AI.\nThis probably won't happen in the next 10 years, but at some point, 100% will do.\nAt some point, we will ask our AI companion to write a program that does X for us or whatever.\nBut, I think this is a great thing, as it will give us more time & energy to focus on what matters, such as:\n- solving real-world problems (not just tech problems)\n- moving to the next level of technology (Bioengineering, interplanetary colonization, etc.)\n- think about the grand scheme of things\n- be more creative\n- more time to connect with our family\n- more time to take care of our\nI personally think it is a significant step for humanity.\n.\nWhat do you think?\nAs an engineer, do you see your job still present in the next 10+ years?\nHere is the full talk\n \n  A Conversation with the Founder of NVIDIA: Who Will Shape the Future of AI?\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Highlight the importance of good data for successful AI implementation and emphasize the need for companies to prioritize data quality.", "content": "    :       \nThere's a lot of excitement about what AI can do, but theres not enough talk about what it takes to make AI work properly.\nThe truth? Good AI needs good data. Without it, even the smartest AI falls short.  \nCompanies want AI for faster decisions and better processes.\nHowever, executives and directors don't want to listen to data engineers who warn them about the risks of rushing into AI without solid data.\nBut can they trust what AI tells them if theyre unsure about their data?\nWhen data gets neglected, doubts creep in. And nobody should make big choices on shaky ground.  \nThe hype of Generative AI is loud, but lets not get carried away.\nLet's focus on getting our data right. Because when we trust our data, we can trust the AI that relies on it.[URL]"}, {"instruction": "Join the FREE webinar on Architecting Your LLM Twin hosted by the Decoding ML team, covering LLM System design, data collection, feature, training, and inference pipelines with MLOps best practices on Fri, May 3. Register at [URL]. Check out the GitHub repository for self-paced learning.", "content": "Do you want to learn to build production LLM systems using good SWE and MLOps practices?   I am hosting a FREE webinar next week.\nI was invited by Maven to speak in their Lighting Lesson series about how to Architect Your LLM Twin. This 30-min session is for ML & MLOps engineers who want to learn:\nLLM System design of your LLM Twin\n  Using the 3-pipeline architecture & MLOps good practices\nDesign a data collection pipeline\n  data crawling, ETLs, CDC, AWS\nDesign a feature pipeline\n  streaming engine in Python, data ingestion for fine-tuning & RAG, vector DBs\nDesign a training pipeline\n  create a custom dataset, fine-tuning, model registries, experiment trackers, LLM evaluation\nDesign an inference pipeline\n  real-time deployment, REST API, RAG, LLM monitoring\n \nJoin LIVE on Fri, May 3!\nRegister here (its free)  \n[URL]\n.\nThe presentation is based on the FREE LLM Twin course I am creating with the\nDecoding ML\nteam.\nIf you can't join the webinar, consider checking out the GitHub repository and learn at your own pace:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Generate a detailed tutorial on building a Real-time News Search Engine using Kafka, Vector DBs, and streaming engines in Python, syncing multiple news sources in real-time, as outlined in the provided article.", "content": "Decoding ML\nreleased an article & code on building a Real-time News Search Engine using Kafka, Vector DBs and streaming engines.\nEverything in Python!\nThe end goal?\nLearn to build a production-ready semantic search engine for news that is synced in real-time with multiple news sources using:\n- a streaming engine\n- Kafka\n- a vector DB.\nThe problem?\nAccording to a research study by\nearthweb.com\n, the daily influx of news articles, both online and offline, is between 2 and 3 million.\nHow would you constantly sync these data sources with your vector DB to stay in sync with the outside world?\nThe solution!\n  Here is where the streaming pipeline kicks in.\nAs soon as a new data point is available, it is:\n- ingested\n- processed\n- loaded to a vector DB\n...in real-time by the streaming pipeline \n.\nHere is what you will learn from the article  \n  Set up your own\nUpstash\nKafka & Vector DB clusters\n  Structure & validate your data points using Pydantic\n  Simulate multiple Kafka Clients using ThreadPoolExecutor & KafkaProducer\n  Stream processing using\nBytewax\n- learn to build a real-time RAG ingestion pipeline\n  Batch-upserting embeddings + metadata to Upstash Vector DB\n  Build a Q&A UI using\nStreamlit\n  Unit Testing  - Yes, we even added unit testing!\n.\nCurious to level up your Python, streaming & RAG game \nCheck out the article & code. Is FREE\n \n  Article:\n[URL]\n  GitHub code:\n[URL]\n.\n  Hey,\nThe post is not finished yet,\nBe sure to try out the code.  \n  It has a beautiful README & Makefile with step-by-step instructions.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily content on production ML and MLOps engineering.[URL]"}, {"instruction": "Create a post highlighting 6 things you are grateful for in the past year, and another list detailing 6 things you plan to achieve in the upcoming year. Conclude the post by emphasizing the importance of taking care of both your physical and mental well-being alongside your technical skills. Encourage engagement by asking readers to share their main goals for the upcoming year or something they are grateful for from the previous year. Incorporate hashtags #machinelearning, #mlops, and #personaldevelopment. Lastly, invite readers to follow you for daily ML engineering and MLOps lessons.", "content": "Ever thought about how a year can completely change your vision? 2023 did just that for me...\nI don't really care about New Year's Eve. It is just another trip around the sun.\n...but it is a great moment to pause and think about the last year and the future.\nPersonally, I love planning things and learning from my past. It helps me gain perspective and prioritize the important things in my life.\n.\nSo here are 6 things that happened in 2023 that I am grateful for:\n1. Enough discipline to work out almost daily & eat healthy.\n2. Grew my MLE & MLOps content creation business.\n3. Created x2 open-source MLE & MLOps free courses that accumulated >1800 stars on GitHub.\n4. Met many incredible MLE & MLOps people from Europe and the US.\n5. Had the chance to work on awesome deep fake projects in production at Metaphysic\n6. Adopted my second cat: Arthur. I love this little fellow. Now we are a happy 2 people - 2 cat family.\n...and my top 6 things I plan to do in 2024:\n1. Grow the Decoding ML publication. I have some exciting news here that I will share in the following days.\n2. Travel & work from a different country every 3-4 months to level up my social and emotional skills while exploring the world.\n3. Level up my MLE & MLOps production-ready skills on real-world projects to share better insights with you.\n4. Better understand myself and be 100% true to myself.\n5. Start creating video content.\n6. This is a grand one: Move to the city center to leave my house more.\n.\nMy final take is that you should always take care of your mind & body as much as your tech skills.\nIf you degrade, your skills degrade with you.\nLet the games begin!\nWhat are your main goals for 2024, or what are you grateful for from last year?\n.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\n  Follow me for daily lessons about ML engineering and MLOps."}, {"instruction": "Create a social media post to announce the upcoming crowdfunding campaign for accelerating cancer treatment using artificial intelligence. Include hashtags #comingsoon, #crowdfunding, #artificialintelligence, #crowdfundingcampaign, #startup, #medtech, #investment, and link to the campaign story.", "content": " We are ON to something BIG!\nhashtag\n#\ncomingsoon\nWe are getting ready for starting our first\nhashtag\n#\ncrowdfunding\ncampaign, along with the\nRnin\nplatform!\nTogether we can be one step closer to achieving our mission:   accelerate cancer treatment through\nhashtag\n#\nartificialintelligence\n!\n  Discover our story:\n[URL]\n[URL]\nhashtag\n#\ncrowdfundingcampaign\nhashtag\n#\nstartup\nhashtag\n#\nmedtech\nhashtag\n#\ninvestment\nhashtag\n#\nartificialintelligence[URL]"}, {"instruction": "Create a post highlighting AIMinded's efforts in using AI to advance healthcare, mentioning the support of Health Venture Lab and GE HealthCare executives in recognizing their work as one of the top three most promising projects.", "content": "Digital technology and AI have the power to solve many of todays healthcare problems. At AIMinded we want to make this as accessible as possible and accelerate the treatment of cancer together.\nHealth Venture Lab\nteam and\nGE HealthCare\nexecutives believed in us and highlighted us in the top three most promising projects.\nRead more below!"}, {"instruction": "Create a post about presenting the vision of AIMinded and how AI can revolutionize the cancer paradigm, using hashtags: #artificialintelligence, #change, and #radiotherapy.", "content": "It was a pleasure to present the vision of AIMinded and how AI can change the entire cancer paradigm.\nhashtag\n#\nartificialintelligence\nhashtag\n#\nchange\nhashtag\n#\nradiotherapy"}, {"instruction": "Write a tutorial on how to build an AWS infrastructure and CI/CD pipeline for data collection, using Docker, AWS services, MongoDB, Pulumni, and GitHub Actions, with a focus on the process and tools involved in the setup.", "content": "6 steps to build your AWS infrastructure (using IaC) and a CI/CD pipeline that will work for 90% of your projects  \nWe will use the data collection pipeline from our free digital twin course as an example, but it can easily be extrapolated to most of your projects.\nFirst, let's see what is in our toolbelt:\n- Docker\n- AWS ECR\n- AWS Lambda\n- MongoDB\n- Pulumni\n- GitHub Actions\nSecondly, let's quickly understand what the data collection pipeline is doing\nIt automates your digital data collection from LinkedIn, Medium, Substack, and GitHub. The normalized data will be loaded into MongoDB.\nNow, let's understand how the AWS infrastructure and CI/CD pipeline works  \n1. We wrap the application's entry point with a `handle(event, context: LambdaContext)` function. The AWS Lambda serverless computing service will default to the `handle()` function.\n2. Build a Docker image of your application inheriting the `public.ecr.aws/lambda/python:3.11` base Docker image\n  Now, you can quickly check your AWS Lambda function locally by making HTTP requests to your Docker container.\n3. Use Pulumni IaC to create your AWS infrastructure programmatically:\n- an ECR as your Docker registry\n- an AWS Lambda service\n- a MongoDB cluster\n- the VPC for the whole infrastructure\n4. Now that we have our Docker image and infrastructure, we can build our CI/CD pipeline using GitHub Actions. The first step is to build the Docker image inside the CI and push it to ECR when a new PR is merged into the main branch.\n5. On the CD part, we will take the fresh Docker image from ECR and deploy it to AWS Lambda.\n6. Repeat the same logic with the Pulumni code   Add a CD GitHub Action that updates the infrastructure whenever the IaC changes.\nWith this flow, you will do fine for 90% of your projects  \n.\nTo summarize, the CI/CD will look like this:\nfeature PR -> merged to main -> build Docker image -> push to ECR -> deploy to AWS Lambda\n.\nWant to run the code yourself?\nCheck out Lesson 2 from the FREE LLM Twin course hosted by @company_decoding-ml\nIt contains a step-by-step article & open-source code.\n \n  The Importance of Data Pipelines in the Era of Generative AI:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Provide detailed steps on how to build a robust streaming pipeline for real-time data processing in a complex ML system using Rust and Python bindings with integration to popular libraries and seamless connections to Kafka and custom connectors. Highlight features like real-time vector storage, scalability, versatile search capabilities, and integration with Bytewax. Emphasize the importance of constant syncing with MongoDB for reliable results in Large Language Model fine-tuning and RAG applications. End the instruction by inviting the audience to access Lesson 4 on Streaming Pipelines and Vector DB through the provided URLs for further learning.", "content": "         ,    !  \n  In a complex ML system, streaming pipelines can be the key to real-time data processing and fine-tuning Large Language Models (LLMs). Heres a snapshot of what it takes to build a robust streaming pipeline:\n - -   \n Built in Rust  for high performance.\n Python   bindings for easy integration with popular libraries like NumPy and PyTorch.\n Plug-and-play setup: Get started quickly without JVM headaches.\n Broad compatibility: Seamlessly connects with Kafka, and custom connectors.\n -  -     -  :\n Real-time vector storage\n Scalability: Efficiently handles large datasets.\n Versatile search capabilities: Includes k-Nearest Neighbors (k-NN) for fast and accurate retrieval.\n Seamless integration: Works smoothly with Bytewax to store and retrieve feature vectors.\nThe pipeline constantly syncs with MongoDB through a  , ensuring the feature store stays updated with the latest changes. This real-time syncing is crucial for reliable results in LLM fine-tuning and RAG applications.\nIf you're curious to level up your knowledge about streaming pipelines and data engineering  \nCheck out        . It's FREE, and no registration is required\n \n  Lesson 4 -\n[URL]\nStreaming Pipelines:\n[URL]\nVector DB:\n[URL]"}, {"instruction": "Create a social media post celebrating being named one of Fast Company's Most Innovative Companies of 2024, highlighting the focus on empowering users, ethical AI development, and commitment to responsible use of technology. Include a link to a related article and relevant hashtags. #metaphysic #ethics #empowerment #genai #ai", "content": "Today we're proud to announce that we have been named one of\nFast Company\n's Most Innovative Companies of 2024.\nWe are recognized for providing a revolutionary safe space, empowering users to own and control their digital identities. Our work ethically pushes the boundaries of what AI can do while upholding individual rights and consent.\nWe believe innovation is about more than just the latest features. It's about solving human problems with technology infused with creativity, vision and ethics.\nCongratulations to the entire Metaphysic team on this well-deserved honor. Their efforts to develop user-empowering AI align with our overall commitment to building a more responsible AI world.\nAs leaders, we must implement AI responsibly. The true potential of technology is realized when it enriches people's lives while safeguarding their rights.\nRead how we empower actors with a safe space for their digital selves:\n[URL]\nhashtag\n#\nmetaphysic\nhashtag\n#\nethics\nhashtag\n#\nempowerment\nhashtag\n#\ngenai\nhashtag\n#\nai[URL]"}, {"instruction": "Write a post discussing the potential of Artificial Intelligence to take control in a proactive way, empowering individuals rather than threatening them. Highlight the role of AI in various fields such as medicine, emphasizing its ability to learn from human experiences to generate solutions not always accessible consciously. Encourage readers to embrace AI as a tool for simplifying life and driving evolution, challenging them to step out of their comfort zones and compete with themselves. Conclude by prompting reflection on the choice individuals have in allowing AI to assist and recognizing their role in shaping the future of humanity.", "content": " Poate Inteligena Artificial s preia controlul?\nDeja o face. Este n buzunarul nostru, pe ecranul din camer, pe pontatorul de la serviciu sau n coul de cumprturi de la Emag.\ni totusi, nu acesta este subiectul pe care vreau sa-l discut, ci mai degrab cum ar putea Inteligena Artificial s preia controlul ntr-un mod asumat, ajutndu-ne pe noi ca indivizi.\nAud foarte des afirmaiile: AI-ul mi va lua locul., AI-ul este ru, AI-ul este de neneles. etc. ntr-adevr, s-a creat o utopie n jurul AI-ului, de parc traim un nou episod din Brave new world. Dar hai s trim episodul din prezentul actual, prezentul tu sau prezentul meu.\nInteligena artificial este creat de un grup de oameni pentru un anumit scop. Dac scopul iniial se schimb, acest fapt este tot rezultat oamenilor.\n Harner Slack, de la Harvard Medical School afirm: Dac un doctor poate poate fi nlocuit de un computer, atunci el sau ea merit s fie nlocuit/ de computer.. . O afirmaie acid, care intrig i declaneaz un conflict mpotriv a tot ce nseamn computer, tehnologie, digitalizare sau Inteligen artificial.\nLum o gur de aer, respirm un pic i apoi ne imginm. Cum nva inteligena artificial? Este ca un copil mic care are nevoie de ndrumarea celor mai mari, adic de experiena lor. AI-ul, n radioterapie sau radiologie de exemplu, va nva din experiena acelor doctori. Scopul? n timp s devin real ajutor pentru acetia, genernd soluii i ipoteze la care un om nu are mereu acces n mod contient. n medicin, AI-ul poate deveni cel mai bun prieten al medicului, uurandu-i munca repetitiv, fiind al treilea lui ochi atunci cand obosete.\n  Am neles n timp c inteligena artificial este ca un tool care ne simplific viaa, care provoac la evoluie. Oamenii sunt obinuii sau programai s evolueze atunci cnd ies din zona de confort sau din simpla competiie cu altcineva. Dac privim deschis ctre aceasta idee, AI-ul ne poate provoca, pe noi ca oameni s evolum, s ne dorim mai mult.\nAI-ul, nvnd din experienele a unor mii de oameni, va gsi soluii la care noi ca indivizi nu ne-am gandi. tim cu toii c energia unui grup este mai mare decat energia unui singur individ.\nEste alegerea noastr cum putem lsa AI-ul s ne ajute i este tot alegerea noastr s distingem rolul propriu n care acionm.\nDeci, dac defapt ne e  team de a pierde controlul asupra evoluiei omenirii, v ntreb acum, cine deine cu adevrat controlul?[URL]"}, {"instruction": "Share this post with your followers and ask them how they determine what is important to them by using hashtags: #productivity, #personaldevelopment, #mentalhealth", "content": "Want to know the best productivity tip that works for me? Nothing fancy... it's quite simple to implement... Here it is  \nLearn to say \"No\" to things not of the highest importance to you.\nEven simple things (meetings, reminders, notes, calls, etc.) add a lot of administrative overhead that doesn't add much value in the long run.\nSo how do you filter the Yes's from the No's?\nTo do that, you must clearly define what is essential for you, as life will throw at you many \"temptations\" that seem important at the moment but are just distractions in the long run.\nI cannot tell you what is important to you...\n...but I have a simple formula that helps me navigate my process of decision-making:\nI try to optimize the following:\n(reward + curiosity) / effort\n  this is the ML engineer inside me...\nWhat do I do with it?\n  I sort everything based on it and pick the top 2 or 3 things (always to have the eyes on the prize).\nI will move forward to the next ones when I am done with them.\nNote: This mainly works for tangible tasks that must be executed soon (from 1 to 6 months). The big goals of your life are a different story.\nThat's it!\n  This works for everything, from learning new things to leveling up your career or building something extraordinary.\n.\nHow do you pick what is important to you or not? I would love to hear your strategy in the comments  \nhashtag\n#\nproductivity\nhashtag\n#\npersonaldevelopment\nhashtag\n#\nmentalhealth[URL]"}, {"instruction": "Provide a detailed guide on how to implement real-time monitoring and metrics for an ML system along with strategies for accessing ground truth data and building a monitoring pipeline using tools like Airflow, GCS, and orchestration tools.", "content": "How to add real-time monitoring and metrics to your ML system.\nYour model is exposed to performance degradation after it is deployed to production.\nThat is why you need to monitor it constantly.\nThe most common way to monitor an ML model is to compute its metrics.\nBut for that, you need the ground truth.\nIn production, you can automatically access the ground truth in 3 main scenarios:\n1. near real-time: you can access it quite quickly\n2. delayed: you can access it after a considerable amount of time (e.g., one month)\n3. never: you have to label the data manually\n.\nFor use cases 2. and 3. you can quickly compute your monitoring pipeline in the following way:\n- store the model predictions and GT as soon as they are available (these 2 will be out of sync -> you can't compute the metrics right away)\n- build a DAG (e.g., using Airflow) that extracts the predictions & GT computes the metrics in batch mode and loads them into another storage (e.g., GCS)\n- use an orchestration tool to run the DAG in the following scenarios:\n1. scheduled: if the GT is available in near real-time (e.g., hourly), then it makes sense to run your monitoring pipeline based on the known frequency\n2. triggered: if the GT is delayed and you don't know when it may come up, then you can implement a webhook to trigger your monitoring pipeline\n- attach a consumer to your storage to use and display the metrics (e.g., trigger alarms and display them in a dashboard)\n.\nIf you want to see how to implement a monitoring pipeline using Airflow and GCS, check out my article:\n  Ensuring Trustworthy ML Systems With Data Validation and Real-Time Monitoring:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps.[URL]"}, {"instruction": "Check out the free Advanced Retrieval for AI with Chroma course from DeepLearning.AI for insights on improving RAG systems and the latest techniques. Follow the course assuming prior knowledge of RAG, and explore query expansion, cross-encoder re-ranking, and embedding adaptors. The course is free, takes 1 hour, and covers essential aspects of RAG systems. Don't miss it!", "content": "RAG systems are far from perfect   This free course teaches you how to improve your RAG system.\nI recently finished the Advanced Retrieval for AI with Chroma free course from\nDeepLearning.AI\nIf you are into RAG, I find it among the most valuable learning sources.\nThe course already assumes you know what RAG is.\nIts primary focus is to show you all the current issues of RAG and why it is far from perfect.\nAfterward, it shows you the latest SoTA techniques to improve your RAG system, such as:\n- query expansion\n- cross-encoder re-ranking\n- embedding adaptors\nI am not affiliated with\nDeepLearning.AI\n(I wouldn't mind though).\nThis is a great course you should take if you are into RAG systems.\nThe good news is that it is free and takes only 1 hour.\nCheck it out  \n  Advanced Retrieval for AI with Chroma:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps.[URL]"}, {"instruction": "Share with your audience the resource for comparing 37 different vector DB solutions across 29 features and encourage contributing to keep the table updated.", "content": "This is the only resource you need to pick the right vector DB for your exact use case.\nSince ChatGPT made AI cool, besides the millions of ChatGPT posts you got tired of and blocked, you realized that a new type of tool started to hit the scene: Vector DBs.\nAs vector DBs play a crucial role in most LLM applications, they popped out everywhere.\nOn this day, there are 37 vector DB solutions, that are constantly changing and adding features.\nNow, how the h**l should I pick one?\n    \"  \"  .\nIt is an effort managed by\nSuperlinked\n, where they carefully compared all these 37 vector DBs across 29 features, such as:\n- License\n- GitHub \n- support for text, image or struct models\n- RAG, RecSys, LangChain or LllamaIndex APIs\n- pricing\n- sharding\n- document size\n- vector dims\n...and more!\nI won't list all 29 features.\nYou have to check it out to see them for yourself  \n    Vector DB Comparison:\n[URL]\nNote: To keep the table updated or add more features, you can contribute to it yourself.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps.[URL]"}, {"instruction": "Guide on implementing a streaming pipeline for financial news using Bytewax functions and the necessary APIs to extract, validate, clean, chunk, embed, and load data into a vector DB for an AI financial assistant.", "content": "This is how you can implement a streaming pipeline to populate a vector DB to do RAG for a financial assistant powered by LLMs.\n  All the following steps are wrapped in\nBytewax\nfunctions and connected in a single streaming pipeline (aka Bytewax flow)  \nExtract financial news from Alpaca\nYou need 2 types of inputs:\n1. A WebSocket API to listen to financial news in real time - used to listen 24/7 for new data and ingest it as soon as it is available.\n2. A RESTful API to ingest historical data in batch mode. When you deploy a fresh vector DB, you use it to populate it with older data.\nYou wrap the ingested HTML document and its metadata in a `pydantic` NewsArticle model to validate its schema.\nRegardless of the input type, the ingested data is the same. Thus, the following steps are the same for both data inputs  \nParse the HTML content\nAs the ingested financial news is in HTML, you must extract the text from particular HTML tags.\n`unstructured` makes it as easy as calling `partition_html(document)`, which will recursively return the text within all essential HTML tags.\nThe parsed NewsArticle model is mapped into another `pydantic` model to validate its new schema:\n- the headline\n- summary\n- full content.\nClean the text\nNow we have a bunch of text that has to be cleaned. Again, `unstructured` makes things easy. Calling a few functions we clean:\n- the dashes & bullets\n- extra whitespace & trailing punctuation\n- non ascii chars\n- invalid quotes\nFinally, we standardize everything to lowercase.\nChunk the text\nAs the text can exceed the context window of the embedding model, we have to chunk it.\nYet again, `unstructured` provides a valuable function that splits the text based on the tokenized text and expected input length of the embedding model.\nThis strategy is naive, as it doesn't consider the text's structure, such as chapters, paragraphs, etc. As the news is short, this is not an issue, but LangChain provides a `RecursiveCharacterTextSplitter` class that does that if required.\nEmbed the chunks\nYou pass all the chunks through an encoder-only model.\nWe have used `all-MiniLM-L6-v2` from `sentence-transformers`, a small model that can run on a CPU and outputs a 384 embedding.\nBut based on the size and complexity of your data, you might need more complex and bigger models.\nLoad the data in the Qdrant vector DB\nFinally, you insert the embedded chunks and their metadata into the Qdrant vector DB.\nThe metadata contains the embedded text, the source_url and the publish date.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Provide step-by-step instructions on defining infrastructure, dependencies, and deploying ML models using Beam as a serverless provider.", "content": "Deploying & managing ML models is hard, especially when running your models on GPUs.\nBut serverless makes things easy.\nUsing Beam as your serverless provider, deploying & managing ML models can be as easy as  \nDefine your infrastructure & dependencies\nIn a few lines of code, you define the application that contains:\n- the requirements of your infrastructure, such as the CPU, RAM, and GPU\n- the dependencies of your application\n- the volumes from where you can load your data and store your artifacts\nDeploy your jobs\nUsing the Beam application, you can quickly decore your Python functions to:\n- run them once on the given serverless application\n- put your task/job in a queue to be processed or even schedule it using a CRON-based syntax\n- even deploy it as a RESTful API endpoint\n.\nAs you can see in the image below, you can have one central function for training or inference, and with minimal effort, you can switch from all these deployment methods.\nAlso, you don't have to bother at all with managing the infrastructure on which your jobs run. You specify what you need, and Beam takes care of the rest.\nBy doing so, you can directly start to focus on your application and stop carrying about the infrastructure.\nThis is the power of serverless!\n.\n  Check out Beam to learn more:  \n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily content on ML and MLOps engineering.[URL]"}, {"instruction": "Utilize MLOps to automate the delegation of chores and enhance the efficiency of writing content about LLMs. Incorporate hashtags #machinelearning, #artificialintelligence, and #automation in the relevant posts.", "content": "I am so excited about completely delegating my chores and writing more crappy posts about LLMs.\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelligence\nhashtag\n#\nautomation[URL]"}, {"instruction": "Explore the MLOps roadmap for 2024 provided by Maria Vechtomova to learn about essential programming skills, containerization, MLOps principles, components, and infrastructure as code.", "content": "Want to learn MLOps but got stuck at the 100th tool you think you must know? Here is the MLOps roadmap for 2024  \nMLOps vs. ML engineer\nIn theory, MLEs focus on deploying models to production while MLOps engineers build the platform used by MLEs.\nI think this is heavily dependent on the scale of the company. As the company gets smaller, these 2 roles start to overlap more.\nThis roadmap will teach you how to build such a platform, from programming skills to MLOps components and infrastructure as code.\n.\nHere is the MLOps roadmap for 2024 suggested by\nMaria Vechtomova\nfrom @company_marvelous-mlops:\n1. Programming\n- Python & IDEs\n- Bash basics & command line editors\n2. Containerization and Kubernetes\n- Docker\n- Kubernetes\n3. Machine learning fundamentals\n...until now we laid down the fundamentals. Now let's get into MLOps  \n4. MLOps principles\n- reproducible,\n- testable, and\n- evolvable ML-powered software\n5. MLOps components\n- Version control & CI/CD pipelines\n- Orchestration\n- Experiment tracking and model registries\n- Data lineage and feature stores\n- Model training & serving\n- Monitoring & observability\n6. Infrastructure as code\n- Terraform\n.\nAs a self-learner, I wish I had access to this step-by-step plan when I started learning MLOps.\nRemember, you should pick up and tailor this roadmap at the level you are currently at.\nFind more details about the roadmap in\nMaria Vechtomova\n's article  \n  MLOps roadmap 2024:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps.[URL]"}, {"instruction": "Join the webinar on Thursday, July 21st at 16:00 to learn about the latest developments in artificial intelligence and its application in radiotherapy for better-targeted healthcare services. Speakers include Vesa Alexandru, Fabian Jichi, and Mihai Zerbea. For event details and registration, visit [URL]. See you there! #artificialintelligence #medical #radiotherapy #webinar", "content": "Utilizarea inteligenei artificiale pentru analiza datelor medicale crete rapid. Acest webinar ofer o privire asupra evoluiilor recente i cum este aplicat inteligena artificial n radioterapie pentru furnizarea unor servicii de sntate mai bine direcionate, centrate pe om.\nAltur-te nou joi, 21 iulie de la ora 16:00 pentru a afla despre cele mai bune practici.\nSpeakeri:\nVesa Alexandru - CEO AIMinded, Software Developer;\nFabian Jichi - Co-fondator AIMinded, Software Developer;\nMihai Zerbea - Doctor Specialist Radioterapie, Doctor Advisor la AIMinded.\nMai multe detalii despre eveniment:\n[URL]\nPentru a participa te rugm s te nregistrezi aici:\n[URL]\nTe ateptm!\nhashtag\n#\nartificialintelligence\nhashtag\n#\nmedical\nhashtag\n#\nradiotherapy\nhashtag\n#\nwebinar"}, {"instruction": "Write a comprehensive guide on building an end-to-end PyTorch-ONNX-TensorRT pipeline for YOLO Object Detection Models, including steps for PyTorch-to-ONNX conversion, configuring TensorRT, running ONNX sanity checks, conversion to TensorRT, performance optimization using Nvidia Perf Analyzer, and generating metric dashboards.", "content": "How to build an end-to-end PyTorch   ONNX   TensorRT pipeline for YOLO Object Detection Models (or other DL models)\nThe problem?\nWhen compiling YOLO models to TensorRT on multiple GPUs, there are always mismatches between ONNX, ORT, TensorRT, etc.\n.\nThe solution?\nThis is how you can automate this process and save hours of repetitive work  \n1. Generic PyTorch-to-ONNX: have a single .py script that converts the .pt 2 .onnx\n2. Single Configuration File: have a config.json file where you specify the TensorRT version I need, the ONNX Operator Set, and metadata fields to save the model\n3.  ONNX Sanity Check: run several tests on the integrity of the model and its configuration\n4. Convert to TensorRT: start the TensorRT container, copy the .onnx model, convert the model to .engine, copy it back to the host path, then remove the container\n5. Optimal configuration search with Nvidia Perf Analyzer: test various use cases, like optimal batch size, min/optimal/max load of concurrent requests, GPU stats under workload, etc.\n6. Metrics Dashboard: generate charts around critical metrics like Queries-per-Second (QPS), Concurrency Analysis, and Precision Accuracy\nCurious to try it out?\nAlexandru Razvant\nwrote a detailed article on Decoding ML and provided the code for implementing this technique on your own models.\nCheck it out on Decoding ML  \n  How to build a PyTorch - TensorRT pipeline for YOLO Object Detection Models! :\n[URL]\nNote: All the credits for the diagram go to\nAlexandru Razvant\n.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps.[URL]"}, {"instruction": "Craft a social media post summarizing your achievements in 2023 related to MLE and MLOps, express gratitude to your followers, and encourage them to follow for daily lessons.", "content": "2023 was crazy. Here are my results after one year of writing content about MLE and MLOps and starting as a nobody.\n...not that now I am somebody  \nI had some humble goals. At the beginning of 2022, I hadn't imagined I would plan to make a living out of creating content:\n- LinkedIn: 3k -> 22.5k followers (goal 10k)\n- Medium: 200 -> 1.3k followers (goal 1k)\n...but I stumbled on some great sources of inspiration and people who helped me grow more than I could have imagined.\nThus, during 2023, I got excited and experimented with quite a few things, such as:\n- x2 open-source courses about MLOps, LLMOps, and MLE that accumulated over 1800 GitHub stars:\n-> Hands-on LLMs:\n[URL]\n-> The Full Stack 7-Steps MLOps Framework:\n[URL]\n- I started my newsletter: \"Decoding ML\", which grew to 2.8k followers\n- I tried to grow my Twitter/X account (only 233 followers): this was quite a mess as I haven't    tweaked my posts well enough to satisfy the\n.\n2023    ,        2024.\nBut...\nI want to thank everybody who followed me and engaged with my content. You are one of the first drivers that keep me going. So... Thank you  \nSecondly, setting such goals isn't that essential. What if I haven't reached out to any of them? That would have demoralized me entirely. But, I think they are critical in giving a clear direction.\nRemember: \"Direction is more important than... anything.\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\n  Follow me for daily lessons about ML engineering and MLOps."}, {"instruction": "Provide guidance on reducing VRAM consumption during LLMs training using mixed-precision, lower-precision, reducing batch size, gradient accumulation, stateless optimizer, gradient checkpointing, and CPU parameter offloading.", "content": "Here are 7 tips you must know to reduce your VRAM consumption of your LLMs during training so you can fit it on x1 GPU.\n1. Mixed-precision: During training you use both FP32 and FP16 in the following way: \"FP32 weights\" -> \"FP16 weights\" -> \"FP16 gradients\" -> \"FP32 gradients\" -> \"Update weights\" -> \"FP32 weights\" (and repeat). As you can see, the forward & backward passes are done in FP16, and only the optimization step is done in FP32, which reduces both the VRAM and runtime.\n2. Lower-precision: All your computations are done in FP16 instead of FP32. But the key is using bfloat16 (\"Brain Floating Point\"), a numerical representation Google developed for deep learning. It allows you to represent very large and small numbers, avoiding overflowing or underflowing scenarios.\n3. Reducing the batch size: This one is straightforward. Fewer samples per training iteration result in smaller VRAM requirements. The downside of this method is that you can't go too low with your batch size without impacting your model's performance.\n4. Gradient accumulation: It is a simple & powerful trick to increase your batch size virtually. You compute the gradients for \"micro\" batches (forward + backward passes). Once the accumulated gradients reach the given \"virtual\" target, the model weights are updated with the accumulated gradients. For example, you have a batch size of 4 and a micro-batch size of 1. Then, the forward & backward passes will be done using only x1 sample, and the optimization step will be done using the aggregated gradient of the 4 samples.\n5. Use a stateless optimizer: Adam is the most popular optimizer. It is one of the most stable optimizers, but the downside is that it has 2 additional parameters (a mean & variance) for every model parameter. If you use a stateless optimizer, such as SGD, you can reduce the number of parameters by 2/3, which is significant for LLMs.\n6. Gradient (or activation) checkpointing: It drops specific activations during the forward pass and recomputes them during the backward pass. Thus, it eliminates the need to hold all activations simultaneously in VRAM. This technique reduces VRAM consumption but makes the training slower.\n7. CPU parameter offloading: The parameters that do not fit on your GPU's VRAM are loaded on the CPU. Intuitively, you can see it as a model parallelism between your GPU & CPU.\nMost of these methods are orthogonal, so you can combine them and drastically reduce your VRAM requirements during training.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Craft a strategic business plan and prioritize building trust and confidence in your vision while pitching your innovative AI and MedTech startup to potential investors and stakeholders. Use relevant hashtags like #artificialintelligence and #leadership to reach a wider audience.", "content": "Pitching an idea isn't everything. Trusting your vision is.\nAIMINDED\nhashtag\n#\nradiotherapy\nhashtag\n#\nartificialintelligence\nhashtag\n#\nmedtech\nhashtag\n#\nstartup\nhashtag\n#\nvision\nhashtag\n#\nleadership"}, {"instruction": "Create a 2-minute LLMOps vs. MLOps summary highlighting the key differences between the two concepts.", "content": "LLMOps here, LLMOps there, but did you take the time to see how it differs from MLOps?\nIf not, here is a 2-min LLMOps vs. MLOps summary  \nWhat is LLMOps?\nWell, everything revolves around the idea that \"Size matters.\"\nLLMOps is about best practices for efficient deployment, monitoring, and maintenance, but this time, it is for large language models.\nLLMOps is a subset of MLOps, focusing on training & deploying large models trained on big data.\nIntuitive right?\nBut here are 5 LLMOps unique factors that set it apart from MLOps  \n1. Computational resources: training your models on CUDA-enabled GPUs is more critical than ever, along with knowing how to run your jobs on a cluster of GPUs leveraging data & model parallelism using techniques such as ZeRO from DeepSpeed. Also, the high cost of inference makes model compression techniques essential for deployment.\n2. Transfer learning: training models from scratch is a thing of the past. In most use cases, you will fine-tune the model on specific tasks, leveraging techniques such as LLaMA-Adapters or QLora.\n3. Human feedback: reinforcement learning from human feedback (RLHF) showed much potential in improving the quality of generated outputs. But to do RLHF, you have to introduce a feedback loop within your ML system that lets you evaluate the generated results based on human feedback, which are even further used to fine-tune your LLMs.\n4. Guardrails: to create safe systems, you must protect your systems against harmful or violent inputs and outputs. Also, when designing your prompt templates, you must consider hallucinations and prompt hacking.\n5. Monitoring & analyzing prompts: most ML platforms (e.g.,\nComet\nML) introduced specialized logging tools to debug and monitor your LLMs to help you find better prompt templates and protect against hallucination and hacking.\n.\nTo conclude...\nLLMOps isn't anything new for those familiar with MLOps and Deep Learning.\nFor example, training deep learning models on clusters of GPUs or fine-tuning them isn't new, but now it is more important than ever to master these skills as models get bigger.\nBut it indeed introduced novel techniques to fine-tune models (e.g., QLora), to merge the fields of RL and DL, and a plethora of tools around prompt manipulation & storing, such as:\n- vector DBs (e.g.,\nQdrant\n)\n- prompt chaining (e.g.,\nLangChain\n)\n- prompt logging & analytics (e.g., Comet LLMOps)\nWhat do you think? Is the term of LLMOps going to stick around?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps.[URL]"}, {"instruction": "Write a comprehensive guide on deploying private LLMs with AWS SageMaker, incorporating the role of AWS SageMaker in ML deployment, nuances of deploying Llama2-7b-chat, a detailed deployment process walkthrough, and the significance of managing private LLMs in today's tech landscape.", "content": "Exploring the frontier of private LLM deployment with AWS SageMaker in my latest article on Llama2-7b-chat  \n  Dive deep with me in \"Introduction to Deploying Private LLMs with AWS SageMaker\" where I unpack the complexities of deploying Llama2-7b-chat.\n  What You'll Discover:\n  AWS SageMaker's role in revolutionizing ML deployment, balancing scalability with robust security.\n  Unraveling Llama2-7b-chat: A leap in AI communication, powered by extensive 2T token training.\n  A meticulous walkthrough of the deployment process, from environment setup to execution.\n  From Theory to Practice:\n  Follow my detailed guide on configuration,\nHugging Face\nintegration, and effective inference strategies.\n  Experience the perfect blend of theoretical knowledge and practical application in deploying LLMs using\nAmazon SageMaker\n.\n  Why This Matters: In today's tech-driven world, knowing how to manage private LLMs like Llama2 and preparing for cloud-based solutions are vital. The AWS SageMaker and Hugging Face LLM DLC are essential for achieving these skills and navigating the AI landscape effectively.\n  Read the full article here  \n[URL]\n#DecodeML#AWS#Sagemaker#AI#LLM#Llama2#HuggingFace[URL]"}, {"instruction": "Create a financial assistant architecture with Feature, Training, and Inference Pipelines using LLMs, vector DBs, and MLOps following the step-by-step guide provided. Check out the Hands-on LLMs course for more details.", "content": "Here is a step-by-step guide on designing the architecture of a financial assistant powered by LLMs, vector DBs and MLOps.\nThe 3-pipeline design, also known as the FTI architecture, makes things simple  \nFeature Pipeline\nWe build a streaming pipeline that listens to real-time financial news, embeds the news, and loads everything in a vector DB. The goal is to add up-to-date news to the user's questions using RAG to avoid retraining.\n1. We listen 24/7 to financial news from Alpaca through a WebSocket wrapped over a\nBytewax\nconnector\n2. Once any financial news is received, these are passed to the Bytewax flow that:\n- extracts & cleans the information from the news HTML document\n- chunks the text based on the LLM's max context window\n- embeds all the chunks using the \"all-MiniLM-L6-v2\" encoder-only model from sentence-transformers\n- inserts all the embeddings + metadata to\nQdrant\n3. The streaming pipeline is deployed to an EC2 that runs multiple Bytewax processes (or to K8s to scale up).\nTraining Pipeline\nWe fine-tune a pretrained LLM to specialize the model to answer financial-based questions.\n1. Manually fill ~100 financial questions.\n2. Use RAG to enrich the questions using the financial news from the Qdrant vector DB.\n3. Use a powerful LLM (e.g., GPT-4) to answer them, or hire an expert if you have more time and resources.\n4. Load Falcon from HuggingFace using QLoRA to fit on a single GPU.\n5. Preprocess the Q&A dataset into prompts.\n6. Fine-tune the LLM and log all the artifacts to\nComet\n's experiment tracker (loss, model weights, etc.)\n7. For every epoch, run the LLM on your test set, log the prompts to Comet's prompt logging feature and compute the metrics.\n8. Send the best LoRA weights to the model registry as the next production candidate.\n9. Deploy steps 4-8 to Beam to run the training on an A10G or A100\nInference Pipeline\nWe hook the financial news stored in the Qdrant Vector DB and the Falcon fine-tuned model into a single entity exposed under a RESTful API.\nSteps 1-7 are all chained together using LangChain.\n1. Use the \"all-MiniLM-L6-v2\" model to embed the user's question.\n2. Using the question embedding, query the vector DB to find the top 3 related financial news.\n3. Attach the news's text (stored as metadata) to the prompt (aka RAG).\n4. Download Falcon's pretrained weights from HF & LoRA's fine-tuned weights from Comet's model registry.\n5. Load the LLM and pass the prompt to it.\n6. Store the conversation in LangChain's memory.\n7. Deploy steps 1-7 under a RESTful API using Beam.\n.\nCheck out the Hands-on LLMs course to see this in action:\n \n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "To build an inference pipeline for a financial assistant powered by LLMs and vector DBs, you will need the following key components: 1. vector DB populated with financial news, 2. embedding language model, 3. fine-tuned open-source LLM, 4. model registry, 5. a framework for LLM applications (LangChain), 6. deploy the LLM app as a RESTful API with Beam, and 7. prompt monitoring with Comet's LLMOps features.", "content": "What do you need to build an inference pipeline for a financial assistant powered by LLMs and vector DBs?\nHere are its 7 key components  \n1. vector DB populated with financial news: This is the output of the feature pipeline. More concretely, a\nQdrant\nvector DB populated with chunks of financial news from Alpaca. During the inference pipeline, we will use it to query valuable chunks of information and do RAG.\n2. embedding language model: To embed the user question and query the vector DB, you need the same embedding model used in the feature pipeline, more concretely `all-MiniLM-L6-v2` from `sentence-transformers`. Using the same encoder-only model is crucial, as the query vector and vector DB index vectors have to be in the same space.\n3. fine-tuned open-source LLM: The output of the training pipeline will be a fine-tuned Falcon 7B on financial tasks.\n4. model registry: The fine-tuned model will be shared between the training & inference pipeline through\nComet\n's model registry. By doing so, you decouple entirely the 2 components, and the model can easily be shared under specific environments (e.g., staging, prod) and versions (e.g., v1.0.1).\n5. a framework for LLM applications: You need LangChain, as your LLM framework, to glue all the steps together, such as querying the vector DB, storing the history of the conversation, creating the prompt, and calling the LLM. LangChain provides out-of-the-box solutions to chain all these steps together quickly.\n6. deploy the LLM app as a RESTful API: One of the final steps is to deploy your awesome LLM financial assistant under a RESTful API. You can quickly do this using Beam as your serverless infrastructure provider. Beam specializes in DL. Thus, it offers quick ways to load your LLM application on GPU machines and expose it under a RESTful API.   Beam:\n[URL]\n7. prompt monitoring: The last step is to add eyes on top of your system. You can do this using Comet 's LLMOps features that allow you to track & monitor all the prompts & responses of the system.\nWanna see how these components are working together?\n \n  Check out our Hands-on LLMs free course:  \n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily content on ML and MLOps engineering.[URL]"}, {"instruction": "Promote the free Full Stack 7-Steps MLOps Framework course from Medium's TDS publication and GitHub, highlighting the hands-on examples and integrated end-to-end ML system for forecasting energy levels across Denmark.", "content": "Want to learn MLE & MLOps in a structured way, for free, and with hands-on examples?\nThen you should check out my The Full Stack 7-Steps MLOps Framework FREE course.\n.\nIn 2.5 hours of reading & video materials, you will learn how to:\n- design a batch-serving architecture\n- use Hopsworks as a feature store\n- design a feature engineering pipeline that reads data from an API\n- build a training pipeline with hyper-parameter tunning\n- use W&B as an ML Platform to track your experiments, models, and metadata\n- implement a batch prediction pipeline\n- use Poetry to build your own Python packages\n- deploy your own private PyPi server\n- orchestrate everything with Airflow\n- use the predictions to code a web app using FastAPI and Streamlit\n- use Docker to containerize your code\n- use Great Expectations to ensure data validation and integrity\n- monitor the performance of the predictions over time\n- deploy everything to GCP\n- build a CI/CD pipeline using GitHub Actions\n- trade-offs & future improvements discussion\n...where all the pieces are integrated into a single end-to-end ML system that forecasts hourly energy levels across Denmark.\nYou can access the course on:\n Medium's TDS publication: text tutorials + videos\n GitHub: open-source code + docs\nThe course is on Medium's TDS publication to make it easily accessible to people worldwide. Thus  \n... anyone can learn the fundamentals of MLE & MLOps.\nSo, no more excuses. Just go and build your own project  \nCheck it out  \n  The Full Stack 7-Steps MLOps Framework:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily content on ML and MLOps engineering.[URL]"}, {"instruction": "Create a blog post highlighting the importance of ethical innovation in AI and the collaboration between Metaphysic and Thorn for child safety measures in generative AI technologies. Mention the partnership with Microsoft, Google, and OpenAI, and introduce the concept of data cards as a tool for ethical AI development. Conclude by emphasizing the collective effort of the tech industry towards responsible AI and child welfare.", "content": "Ethical and responsible innovation is paramount, especially when it comes to protecting the most vulnerable members of society. As we embrace the potential of AI, we have a vital responsibility to ensure our technologies are a force for good. That is why Metaphysic has joined forces with\nThorn\n, a nonprofit dedicated to enacting strong child safety commitments for generative AI. We've partnered with the likes of\nMicrosoft\n,\nGoogle\n, and\nOpenAI\nand together we've committed to implementing robust child safety measures throughout the development, deployment, and maintenance of generative AI technologies.\nMetaphysic, with its dedication to advancing AI research ethically and responsibly, has been an instrumental partner in its commitment to developing and designing a new type of data documentation called a \"data card.\" An innovative and groundbreaking tool, a data card is a structured form that developers must complete, detailing specific aspects of their AI models or datasets. This particular data card, the protocol used by our partners for this initiative, will require developers to provide information about how data related to minors is utilized and the measures taken to prevent its non-consensual processing.\nThis collective action underscores the tech industry's shared commitment to ethical AI development and safeguarding child welfare. We're honored that we stand alongside Thorn in setting new standards for responsible AI.\nhashtag\n#\nMetaphysic\nhashtag\n#\nGenAI\nhashtag\n#\nGenerativeAI\nhashtag\n#\nThorn\nLearn more here:\n[URL]"}, {"instruction": "Create a post to promote a FREE Hands-on LLM Course, detailing 4 video lectures covering fine-tuning, building pipelines, transforming data, and deploying a model, including relevant URLs and a call-to-action to support the course on GitHub.", "content": "Want to build your first LLM project but don't know where to start?\nHere are 4 FREE lectures to put you on the right track  \n#1. -   - \nYou will learn:\n- What is model fine-tuning?\n- Why is it useful?\n- When to use it?\n- Why to fine-tune an LLM using QLoRA\n- How to architect a fine-tuning pipeline in a real-world project\n   Video Lecture 1: Fine-tuning pipeline (Part 1) -  \n[URL]\n#2. - -\nLet's apply what we learned in lesson 1 to build our first fine-tuning pipeline.\n  Video Lecture 2: Fine-tuning pipeline (Part 2) -  \n[URL]\n#3.  &   -  \nYou will learn:\n- How to transform HTML docs into vector embeddings.\n- How to process data in real-time\n- How to store & retrieve embeddings from a vector DB\n- How to deploy it to AWS.\n  Video Lecture 3: Streaming pipeline  -  \n[URL]\n#4.  \nFinally, you will learn how to use LangChain to glue together your fine-tuned LLM and your financial news stored as embeddings in a vector DB to serve predictions behind a RESTful API.\n   Video Lecture 4: Inference pipeline  -  \n[URL]\n.\nThese 4 lectures are part of the open-source and FREE Hands-on LLM Course by Pau Labarta, Alexandru Rzvan   and myself.\n  Check out the Hands-on LLMs course on GitHub and support us with a :   \n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps."}, {"instruction": "Write a post emphasizing the importance of learning tools like Pulumi and Terraform for AI engineers to build better AI applications and stay versatile in a rapidly evolving tech landscape.", "content": "              .\n  But that's not all there is to it. I've also learned that knowing how to use tools like\nPulumi\nand Terraform is super important.\n  In a world where everything moves at lightning speed, the ability to quickly spin up        is crucial. It's about swiftly simulating reality to test our ideas in real-world conditions.\n        \nPulumi\n,  '      !\nBoth Terraform and Pulumi are fantastic IaC tools, but for my projects, Pulumi offered some key advantages:  \n :  ,      (),        -     , , ,  .\n :      ,   -          .                 .\n :              -    .       ,                .\nIn the world of generative AI, it's more important than ever for us AI engineers to not just focus on AI itself, but also on other key software engineering skills.\nUnderstanding tools like Pulumi or Terraform helps us build better, smarter AI applications.\nAs the tech grows, so must our skills  it's all about staying versatile and ready for anything![URL]"}, {"instruction": "Encourage viewers to watch the short interview discussing the journey into MLE or MLOps and emphasize the importance of experiences and finding what one enjoys in life.", "content": "If you want to break into MLE or MLOps, check out this short interview where I briefly discussed my journey's ups and downs.\nThese kinds of videos are great to see that we are all people.\nWe are not perfect.\nWe make mistakes.\nWe have to experience many things before finding what we really enjoy in our lives.\nThank you,\nHopsworks\nand\nRik Van Bruggen\n, for inviting me. It was a great experience  \nCheck it out to find out more about my journey  \nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Create a post highlighting the importance of thinking like a copywriter for intensifying the entire business strategy, focusing on asking key questions to convey the desired message and evoke specific feelings. Include relevant hashtags such as #marketing, #contentmarketing, and #copywriting. Provide a link for further reading.", "content": "Dac gndeti ca un copywriter, i poi intensifica ntreaga strategie de afaceri.\nUn copywriter pune ntrebri. Care este cel mai important lucru de spus aici? Ce vrei s realizeze oamenii dup ce citesc acest text? Cum vrei s se simt oamenii? Copywriterii buni intr n capul cititorului ideal, la fel cum tu doreti s intri n capul clientului tu ideal.\nGndete ca un copywriter i vei ti exact ce s le transmii oamenilor pe care i serveti.\nCitete mai multe aici:\n[URL]\nhashtag\n#\nmarketing\nhashtag\n#\ncontentmarketing\nhashtag\n#\ncopywriting"}, {"instruction": "Always consider the hardware limitations and capabilities when optimizing DL models for inference. Batch size optimization may vary depending on the GPU model and its specifications. Experiment with different batch sizes to find the sweet spot for improved performance.", "content": "This is how I FAILED to optimize the inference of my DL models when running them on a Nvidia GPU. Let me tell you what to avoid  \nI had a simple task. To reduce the latency of the DL models used in production.\nWe had 4 DL models that were running on Nvidia GPUs.\nAfter a first look at the inference code, I saw that the inputs to the models weren't batched.\nWe were processing one sample at a time.\nI said to myself: \"Ahaa! That's it. I cracked it. We just have to batch as many samples as possible, and we are done.\"\nSo, I did just that...\nAfter 2-3 days of work adding the extra batch dimension to the PyTorch preprocessing & postprocessing code, I realized I WAS WRONG.\nHere is why\n \nWe were using Nvidia GPUs from the A family (A6000, A5000, etc.).\nAs these GPUs have a lot of memory  (>40GB), I managed to max out the VRAM and squash a batch of 256 images on the GPU.\nRelative to using a \"batch = 1\" it was faster, but not A LOT FASTER, as I expected.\nThen I tried batches of 128, 64, 32, 16, and 8.\n...and realized that everything > batch = 16 was running slower than using a batch of 16.\n  A batch of 16 was the sweet spot.\nBut that is not good, as I was using only ~10% of the VRAM...\nWhy is that?\nThe Nvidia A family of GPUs are known to:\n- having a lot of VRAM\n- not being very fast (the memory transfer between the CPU & GPU + the number of CUDA cores isn't that great)\nThat being said, my program was throttled.\nEven if my GPU could handle much more memory-wise, the memory transfer & processing speeds weren't keeping up.\nIn the end, it was a good optimization: ~75% faster\nBut the lesson of this story is:\n  ALWAYS KNOW YOUR HARDWARE \nMost probably, running a bigger batch on an A100 or V100 wouldn't have the same problem.\nI plan to try that.\nBut that is why...\n               !\n.\nIn theory, I knew this, but it is completely different when you encounter it in production.\nLet me know in the comments if you want more similar stories on \"DO NOTs\" from my experience.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily content on production ML and MLOps engineering.[URL]"}, {"instruction": "Encourage readers to follow the Medium's Decoding ML publication for updates on future lessons of the Hands-on LLMs course by the end of January 2024.", "content": "Do you want to learn to build hands-on LLM systems using good LLMOps practices? A new Medium series is coming up for the Hands-on LLMs course\n.\nBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.\nWe will primarily focus on the engineering & MLOps aspects.\nThus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks.\nThere are 3 components you will learn to build during the course:\n- a real-time streaming pipeline\n- a fine-tuning pipeline\n- an inference pipeline\n.\nWe have already released the code and video lessons of the Hands-on LLM course.\nBut we are excited to announce an 8-lesson Medium series that will dive deep into the code and explain everything step-by-step.\nWe have already released the first lesson of the series  \nThe LLMs kit: Build a production-ready real-time financial advisor system using streaming pipelines, RAG, and LLMOps:  \n[URL]\n  In Lesson 1, you will learn how to design a financial assistant using the 3-pipeline architecture (also known as the FTI architecture), powered by:\n- LLMs\n- vector DBs\n- a streaming engine\n- LLMOps\n.\n  The rest of the articles will be released by the end of January 2024.\nFollow us on Medium's Decoding ML publication to get notified when we publish the other lessons:  \n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"}, {"instruction": "Provide a detailed guide on how to write a streaming retrieval system for RAG on social media data, emphasizing the importance of real-time responses and the components involved such as the streaming ingestion pipeline and retrieval client.", "content": "Batch systems are the past. Here is how to write a streaming retrieval system for RAG on social media data  \nWhy streaming over batch?\nIn environments where data evolves quickly (e.g., social media platforms), the system's response time is critical for your application's user experience.\nThat is why TikTok is so addicting. Its recommender system adapts in real-time based on your interaction with the app.\nHow would it be if the recommendations were updated daily or hourly?\nWell, it would work, but you would probably get bored of the app much faster.\nThe same applies to RAG for highly intensive data sources...\n  where you must sync your source and vector DB in real time for up-to-date retrievals.\nLet's see how it works.\n \nI wrote an article on how to build a real-time retrieval system for RAG on LinkedIn data in collaboration with\nSuperlinked\n.\nThe retrieval system is based on 2 detached components:\n- the streaming ingestion pipeline\n- the retrieval client\nThe streaming ingestion pipeline runs 24/7 to keep the vector DB synced with the current raw LinkedIn posts data source.\nThe retrieval client is used in RAG applications to query the vector DB.\n  These 2 components are completely decoupled and communicate with each other through the vector DB.\n#1. The streaming ingestion pipeline\n  Implemented in\nBytewax\n- a streaming engine built in Rust (speed& reliability) that exposes a Python interface\nMain flow:\n- uses CDC to add changes from the source DB to a queue\n- listens to the queue for new events\n- cleans, chunks, and embeds the LI posts\n- loads them to a\nQdrant\nvector DB\nand... everything in real-time!\n#2. The retrieval client\n  A standard Python module.\nThe goal is to retrieve similar posts using a variety of query types - e.g., posts, questions, sentences.\nMain flow:\n- preprocess user queries (the same way as they were ingested)\n- search the Qdrant vector DB for the most similar results\n- use rerank to improve the retrieval system's accuracy\n- visualize the results on a 2D plot using UMAP\n.\nYou don't believe me? \nCheck out the full article & code on Decoding ML  \n  A Real-time Retrieval System for RAG on Social Media Data:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Outline the main steps to build a content generation system fine-tuned with your content using a vector DB.", "content": "  ... \nI bet all of you, including me, use ChatGPT to help us write content or brainstorm new ideas.\n      :\n  73% of marketers use AI to some degree\n 31% of non-users plan to adopt within a year\n 46% will embrace AI within two years\nWhat does this mean?\nIdentical content from one user to another means the brand identity will be very affected.\n ?\n your LLM twin for generating posts or articles using your voice  \nThis requires a system fine-tuned with your content, using a vector DB to stick to factual writing and avoid off-track results.\nFrom an     here are the main steps you need to build this:\n  Data Collection: Aggregate your digital footprint from platforms like Medium and LinkedIn, process and store it in MongoDB.\n Change Detection: Use CDC to monitor\nMongoDB\nupdates, channeling these changes to a\nRabbitMQ\nqueue.\n Feature Pipeline: A\nBytewax\npipeline listens to the queue, cleans, and structures the data, supported by\nSuperlinked\n.\n Data Storage: Store processed data in a\nQdrant\nvector DB.\n Training: Create your dataset from cleaned data and fine-tune with\nQwak\na Mistral LLM using QLoRA, tracking experiments with\nComet\n.\n Evaluation and Deployment: Choose the best LLM model based on Comet's evaluation, then deploy it for inference via a REST API.\n  In the following articles and posts, the & Decoding ML team  will show you all the steps to build the LLM Twin, which learns your     by providing unique and valuable content.\nCheck it out on GitHub and support us with a \n \n  LLM Twin Course: Building Your Production-Ready AI Replica GitHub Repository:\n[URL]\nAlso, if you want to learn more about the course check this medium article.\n \n \n[URL]"}, {"instruction": "Create a social media post highlighting excitement for collaborating with an innovative AI company, using hashtags #ai, #datascience, #machinelearning, and include a URL link.", "content": "So excited to work with such an innovative company in the world of AI  \nhashtag\n#\nai\nhashtag\n#\ndatascience\nhashtag\n#\nmachinelearning[URL]"}, {"instruction": "Explain the significance of vector databases in ML applications and the process of integrating them into real-world scenarios using embeddings and payloads.", "content": "Why have vector DBs become so popular & why are they so crucial for most ML applications (not only LLMs)?\nIn the world of ML, everything can be represented as an embedding.\nA vector DB is an intelligent way to use your data embeddings as an index and perform fast and scalable searches between unstructured data points.\nSimply put, a vector DB allows you to find matches between anything and anything (e.g., use an image as a query to find similar pieces of text, video, other images, etc.).\n.\nIn a nutshell, this is how you can integrate a vector DB in real-world scenarios  \nUsing various DL techniques, you can project your data points (images, videos, text, audio, user interactions) into the same vector space (aka the embeddings of the data).\nYou will load the embeddings along a payload (e.g., a URL to the image, date of creation, image description, properties, etc.) into the vector DB, where the data will be indexed along the:\n- vector\n- payload\n- text within the payload\nNow that the embedding indexes your data, you can query the vector DB by embedding any data point.\nFor example, you can query the vector DB with an image of your cat and use a filter to retrieve only \"black\" cats.\nTo do so, you must embed the image using the same model you used to embed the data within your vector DB. After you query the database using a given distance (e.g., cosine distance between 2 vectors) to find similar embeddings.\nThese similar embeddings have attached to them their payload that contains valuable information such as the URL to an image, a URL to a site, an ID of a user, a chapter from a book about the cat of a witch, etc.\n.\nUsing this technique, I used Qdrant to implement RAG for a financial assistant powered by LLMs.\nBut vector DBs go beyond LLMs & RAG.\nHere is a list of what you can build using vector DBs (e.g., Qdrant ):\n- semantic image search:\n[URL]\n- recommender systems:\n[URL]\n- RAG framework:\n[URL]\n- anomalies detection:\n[URL]\n.\n  Check out Qdrant's guides and tutorials to learn more about vector DBs:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps.[URL]"}, {"instruction": "Provide a detailed guide on how to deploy an open-source LLM like Llama2 on AWS SageMaker, highlighting steps like designing a config class, setting up AWS, deploying the LLM, implementing an inference class, defining a prompt template function, and engaging with the deployed LLM, based on Alex Vesa's article on DML.", "content": "Ever wondered how to deploy in <30 minutes open-source LLMs, such as Llama2, on AWS SageMaker? Then wonder no more  \nThe sweet thing about SageMaker is that it accelerates the development process, enabling a more efficient and rapid transition to the production stage.\nAlex Vesa\nsmashed with his first article on DML about showing step-by-step how to deploy an LLM from HuggingFace to AWS SageMaker using good practices, such as:\n- designing a config class for the deployment of the LLM\n- set up AWS and deploy the LLM to SageMaker\n- implement an inference class to call the deployed LLM in real time through a web endpoint\n- define a prompt template function to ensure reproducibility & consistency\n...and, ultimately, how to play yourself with your freshly deployed LLM.\n.\nIf that is something for you, check it out for FREE on Decoding ML  \n   Introduction to Deploying Private LLMs with AWS SageMaker: Focus on Llama2-7b-chat:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps.[URL]"}, {"instruction": "Create a detailed tutorial on building a data pipeline following MLOps best practices. Cover topics such as data collection process, ETL pipelines, ODM techniques, NoSQL databases, CDC pattern, feature pipelines, and tools like Bytewax, Superlinked, and RabbitMQ. Emphasize the importance of each step and provide resources for further learning.", "content": "         . \nHello everybody!  \nWelcome to the 2nd lesson of our  :   -   -  .\nAccess it here:  \n[URL]\nWe are kicking off this lesson by showing you how to build a data pipeline by respecting the best\nhashtag\n#\nMLOps\npractices.\n   is the engine of any ML model.\nIf we dont give enough importance to it, we cannot be in control of the final output.\nThat is why I emphasize this first and very important step.\nWhat's in store for today's lesson \n  Data collection process -> Medium, Github, Substack & Linkedin crawlers\n  ETL pipelines -> data is cleaned and normalized\n  ODM (Object Document Mapping ) -> a technique that maps between an object model in an application and a document database\n  NoSQL Database (\nMongoDB\n) & CDC (Change Data Capture) pattern\n- Tracks data changes log them, and queues messages for real-time system updates\n- The clean data is stored in a NoSQL database\n  Feature Pipeline\n- A streaming ingestion pipeline is part of the feature pipeline that processes -Articles, Posts, and Code.\n- Tools like\nBytewax\nand\nSuperlinked\nare used, likely for further data processing and transformation.\n- This processed data is then queued in\nRabbitMQ\n, a message broker that helps in asynchronous processing and communication between different services.\nThe LLM Twin Challenge STARTS NOW  for the next 9 weeks you are embarking on a hands-on experience that will bring another skill to your portfolio: deploying an end-to-end production-grade LLM system.\nIf you want to learn more about the technologies used in this lesson check out:\n  :\n[URL]\n   :\n[URL]\n   :\n[URL]\n Steal our code:\n \n[URL]"}, {"instruction": "Guide on prompt engineering for better AI and MLOps: 1. Provide high-quality demonstrations with input and output examples. 2. Sample multiple outputs with 'temperature > 0' and select the best. 3. Use CoT to force LLM's explanation process. 4. Utilize LLM as a reasoning engine for information retrieval. 5. Stick to one task/prompt to avoid confusion. 6. Explain tasks step-by-step as if to a 7-year-old with varying prompt detail based on model complexity.", "content": "The whole field of prompt engineering can be reduced to these 6 techniques I use almost daily when using ChatGPT (or other LLMs).\nHere they are  \n#1.   \nAdd in your prompt 2 or 3 high-quality demonstrations, each consisting of both input and desired output, on the target task.\nThe LLM will better understand your intention and what kind of answers you expect based on concrete examples.\n#2. - \nSample multiple outputs with \"temperature > 0\" and select the best one out of these candidates.\nHow to pick the best candidate?\nIt will vary from task to task, but here are  2 primary scenarios  \n1. Some tasks are easy to validate, such as programming questions. In this case, you can write unit tests to verify the correctness of the generated code.\n2. For more complicated tasks, you can manually inspect them or use another LLM (or another specialized model) to rank them.\n#3. -- ()\nYou want to force the LLM to explain its thought process, which eventually leads to the final answer, step by step.\nThis will help the LLM to reason complex tasks better.\nYou want to use CoT for complicated reasoning tasks + large models (e.g., with more than 50B parameters). Simple tasks only benefit slightly from CoT prompting.\nHere are a few methods to achieve CoT:\n- provide a list of bullet points with all the steps you expect the LLM to take\n- use \"Few shot prompt\" to teach the LLM to think in steps\n... or my favorite: use sentences such as \"Let's think step by step.\"\n#4.  \nThe LLM's internal knowledge is limited to the data it was trained on. Also, often, it forgets specific details of older training datasets.\nThe most common use case is Retrieval-Augmented Generation (RAG).\nThat is why using the LLM as a reasoning engine is beneficial to parse and extract information from a reliable source of information given as context in the prompt.\nWhy?\n- avoid retraining the model on new data\n- avoid hallucinating\n- access to references on the source\n#5.     \nQuite self-explanatory. It is similar to the DRY principle in SWE.\nHaving only x1 task/prompt is good practice to avoid confusing the LLM.\nIf you have more complex tasks, split them into granular ones and merge the results later in a different prompt.\n#6.     \nThe LLM cannot read your mind. To maximize the probability of getting precisely what you want, you can imagine the LLM as a 7-year-old to whom you must explain everything step-by-step to be sure he understood.\nNote: The level of detail in the prompt is inversely proportional to the size & complexity of the model.\n.\nThe truth is that prompt engineering is quite intuitive, and we don't have to overthink it too much.\nWhat would you add to this list?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"}, {"instruction": "Discuss the role differentiators between a backend engineer and an AI engineer in the context of MVP setting and advanced product development.", "content": "Backend Engineering IS NOT AI Engineering. \n          ?\nI was asked this question yesterday at an interview, and my mind froze for a couple of minutes.\nI realized that working in multiple startups has made me wear many hats   from software and backend engineering to AI expertise and even the CTO role. This journey has taught me the art of adaptability, always ready to jump into any challenge that comes my way.\nBut that doesnt mean thats the only path of evolution you can have as an engineer. There are also drawbacks to following a multi-perspective technical journey:\n You get into too many roles and responsibilities and forget the details;\n You get distracted by soaking up a lot of information in different domains;\n Sacrifice quality to accumulate quantity.\nFinally, my answer to this interview was a personal experience opinion.\nTheres a fine line between backend and AI engineering, and it depends on the following question:\n         \nIn an MVP setting, blending backend and AI engineering is more common. Quick, functional prototypes are key, requiring a broad skill set.\nHowever, for an advanced product, the distinction is clearer.\n Backend engineering focuses on building a robust, scalable infrastructure - the foundation essential for AI implementation.\n AI Engineering, meanwhile, involves developing intelligent algorithms that leverage this infrastructure, transforming data into actionable insights and advanced features.\nThere's indeed an overlap in technical skills, but backend and AI engineering divide in their core objectives and contributions to the products lifecycle.\nThe backend lays the groundwork,  while AI brings the innovation.\n     !\nWhat do you consider to be a role differentiator between a backend engineer and an AI engineer?\nhashtag\n#\naiengineering\nhashtag\n#\nartificialintelligence\nhashtag\n#\naiengineer[URL]"}, {"instruction": "Write a social media post promoting AIMINDED's crowdfunding campaign, emphasizing the ease of getting involved with just a single click and using relevant hashtags.", "content": "Check out our crowdfunding campaign.\n You can be part of\nAIMINDED\nvision with just a single click !\nhashtag\n#\nartificialintelligence\nhashtag\n#\ncrowdfunding[URL]"}, {"instruction": "Create a social media post featuring a 5-minute interview with Paul Iusztin from Metaphysic.ai discussing machine learning and artificial intelligence. Encourage audience participation and feedback. Include the provided URL.", "content": "Happy Friday!\nHere's another\nHopsworks\n5-minute\nhashtag\n#\ninterview\nfor you, with some great insights, commentary and content from our\nhashtag\n#\ncommunity\n. This time I am speaking to\nPaul Iusztin\n- who is sharing his experience with\nhashtag\n#\nmachinelearning\nand\nhashtag\n#\nartificialintelligence\nat\nMetaphysic.ai\n. Hope you like the chat - and comments always welcome!\n[URL]"}, {"instruction": "Provide detailed instructions on setting up a real-time data pipeline using Upstash for serverless key-value database, Kafka for data distribution, and Pydantic for structuring data.", "content": "Are you exploring serverless solutions for real-time data processing?\nWith\nUpstash\n, a serverless key-value database, we were able to:\n  Simplify our infrastructure setup\n  Scale effortlessly with demand\n  Ensure reliable data management\nThis producer logic diagram illustrates how we fetch news data from multiple sources, process it through ingestion threads, and then send it to Kafka for distribution. The entire setup is streamlined, thanks to Upstash's serverless architecture.\n'         :\n  Each data source has its own ingestion thread to fetch, parse, and format the news content.\n The data is structured with Pydantic, providing a consistent format with fields like author, content, title, and date.\n The Kafka Common Producer manages the data flow, sending it to the Kafka Cluster, where it's organized into various topics.\nThis   allows us to handle real-time news data easily and ensures a scalable backend infrastructure.\nIf you're interested in building a real-time data pipeline, this setup can be a game-changer.\nBut where is the code?\n  Check it out on:\n[URL]\n \n  Full article:\n[URL]"}, {"instruction": "Provide a step-by-step mini-guide on building a Social Media Content Generator using OpenAI's GPT API, focusing on designing scalable and effective prompt templates for diverse clientele.", "content": "How to build a Social Media Content Generator that works\nI am sharing my step-by-step mini-guide thats going to help you build a  Social Media Content Generator.\nOne that works!  \nWriting instructions for OpenAI's GPT API might seem straightforward, but the real challenge lies in designing scalable, effective prompt templates, especially for a diverse clientele.\nI wanted to give a practical example from my professional experience, so I picked  \nBookingham\n, a dynamic booking platform that collaborates with hundreds of restaurants.\nAnd man, that content needs to be provided continuously, on a variety of social media channels and needs to keep various writing styles, tones of voice, and brand statements.\n So heres the challenge: How to craft individualized and captivating social media content for each restaurant, scalable across hundreds of clients?\n Below you can tap into my\nhashtag\n#\nstrategy\nmind-map: A double-perspective approach  Business Discovery and Product Discovery, leading to the development of versatile prompt templates.\n After I take you into the steps for business discovery and product discovery, I am unveiling my 3 staple phases in which I develop my AI product and its way of thinking and writing. \n I am also sharing a diagram example I use that will guide you through the whole given example for Bookingham.\nRemember! This is the development phase. You need to test everything, according to your industry and type of content generator. I keep that as a future topic.\nLet me know if this is helpful for you!\nhashtag\n#\nAI\nhashtag\n#\nOpenAI\nhashtag\n#\nDigitalMarketing\nhashtag\n#\ncontentgenerator\nhashtag\n#\naicontent\nhashtag\n#\naiproducts[URL]"}, {"instruction": "Encourage team members to ask questions and seek clarification to enhance learning and promote discussion.", "content": "Here is one thing that I do that sets me apart from the crowd:\n\"I am okay with being the dump one that asks many questions.\"\n... ?\nThe reality is that even the brightest minds cannot understand everything from the first shot.\nIt is not necessarily that you cannot understand the concepts.\nThere are other factors, such as:\n- you are tired\n- you haven't paid enough attention\n- the concept wasn't explained at your level\n- the presenter wasn't clear enough, etc.\nAlso, the truth is that many of us don't understand everything from the first shot when presented with a new concept.\nBut because of our ego, we are afraid to come out and ask something because we are worried that we will sound stupid.\nThe jokes are on you.\nMost people will be grateful you broke the ice and asked to explain the concept again.\n?\nIt will help the team to learn the new concepts better.\nIt will start a discussion to dig deeper into the subject.\nIt will piss off or annoy the people you don't like.\nIt will help other people ask questions next time.\nIt will open up new perspectives on the problem.\nTo conclude...\nIgnore your ego and what people think of you. Own your curiosity and ask questions when you feel like it.\nIt is ok not to know everything.\nIt is better to be stupid for 5 minutes than your entire life.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlop\nhashtag\n#\npersonaldevelopment\n.\n  Follow me for daily lessons about ML engineering and MLOps."}, {"instruction": "Refer to the step-by-step guide provided to build a production LLM system with 4 Python microservices using MLOps & the 3-pipeline architecture, and check the code on Decoding ML's GitHub repository for the implementation details.", "content": "Step-by-step guide to building a production LLM system with 4 Python microservices using MLOps & the 3-pipeline architecture\nAs an example, you will build a system for an LLM Twin.\nBut you can use the same strategy for most use cases.\nHere is the architecture of our LLM Twin  \n1. Data Collection Pipeline + CDC\nThe data collection pipeline will crawl your digital data (only yours) from various platforms, such as Medium, Substack, LinkedIn, and GitHub.\n- An ETL pipeline for every platform built with selenium to crawl the data and Beautiful Soup to parse the HTML\n- The data is loaded into a MongoDB normalized as posts, articles or code\nUsing CDC, a watcher will listen to changes made to the MongoDB.\nAn event will be added to a RabbitMQ queue during any CRUD operation.\n2. ML Pipelines\nHere, we will leverage the 3-pipeline architecture and split the ML system into 3 pipelines:\nThe feature pipeline\n  A streaming ingestion pipeline using\nBytewax\nas its streaming engine\nInput: queue\nOutput:\nQdrant\n(feature store)\n- listens to the RabbitMQ queue\n- cleans the data and creates a snapshot into Qdrant used for fine-tuning\n- chunks and embeds the data for a second snapshot into Qdrant used for RAG\nThe training pipeline\n  A fine-tuning pipeline using open-source LLMs from\nHugging Face\nInput: Qdrant (feature store)\nOutput: fine-tuned LLM stored in\nComet\n's Model registry\n- uses the retrieval client to access your articles, posts, or code\n- uses ChatGPT to create a custom Q&A dataset\n- fine-tunes the LLM using QLoRA\n- logs all the metrics using Comet 's model registry\n- picks the best LLM and publishes it to the model registry as the production candidate\n- a second evaluation step is performed using an LLM evaluation tool\n- if it passes, the LLM is flagged as accepted and deployed to the inference pipeline\nThe inference pipeline\n-> Acts as the RAG real-time client\nInput: fined-tuned LLM from the model registry\nOutput: real-time predictions\n- loads & quantized the LLM from the model registry\n- accepts client requests using a REST API endpoint\n- augments the prompt using RAG\n- calls the LLM Twin & logs everything to a prompt monitoring tool\n.\nOk, ok, that sounds great...\nBut where is the code?\nCheck it out on\nDecoding ML\n's GitHub repository\n \n  LLM Twin Course:\n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience[URL]"}, {"instruction": "Write a reflective and engaging blog post on finding a balance between passion, money, and perspectives for personal and professional growth, emphasizing the importance of knowledge and connections in moving forward.", "content": "How to become a millionaire in 2024? A Sunday perspective\nNo, this is DEFINITELY not a guide to making your first million . I'm not a fan of that kind of script.\nI once had a mentor who told me this:  ,     .\nNow, Im being honest, this   fast-paced era we are living in is crazy sometimes (I say that even if Im an innovation enthusiast).\nWe are caught up in a ping-pong of offers, choices, opportunities, and of coursemoney.\nIm going to ask you 2 questions:\nIs every option an opportunity?\nWhere does  I want more? stand, and when do you conclude that  Its enough.\nBeing young and ambitious, Ive always searched for more.\nNot necessarily more money, but building knowledge and working with different people in different areas of my domain.\nDiscovery is the best pleasure and feeling great at what you do. And thats the feeling youre probably searching for too, deep down.\nOver the years I realized that there must be a balance between money, passion, and perspectives.\nIm a little bit subjective because Im obsessed with building and creating things with people that inspire me. I sometimes tend to sacrifice money for the greater good, because, in the end, I just want a balance that keeps me going, that keeps my drive on a high note! \nHere are my top inner reality - checks :\n Does it affect my inner peace/energy?\n How does it contribute to my personal and professional development?\n Does it help me upgrade my weak points?\nI encourage everyone to take as many opportunities as they can, but in the end, the only thing that will help you move forward is knowledge and people.\n       : , ,  .\nIm curious.\nFor you, where is the fine line between Its enough and I want more [URL]"}, {"instruction": "Post about Industry Day at SSIMA Re:Imagine Healthcare with a focus on AI in Radiotherapy and the value it brings in the cancer treatment process, highlighting the pitch and Q&A session at 7 PM. Use hashtags: #ai #healthcare #startup #medtech #artificialintelligence #radiotherapy", "content": "It's Industry Day at\nSSIMA Re:Imagine Healthcare\nand we are enjoying some awesome technology talks!\nAt 7 PM we are holding a pitch and Q&A session about AI in  Radiotheraphy and the value it brings in the cancer treatment process.\n  We are ready to expand our vision with a great public in front of us.\nStay tuned!\nhashtag\n#\nai\nhashtag\n#\nhealthcare\nhashtag\n#\nstartup\nhashtag\n#\nmedtech\nhashtag\n#\nartificialintelligence\nhashtag\n#\nradiotherapy"}, {"instruction": "Provide clear examples of the best ways to interact with AI tools, demonstrate how inputs can be tweaked for improved outputs, and enable easy user feedback to enhance the system's performance. Ensure transparency by informing users when interacting with an LLM (Large Language Model), cite sources when possible, and design the UI to subtly guide users towards optimal inputs while encouraging creativity. Include a link for users to access additional information about Responsible AI.", "content": "             ?\nCreating a -  () is crucial for non-technical users navigating AI tools.\nThis is where Responsible AI comes in  ensuring AI systems are intuitive and accessible to everyone.\n Imagine trying to use complex tech with no instructions  that's what using AI tools without proper UX can feel like, especially for non-technical users.\n    ,     -     :\n   -: Share clear examples of the best ways to interact with the AI tool.\n   : Show users how tweaking their inputs gets even better AI outputs.\n   : Make it easy for users to provide feedback, actively improving the system.\n'      :\n  : Let people know when they're interacting with an LLM (Large Language Model). This manages expectations and builds trust.\n, '  : Can the LLM cite its sources or reveal its thought process? This boosts confidence in the results.\n,  ' : Design the UI to subtly suggest good inputs, while still leaving room for creativity.\n    /          - ?[URL]"}, {"instruction": "Write a post highlighting the importance of collaboration in the AI community, referencing NTT DATA Romania for their role in fostering a collaborative ecosystem.", "content": "It's not about winning. It's about gathering together great minds from the same ecosystem.\nThanks,\nNTT DATA Romania\nfor making us part of this ecosystem."}, {"instruction": "Write a post highlighting the successful participation of AIMINDED in the SSIMA event, emphasizing the live showcase of their product at Oradea Hospital and their mission in the healthcare system.", "content": "AIMINDED\nis at SSIMA!\nIt feels amazing to be attending a great environment of MedTech experts and presenting our StartUp in front of an inspirational audience of researchers.\n We will open the event tomorrow, with a LIVE showcase of our product at Oradea Hospital, and in the evening we will discuss AIMINDED's mission and vision in the healthcare system.\nArtificial Intelligence saves lives, and we are here to prove it!"}, {"instruction": "Encourage readers to follow your new Medium publication 'Decoding ML' for upcoming surprises in January 2024, and engage with the content by supporting and following the author for daily lessons about ML engineering and MLOps.", "content": "Do you want to level up your production-ready MLE & MLOps game?\nThen I have some great news  \nSince I started creating content, I learned one crucial thing: \"Everybody likes to read and learn differently.\"\nThat is why I started my own Medium publication, called after my newsletter: \"Decoding ML\"\nStarting from now, all my articles can be found under this Medium publication:  \n[URL]\nCurrently, it is empty, but in January 2024, I plan to drop 9 surprises in my new Decoding ML Medium publication.\nIf you have liked my content so far...\n-> Support me and follow my new publication as you will enjoy what I prepared:  \n[URL]\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps."}, {"instruction": "Outline the blueprint of the system architected for evaluating LLM output in a production environment.", "content": "  You dont need to evaluate the LLM tasks.\nIt was my first thought when I started to design a complex system based on 3 tasks: summarization, classification, and entity extraction.\n With big clients and thrilling projects on the horizon, the weight of responsibility grew heavier. It prompted a hard question:\nHow can I guarantee a workflow that evaluates the output of the Large Language Model(LLM ) system effectively and progresses toward autonomy over time?\n   Heres the blueprint of the system I architected:\n Step 1: Document Digitization The documents are digitalized using an OCR system, extracting the relevant information like text, pages, and tables.\n Step 2: AI System Integration with Llama2-7B Core:\nAI Summarization: Efficiently condenses complex information, ensuring key insights remain intact.\nAI Entity Extraction: Accurately identifies and categorizes over 20 distinct entities within the documents.\nAI Classification: Classify the document based on its label (invoice, contract, and many others)\n Step 3: Data Storage and Refinement The refined data is then securely housed in Amazon DynamoDB, ensuring easy access and retrieval while maintaining data integrity.\n Step 4: Streamlit and Human Evaluation Interface To add a layer of quality assurance, we introduced the Streamlit Data Labeling Interface. It serves as a platform for human evaluators to oversee and validate the AI's conclusions, combining the precision of human oversight with the efficiency of AI.\n Step 5: LLM Evaluator Integration A Large Language Model (LLM) Evaluator is deployed to cross-examine the AI's work. This dual-layer evaluation - both human and AI-driven - ensures our system's output stands up to the highest quality. The end goal here is to have an autonomous LLM evaluator that can run periodically and tell us if something wrong is happening.\n Step 6: Refined Data\nHuman-refined data is stored for ongoing system enhancement, focusing on fine-tuning and RAG applications.\nIn conclusion, the key takeaway from our system's development underscores the indispensable role of human intervention in evaluating LLM outputs within a production environment. Maintaining a periodic check on these outputs is crucial for ensuring the gradual progression of our LLM evaluator towards autonomy.\nIm curious. How do you evaluate the LLM output in a production environment?[URL]"}, {"instruction": "Explore Bytewax for building streaming pipelines in LLM projects, leveraging its central flow and diverse data handling capabilities. Use Pydantic models for data validation and explore the built-in Rust for speed, Python bindings, plug-and-play features, and out-of-the-box connectors for Kafka. Check the provided URLs for additional resources.", "content": "Streaming Pipelines - the cherry on top of a LLM project \nLLM projects often deal with a massive, never-ending stream of data  think social media feeds, news updates, or code repositories.\n   is built to handle this constant flow, preventing your LLM from blocking on huge data dumps. It processes and embeds information on the fly, keeping your model up-to-date.\nBytewax\noffers a central streaming flow, like the \"graph\" of your pipeline.\nThink input() -> process() -> output().\n  In my case, I ingested posts, articles, and code from RabbitMQ, cleaned them, chunked them, and embedded them for a\nQdrant\nvector DB (feature store).\n    \nThe beauty of\nBytewax\n? It handles diverse data types.\nWe use a dispatcher to ensure posts, articles, and code are processed differently.\nPydantic\nmodels ensure data validation at each step.  \nWhy the streaming pipeline with\nBytewax\n?  \n   : Built-in Rust for lightning speed!\n   : Python bindings for all your favorite ML libraries.\n    : Plug-and-play, perfect for notebooks and projects.\n   :   Out-of-the-box connectors for Kafka and more (or build your own!).\nIf you're curious to level up your knowledge about streaming pipelines and data engineering  \nCheck out        . It's FREE, and no registration is required\n \n  Lesson 4 -\n[URL]\n  LLM Twin Github Repository -\n[URL]"}, {"instruction": "Create an engaging post sharing your experience transitioning from PyCharm to VS Code, highlighting the superior SSH remote work integration and the ability to sync settings to your account. Mention the minimal customization needed for VS Code and emphasize the convenience of installing extensions just once. Conclude with a lineup of hashtags including machinelearning, mlops, datascience, and programming along with a relevant URL.", "content": "I hate customization. I want to be productive everywhere, anywhere.\niPhone > android\nMacbooks > Windows, Linux\nPyCharm > VS Code\nJoking.\nI was a PyCharm fanboy but switched to VS Code due to their superior SSH remote work integration.\nSince then, I have entirely ditched PyCharm.\nInteresting fact: VS Code doesnt need that much customization to make it work + you can sync all your settings to your account (so installing all the required extensions is just a one-time thing)\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\nhashtag\n#\nprogramming[URL]"}, {"instruction": "Create a post discussing Sam Altman's 3 great tips about learning & making decisions, emphasizing the importance of compounding efforts in one's career, knowing when to stick or quit, and sustaining motivation over time. Encourage followers to reflect on how these principles apply to their own lives and include hashtags #machinelearning, #mlops, and #datascience. Finish the post by inviting readers to follow for daily lessons about ML engineering and MLOps.", "content": "Here are 3 great tips from Sam Altman about learning & making decisions that can benefit you to level up your MLE or MLOps game.\nHe mainly referred to \"business\" decisions, but I think they can significantly be transferred to pursuing any goal.\n            .\n1. Compound Interest in Your Career\nThink about it like this - your career is much like your retirement fund. The more you put in early, you'll have to chill on later. It's not just adding up; it's multiplying.\n- Early career efforts have a compounding effect.\n- It's about working smart and persistently.\n- This stage sets your career trajectory.\n2. Knowing When to Stick or Quit\nSometimes, it's hard to tell if you need to keep pushing or if it's time to change things up. But don't let all the noise around you make that decision. Listen to your gut and remember, it's okay to pivot when you're out of ideas and nothing works.\n- Quitting too soon is a common mistake.\n- The decision to continue should be internal.\n- Pivot when you're out of ideas, not just because of external opinions.\n3. Sustaining Motivation Over Time\nFind what gets you up in the morning and keeps you grinding late into the night. Trust me, it's better to be fueled by your passion for your work rather than dreams of fame and fortune.\n- Intrinsic motivation is critical for longevity.\n- Success needs belief in your work and enjoying it.\n- Initial motives (fame, money) often evolve into deeper missions.\n.\nSo, what about you? How do you see these principles playing out in your life?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily lessons about ML engineering and MLOps."}, {"instruction": "Provide a detailed guide on achieving real-time capabilities for a RAG system by considering components like scalability, data synchronization, data processing, and adapting to diverse data sources. Highlight Paul Iusztin's article in Decoding ML that demonstrates designing a real-time retrieval system using a real-world LinkedIn example and advanced techniques such as real-time data sync and preprocessing.", "content": "This is how you can achieve real-time capabilities of a RAG system.\nAlways, the first step is to think what components are needed for the system.\nThe system design is one of the most important steps in any software engineering process.\nAlways, build the system with these things in mind:\n  Scalability: The system's architecture is built with scalability in mind, ensuring it can handle growing data volumes and user queries without performance degradation\n  Data Synchronization\n  Data processing: The design includes efficient data ingestion, cleaning, chunking, and embedding processes\n Adapts to Diverse Data Sources: The modular and flexible nature of the system design makes it easy to extend and adapt to various data sources beyond social media, underscoring its versatility.\nPaul Iusztin\nwrote an amazing and very detailed article in Decoding ML where he:\n-Demonstrated with a real-world example(Linkedin posts) how to design a real-time retrieval system\nHere is what you will learn  \nReal-time streaming pipeline for up-to-the-minute data sync  \nAdvanced preprocessing for LinkedIn posts to enhance data quality \nA retrieval client that queries with high precision  \nAccuracy boost using a rerank pattern and visualization with UMAP for deeper insights  \n The magic of this article lies in the adaptable framework and patterns we've presented, which can effortlessly be extended beyond social media data to embrace a wide array of data types.\nCheck it out on Decoding ML\n \n[URL]"}, {"instruction": "Encourage engagement by replying with a thank you message and inviting them to share their thoughts or questions.", "content": "Folow them . They are awesome !"}, {"instruction": "Explain the difference between encoders, decoders, and encoder-decoder LLMs, and provide use cases for each.", "content": "The difference between encoders, decoders and encoder-decoder LLMs.\nEmbeddings are everywhere... both encoders and decoders use self-attention layers to encode word tokens into embeddings.\nLet's see when to use each architecture  \nThe key difference between an encoder & decoder is in how it processes its inputs & outputs.\n=== Encoders ===\nThe role of an encoder is to extract relevant information from the whole input and encode it into an embedding (e.g., BERT, RoBERTa).\nWithin the \"Multi-head attention\" of the transformer, all the tokens are allowed to speak to each other.\nA token at position t can talk to all other previous tokens [0, t-1] and future tokens [t+1, T]. This means that the attention mask is computed along the whole vector.\nThus, because the encoder processes the whole input, it is helpful for classification tasks (e.g., sentiment analysis) and creates embeddings for clustering, recommender systems, vector DB indexes, etc.\n=== Decoders ===\nOn the flip side, if you want to generate text, use decoder-only models (e.g., GPT family).\nOnly the current and previous tokens (not the whole input) are used to predict the next token.\nWithin the \"Masked Multi-head attention,\" the future positions are masked to maintain the autoregressive property of the decoding process.\nFor example, within the \"Masked Multi-head attention,\" instead of all the tokens talking to each other, a token at position t will have access only to previous tokens at positions t-1, t-2, t-3, ..., 0.\n=== Encoder-decoder ===\nThis technique is used when you have to understand the entire input sequence (encoder) and the previously generated sequence (decoder -> autoregressive).\nTypical use cases are text translation & summarization (the original transformer was built for text translation), where the output heavily relies on the input.\nWhy? Because the decoding step always has to be conditioned by the encoded information. Also known as cross-attention, the decoder queries the encoded information for information to guide the decoding process.\nFor example, when translating English to Spanish, every Spanish token predicted is conditioned by the previously predicted Spanish tokens & the entire English sentence.\n.\nTo conclude...\n- a decoder takes as input previous tokens and predicts the next one (in an autoregressive way)\n- by dropping the \"Masked\" logic from the \"Masked Multi-head attention,\" you process the whole input, transforming the decoder into an encoder\n- if you hook the encoder to the decoder through a cross-attention layer, you have an encoder-decoder architecture\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n  Follow me for daily content on ML and MLOps engineering.[URL]"}, {"instruction": "Discuss the challenges in choosing the best-fitting OCR system based on text extraction quality, speed, and hardware requirements. Share insights on balancing speed and accuracy in OCR solutions. Encourage engagement by asking about similar challenges faced in document processing.", "content": "Is it hard for you to choose the best-fitting OCR system, based on the text extraction quality, hardware requirements, and processing time constraints? \nThere are multiple open-source libraries and OCRs like   PyPDF, Tesseract or EasyOCR.\nRegarding speed, flexibility, and HW constraints, the first choice is the PyPDF loader from @langhcain or the PyPDF module directly in our Python code which runs a document in milliseconds on the CPU.\nBut have you tested the quality of text extraction? As AI engineers, its our job to check the quality of this task.\n PyPDF fails to extract the text for scanned or low-quality documents.\nSelecting the right OCR tool involves balancing speed and accuracy. PyPDF excels in speed but may struggle with quality in certain documents.\n  A Readability Algorithm can be a game-changer in deciding what OCR we should use.\n Introducing the Readability Algorithm:\n1. Merge & Standardize: It consolidates OCR text fragments and standardizes them, preparing the text for analysis.\n2. Detect Non-Text Elements: The algorithm identifies non-textual characters, crucial for assessing text purity.\n3 . Red Flags: It checks for indicators of unreadability, adding depth to the analysis.\n4. Weigh & Score: Each aspect is evaluated and scored to determine text readability.\n5. Final Verdict: This score is compared against a set threshold to decide if the text is readable.\n  Empirical thresholds guide your algorithm in routing documents. Clear texts are stored in your S3 bucket for immediate use. Those failing the test are escalated to Pytesseract or EasyOCR for more intensive processing.\nThis approach ensures you don't just process documents rapidly; you ensure their quality remains accurate.\nWe're not just digitizing text; we're curating knowledge with precision.\n  Your Thoughts?\nAre you facing similar challenges in document processing?\nHow do you balance speed and accuracy in your OCR solutions?\nLet's discuss this in the comments below![URL]"}]