[{"instruction": "Build a data pipeline for aggregating data from various social media platforms such as Medium, LinkedIn, Github, and Substack to collect organized, clean, and normalized data in Lesson 2 of the course.", "content": "The Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataData Data Pipeline Engineering for production-ready Generative AI | Hands-on course  | Decoding MLOpen in appSign upSign inWriteSign upSign inLLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICAThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable dataVesa AlexandruFollowPublished inDecoding ML17 min readMar 23, 20244343ListenShare  the 2nd out of 11 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the LLM Twin: Building Your Production-Ready AI Replica free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care?   No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish  from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.What is an LLM Twin? It is an AI character that learns to write like somebody by incorporating its style and personality into an LLM.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet MLs experiment tracker to monitor the experiments. Evaluate and save the best model to Comets model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comets model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comets prompt monitoring dashboard (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps sound principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 11 hands-on written lessons and the open-source code you can access on GitHub.You can read everything at your own pace.  To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you must know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps Engineer  Check out the code on GitHub [1] and support us with a LessonsThe course is split into 11 lessons. Every Medium article will be its lesson.An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe importance of Data Pipelines in the era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG  in Real-Time!Vector DB retrieval clients [Module 2] WIPTraining data preparation [Module 3] WIPFine-tuning LLM [Module 3] WIPLLM evaluation [Module 4] WIPQuantization [Module 5] WIPBuild the digital twin inference pipeline [Module 6] WIPDeploy the digital twin as a REST API [Module 6] WIP  Check out Lesson 1 to better understand the courses goal, technical details and system design.Lets start with Lesson 2  Lesson 2: The Importance of Data Pipelines in the Era of Generative AIWe have data everywhere. Linkedin, Medium, Github, Substack, and many other platforms.To be able to build your Digital Twin, you need data.Not all types of data, but organized, clean, and normalized data.In Lesson 2, we will learn how to think and build a data pipeline by aggregating data from:MediumLinkedinGithubSubstackWe will present all our architectural decisions regarding the design of the data collection pipeline for social media data and why separating raw data and feature data is essential.In Lesson 3, we will present the CDC (change data capture) pattern, a database architecture, and a design for data management systems.CDC's primary purpose is to identify and capture changes made to database data, such as insertions, updates, and deletions.It then logs these events and sends them to a message queue, like RabbitMQ. This allows other system parts to react to the data changes in real-time by reading from the queue, ensuring that all application parts are up-to-date.We will go into detail in Lesson 3.Data Pipeline System Architecture [Image by the Author]Table of ContentsWhat is a data pipeline? The critical point in any AI project.Data crawling. How to collect your data?How do you store your data?Raw data vs. Features dataCloud InfrastructureWrap-up  Run everything  Check out the code on GitHub [1] and support us with a 1. What is a data pipeline? The critical point in any AI project.Data is the lifeblood of any successful AI project, and a well-engineered data pipeline is the key to harnessing its power.This automated system acts as the engine, seamlessly moving data through various stages and transforming it from raw form into actionable insights.But what exactly is a data pipeline, and why is it so critical?A data pipeline is a series of automated steps that guide data on a purpose.It starts with data collection, gathering information from diverse sources, such as LinkedIn, Medium, Substack, Github, etc.The pipeline then tackles the raw data, performing cleaning and transformation.This step removes inconsistencies and irrelevant information and transforms the data into a format suitable for analysis and ML models.But why are data pipelines so crucial in AI projects? Here are some key reasons:Efficiency and Automation: Manual data handling is slow and prone to errors. Pipelines automate the process, ensuring speed and accuracy, especially when dealing with massive data volumes.Scalability: AI projects often grow in size and complexity. A well-designed pipeline can scale seamlessly, accommodating this growth without compromising performance.Quality and Consistency: Pipelines standardize data handling, ensuring consistent and high-quality data throughout the project lifecycle, leading to more reliable AI models.Flexibility and Adaptability: The AI landscape is constantly evolving. A robust data pipeline can adapt to changing requirements without a complete rebuild, ensuring long-term value.Data is the engine of any ML model. If we dont give it enough importance, the models output will be very unexpected.Importance of Data [Image by the Author]But how can we transform the raw data into actionable insights?2. Data crawling. How to collect your data?The first step in building a database of relevant data is choosing our data sources. In this lesson, we will focus on four sources:LinkedinMediumGithubSubstackWhy do we choose 4 data sources? We need complexity and diversity in our data to build a powerful LLM twin. To obtain these characteristics, we will focus on building three collections of data:ArticlesSocial Media PostsCodeFor the data crawling module, we will focus on two libraries:BeautifulSoup: A Python library for parsing HTML and XML documents. It creates parse trees that help us extract the data quickly, but BeautifulSoup needs to fetch the web page for us. Thats why we need to use it alongside libraries like requests or Selenium which can fetch the page for us.Selenium: A tool for automating web browsers. Its used here to interact with web pages programmatically (like logging into LinkedIn, navigating through profiles, etc.). Selenium can work with various browsers, but this code configures it to work with Chrome. We created a base crawler class to respect the best software engineering practices.The BaseAbstractCrawler class in a web crawling context is essential for several key reasons:Code Reusability and Efficiency: It contains standard methods and properties used by different scrapers, reducing code duplication and promoting efficient development.Simplification and Structure: This base class abstracts complex or repetitive code, allowing derived scraper classes to focus on specific tasks. It enforces a consistent structure across different scrapers.Ease of Extension: New types of scrapers can easily extend this base class, making the system adaptable and scalable for future requirements.Maintenance and Testing: Updates or fixes to standard functionalities must be made only once in the base class, simplifying maintenance and testing.The class can be found here base.pyimport timefrom selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsfrom selenium.webdriver.chrome.service import Servicefrom webdriver_manager.chrome import ChromeDriverManagerfrom collector.documents import BaseDocumentclass BaseCrawler:    model: BaseDocument    def extract(self, link: str, **kwargs):        raise NotImplemented(\"Needs implementation in subclass.\")class BaseAbstractCrawler(BaseCrawler):    def __init__(self, scroll_limit: int = 5):        options = self.set_driver_options()        self.scroll_limit = scroll_limit        self.driver = webdriver.Chrome(            service=Service(ChromeDriverManager().install()), options=options        )    def set_driver_options(self) -> Options:        return Options()    def login(self):        pass    def scroll_page(self):        \"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"        current_scroll = 0        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")        while True:            self.driver.execute_script(                \"window.scrollTo(0, document.body.scrollHeight);\"            )            time.sleep(5)            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")            if new_height == last_height or (                self.scroll_limit and current_scroll >= self.scroll_limit            ):                break            last_height = new_height            current_scroll += 1pythonWe created separate crawlers for each collection (posts, articles, and repositories): github.py, medium.py, substack.py and linkedin.py.Every crawler extends the BaseCrawler or BaseAbstractCrawler class, depending on the purpose.The MediumCrawler, SubstackCrawler and LinkedinCrawler extend the BaseAbstractCrawler (as they depend on the login and scrolling functionality).Here is what the LinkedInCrawler looks like  import osimport timefrom typing import Dict, Listfrom bs4 import BeautifulSoupfrom bs4.element import Tagfrom selenium.webdriver.chrome.options import Optionsfrom selenium.webdriver.common.by import Byfrom config import settingsfrom crawlers.base import BaseAbstractCrawlerfrom documents import PostDocumentfrom errors import ImproperlyConfiguredclass LinkedInCrawler(BaseAbstractCrawler):    model = PostDocument    def set_driver_options(self) -> Options:        options = Options()        options.add_experimental_option(\"detach\", True)        return options    def extract(self, link: str, **kwargs):        print(f\"Starting to scrape data for profile: {link}\")    def _scrape_section(self, soup: BeautifulSoup, *args, **kwargs):        \"\"\"Scrape a specific section of the LinkedIn profile.\"\"\"        # Example: Scrape the 'About' section        parent_div = soup.find(*args, **kwargs)        return parent_div.get_text(strip=True) if parent_div else \"\"    def _extract_image_urls(self, buttons: List[Tag]) -> Dict[str, str]:        \"\"\"        Extracts image URLs from button elements.        Args:            buttons (List[Tag]): A list of BeautifulSoup Tag objects representing buttons.        Returns:            Dict[str, str]: A dictionary mapping post indexes to image URLs.        \"\"\"    def _get_page_content(self, url: str) -> BeautifulSoup:        \"\"\"Retrieve the page content of a given URL.\"\"\"    def _extract_posts(        self, post_elements: List[Tag], post_images: Dict[str, str]    ) -> Dict[str, Dict[str, str]]:        \"\"\"        Extracts post texts and combines them with their respective images.        Args:            post_elements (List[Tag]): A list of BeautifulSoup Tag objects representing post elements.            post_images (Dict[str, str]): A dictionary containing image URLs mapped by post index.        Returns:            Dict[str, Dict[str, str]]: A dictionary containing post data with text and optional image URL.        \"\"\"    def _scrape_experience(self, profile_url: str):        \"\"\"Scrapes the Experience section of the LinkedIn profile.\"\"\"        self.driver.get(profile_url + \"/details/experience/\")    def _scrape_education(self, profile_url: str) -> str:        self.driver.get(profile_url + \"/details/education/\")    def login(self):        \"\"\"Log in to LinkedIn.\"\"\"For example, the GitHub crawler is a static crawler that doesnt need a login function, scroll_page function, or driver. It uses only git commands.The GithubCrawler extends the BaseCrawler class and uses the extract method to retrieve the desired repository.import osimport shutilimport subprocessimport tempfilefrom crawlers.base import BaseCrawlerfrom documents import RepositoryDocumentclass GithubCrawler(BaseCrawler):    model = RepositoryDocument    def __init__(self, ignore=(\".git\", \".toml\", \".lock\", \".png\")):        super().__init__()        self._ignore = ignore    def extract(self, link: str, **kwargs):        repo_name = link.rstrip(\"/\").split(\"/\")[-1]        local_temp = tempfile.mkdtemp()        try:            os.chdir(local_temp)            subprocess.run([\"git\", \"clone\", link])            repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])            tree = {}            for root, dirs, files in os.walk(repo_path):                dir = root.replace(repo_path, \"\").lstrip(\"/\")                if dir.startswith(self._ignore):                    continue                for file in files:                    if file.endswith(self._ignore):                        continue                    file_path = os.path.join(dir, file)                    with open(os.path.join(root, file), \"r\", errors=\"ignore\") as f:                        tree[file_path] = f.read().replace(\" \", \"\")            instance = self.model(                name=repo_name, link=link, content=tree, owner_id=kwargs.get(\"user\")            )            instance.save()        except Exception:            raise        finally:            shutil.rmtree(local_temp)3. How do you store your data? An ODM approachObject Document Mapping (ODM) is a technique that maps between an object model in an application and a document database.By abstracting database interactions through model classes, it simplifies the process of storing and managing data in a document-oriented database like MongoDB. This approach is particularly beneficial in applications where data structures align well with object-oriented programming paradigms.The documents.py module serves as a foundational framework for interacting with MongoDB.Our data modeling centers on creating specific document classes  UserDocument, RepositoryDocument, PostDocument, and ArticleDocument  that mirror the structure of our MongoDB collections.These classes define the schema for each data type we store, such as users details, repository metadata, post content, and article information.By using these classes, we can ensure that the data inserted into our database is consistent, valid, and easily retrievable for further operations.class BaseDocument(BaseModel):    id: UUID4 = Field(default_factory=uuid.uuid4)    model_config = ConfigDict(from_attributes=True, populate_by_name=True)    @classmethod    def from_mongo(cls, data: dict):        \"\"\"Convert \"_id\" (str object) into \"id\" (UUID object).\"\"\"    def to_mongo(self, **kwargs) -> dict:        \"\"\"Convert \"id\" (UUID object) into \"_id\" (str object).\"\"\"    def save(self, **kwargs):        collection = _database[self._get_collection_name()]    @classmethod    def get_or_create(cls, **filter_options) -> Optional[str]:        collection = _database[cls._get_collection_name()]    @classmethod    def bulk_insert(cls, documents: List, **kwargs) -> Optional[List[str]]:        collection = _database[cls._get_collection_name()]    @classmethod    def _get_collection_name(cls):        if not hasattr(cls, \"Settings\") or not hasattr(cls.Settings, \"name\"):            raise ImproperlyConfigured(                \"Document should define an Settings configuration class with the name of the collection.\"            )        return cls.Settings.nameclass UserDocument(BaseDocument):    first_name: str    last_name: str    class Settings:        name = \"users\"class RepositoryDocument(BaseDocument):    name: str    link: str    content: dict    owner_id: str = Field(alias=\"owner_id\")    class Settings:        name = \"repositories\"class PostDocument(BaseDocument):    platform: str    content: dict    author_id: str = Field(alias=\"author_id\")    class Settings:        name = \"posts\"class ArticleDocument(BaseDocument):    platform: str    link: str    content: dict    author_id: str = Field(alias=\"author_id\")    class Settings:        name = \"articles\"In our ODM approach for MongoDB, key CRUD operations are integrated:Conversion: The to_mongo method transforms model instances into MongoDB-friendly formats.Inserting: The save method uses PyMongo's insert_one for adding documents, returning MongoDB's acknowledgment as the inserted ID.Bulk Operations: bulk_insert employs insert_many for adding multiple documents and returning their IDs.Upserting: get_or_create either fetches an existing document or creates a new one, ensuring seamless data updates.Validation and Transformation: Using Pydantic models, each class ensures data is correctly structured and validated before database entry.4. Raw data vs featuresNow that we understand the critical role of data pipelines in preparing raw data lets explore how we can transform this data into a usable format for our LLM twin. This is where the concept of features comes into play.Features are the processed building blocks used to fine-tune your LLM twin.Imagine youre teaching someone your writing style. You wouldnt just hand them all your social media posts! Instead, you might point out your frequent use of specific keywords, the types of topics you write about, or the overall sentiment you convey. Features work similarly for your LLM twin.Raw data, on the other hand, is the unrefined information collected from various sources. Social media posts might contain emojis, irrelevant links, or even typos. This raw data needs cleaning and transformation before it can be used effectively.In our data flow, raw data is initially captured and stored in MongoDB, which remains unprocessed.Then, we process this data to create features  key details we use to teach our LLM twin  and keep these in Qdrant. We do this to keep our raw data intact in case we need it again, while Qdrant holds the ready-to-use features for efficient machine learning.Raw Data vs Features [Image by the Author]5. Cloud InfrastructureIn this section, we will focus on how to constantly update our database with the most recent data from the 3 data sources.Before diving into how to build the infrastructure of our data pipeline, I would like to show you how to think through the whole process before stepping into the details of AWS.The first step in doing an infrastructure is to draw a high-level overview of my components.So, the components of our data pipeline are:Linkedin crawlerMedium crawlerGithub crawlerSubstack crawlerMongoDB (Data Collector)High-Level AWS Infrastructure [Image by the Author]Every crawler is a .py file. Since this data pipeline must be constantly updated, we will design a system based on lambda functions, where every lambda function represents a crawler.What is a lambda function in the AWS Environment?AWS Lambda is a serverless computing service that allows you to run code without provisioning or managing servers. It executes your code only when needed and scales automatically, from a few daily requests to thousands per second.Heres how Lambda fits within the AWS environment and what makes it particularly powerful:Event-Driven: AWS Lambda is designed to use events as triggers. These events could be changes to data in an Amazon S3 bucket, updates to a DynamoDB table, HTTP requests via Amazon API Gateway, or direct invocation via SDKs from other applications. In the diagram I provided, the events would likely be new or updated content on LinkedIn, Medium, or GitHub.Scalable: AWS Lambda can run as many instances of the function as needed to respond to the rate of incoming events. This could mean running dozens or even hundreds of cases of your function in parallel.Managed Execution Environment: AWS handles all the administration of the underlying infrastructure, including server and operating system maintenance, capacity provisioning and automatic scaling, code monitoring, and logging. This allows you to focus on your code.How can we put the medium crawler on an AWS Lambda function?We need a handler.The handler function is the entry point for the AWS Lambda function. In AWS Lambda, the handler function is invoked when an event triggers the Lambda function.In the next section, I will show how this handler will be used to deploy this function on AWS Lambda.from aws_lambda_powertools import Loggerfrom aws_lambda_powertools.utilities.typing import LambdaContextfrom crawlers import MediumCrawler, LinkedInCrawler, GithubCrawlerfrom dispatcher import CrawlerDispatcherfrom documents import UserDocumentlogger = Logger(service=\"decodingml/crawler\")_dispatcher = CrawlerDispatcher()_dispatcher.register(\"medium\", MediumCrawler)_dispatcher.register(\"linkedin\", LinkedInCrawler)_dispatcher.register(\"github\", GithubCrawler)def handler(event, context: LambdaContext):    first_name, last_name = event.get('user').split(\" \")    user = UserDocument.get_or_create(first_name=first_name, last_name=last_name)    link = event.get('link')    crawler = _dispatcher.get_crawler(link)    try:        crawler.extract(link=link, user=user)        return {\"statusCode\": 200, \"body\": \"Articles processed successfully\"}    except Exception as e:        return {\"statusCode\": 500, \"body\": f\"An error occurred: {str(e)}\"After you define what it means to transform your Python script into a valid AWS Lambda function, the next phase is to draw the diagram to understand the data flow and how the system will be triggered.AWS High Level Architecture  Overview [Image by the Author]Each crawler function is tailored to its data source: fetching posts from LinkedIn, articles from Medium & Substack, and repository data from GitHub.In order to trigger the lambda function, we have created a python dispatcher which is responsible to manage the crawlers for specific domains.You can register crawlers for different domains and then use the get_crawler method to get the appropriate crawler for a given URL.import refrom crawlers.base import BaseCrawlerclass CrawlerDispatcher:    def __init__(self):        self._crawlers = {}    def register(self, domain: str, crawler: BaseCrawler):        self._crawlers[r\"[URL] = crawler    def get_crawler(self, url: str) -> BaseCrawler:        for pattern, crawler in self._crawlers.items():            if re.match(pattern, url):                return crawler()        else:            raise ValueError(\"No crawler found for the provided link\")The lambda function can be triggered by invoking the function with a link payload.aws lambda invoke \\  --function-name crawler \\  --cli-binary-format raw-in-base64-out \\  --payload '{\"user\": \"Paul Iuztin\", \"link\": \"[URL] \\  response.jsonThe responsible crawler process its respective data and then pass it to the central Data Collector MongoDB.The MongoDB component acts as a unified data store, collecting and managing the data harvested by the lambda functions.This infrastructure is designed for efficient and scalable data extraction, transformation, and loading (ETL) from diverse sources into a single database.6. Wrap-up  Run everythingCloud Deployment with GitHub Actions and AWSIn this final phase, weve established a streamlined deployment process using GitHub Actions. This setup automates the build and deployment of our entire system into AWS.Its a hands-off, efficient approach ensuring that every push to our .github folder triggers the necessary actions to maintain your system in the cloud.You can delve into the specifics of our infrastructure-as-code (IaC) practices, particularly our use of Pulumi, in the ops folder within our GitHub repository.This is a real-world example of modern DevOps practices, offering a peek into industry-standard methods for deploying and managing cloud infrastructure.Local Testing and Running OptionsFor those preferring a more hands-on approach or wishing to avoid cloud costs, we provide another alternative.A detailed Makefile is included in our course materials, allowing you to effortlessly configure and run the entire data pipeline locally.Its especially useful for testing changes in a controlled environment or for those just starting with cloud services.For an in-depth explanation and step-by-step instructions, please refer to the README in the GitHub repository.ConclusionThis is the 2nd article of the LLM Twin: Building Your Production-Ready AI Replica free course.In this lesson, we presented how to build a data pipeline and why its so important in an ML project:  Data collection process -> Medium, Github, Substack & Linkedin crawlers  ETL pipelines -> data is cleaned and normalized  ODM (Object Document Mapping ) -> a technique that maps between an object model in an application and a document database  NoSQL Database (MongoDB) & CDC (Change Data Capture) patternTracks data changes, logs them, and queues messages for real-time system updatesThe clean data is stored in a NoSQL database  Feature PipelineA streaming ingestion pipeline is part of the feature pipeline that processes Articles, Posts, and Code.Tools like Bytewax and Superlinked are used, likely for further data processing and transformation.This processed data is then queued in RabbitMQ, a message broker that helps in asynchronous processing and communication between different services.After we went into the details of how to build data crawlers for different collections like: user articles, github repositories and user social media posts.Ultimately, we went through the process of how to think and prepare your code for AWS by deploying it on lambda functions.In Lesson 3, we will dive deeper into the CDC(change data capture) pattern and explain why its a crucial component in any machine learning project, where data is involved.  Check out the code on GitHub [1] and support us with a Have you enjoyed this article? Then Join 5k+ engineers in the Decoding ML Newsletter for battle-tested content on production-grade ML. Every week:Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. Fordecodingml.substack.comReferences[1] Your LLM Twin Course  GitHub Repository (2024), Decoding ML GitHub OrganizationSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipAccess the best member-only stories.Support independent authors.Listen to audio narrations.Read offline.Join the Partner Program and earn for your writing.Try for $5/monthData PipelineGenerative AiCloud InfrastructureMachine LearningSoftware Engineering4344343FollowWritten by Vesa Alexandru54 FollowersEditor for Decoding MLI am an ambitious Senior AI Engineer, with 10 years of experience in the Software Development industry.FollowMore from Vesa Alexandru and Decoding MLVesa AlexandruinDecoding MLChange Data Capture: Enabling Event-Driven ArchitecturesTransforming Data Streams: The Core of Event-Driven Architectures21 min readApr 6, 2024416Razvant AlexandruinDecoding MLHow to build a Real-Time News Search Engine using Serverless Upstash Kafka and Vector DBA hands-on guide to implementing a live news aggregating streaming pipeline with Apache Kafka, Bytewax, and Upstash Vector Database.19 min readApr 13, 20247075Paul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.16 min readMar 16, 20241.5K9Razvant AlexandruinDecoding MLHow to ensure your deep learning stack is fail-safe in production?Build an end-to-end monitoring dashboard using Prometheus, Triton, and Grafana.10 min readApr 11, 20241692See all from Vesa AlexandruSee all from Decoding MLRecommended from MediumSrijanie Dey, PhDinTowards Data ScienceDeep Dive into Self-Attention by HandExplore the intricacies of the attention mechanism responsible for fueling the transformers11 min read6 days ago4303Vatsal SaglaniinTowards AILlama 3 + Groq is the AI HeavenLlama 3 shines on Groq with blazing generation9 min readApr 21, 20244374ListsPredictive Modeling w/ Python20 stories1132 savesPractical Guides to Machine Learning10 stories1357 savesNatural Language Processing1409 stories905 savesGeneral Coding Knowledge20 stories1152 savesGavin LiinAI AdvancesRun the strongest open-source LLM model: Llama3 70B with just a single 4GB GPU!The strongest open source LLM model Llama3 has been released, Here is how you can run Llama3 70B locally with just 4GB GPU, even on Macbook4 min readApr 21, 20241K2Fabio MatricardiinGenerative AILlama3 is out and you can run it on your Computer!After only 1 day from the release, here is how you can run even on your Laptop with CPU only the latest Meta-AI model.8 min readApr 20, 20241.4K16Paul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streamingLesson 1: LLM architecture system design using the 3-pipeline pattern12 min readJan 5, 2024386Vishal RajputinAIGuysAI Agents Are All You NeedAI Agents, Understanding the  role of Tools, Memory, and Planning in making them work.14 min read6 days ago2191See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."}, {"instruction": "Outline the key concepts and benefits of Change Data Capture (CDC) pattern for enabling event-driven architectures.", "content": "Change Data Capture: Enabling Event-Driven ArchitecturesTransforming Data Streams: The Core of Event-Driven ArchitecturesCDC: Enabling Event-Driven Architectures | Decoding MLOpen in appSign upSign inWriteSign upSign inLLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICAChange Data Capture: Enabling Event-Driven ArchitecturesTransforming Data Streams: The Core of Event-Driven ArchitecturesVesa AlexandruFollowPublished inDecoding ML21 min readApr 6, 2024416ListenShare  the 3rd out of 11 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the LLM Twin: Building Your Production-Ready AI Replica free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care?   No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish  from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.What is an LLM Twin? It is an AI character that learns to write like somebody by incorporating its style and personality into an LLM.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet MLs experiment tracker to monitor the experiments. Evaluate and save the best model to Comets model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comets model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comets prompt monitoring dashboard (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps sound principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 11 hands-on written lessons and the open-source code you can access on GitHub.You can read everything at your own pace.  To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you must know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps Engineer  Check out the code on GitHub [1] and support us with a LessonsThe course is split into 11 lessons. Every Medium article will be its lesson.An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG  in Real-Time!Vector DB retrieval clients [Module 2] WIPTraining data preparation [Module 3] WIPFine-tuning LLM [Module 3] WIPLLM evaluation [Module 4] WIPQuantization [Module 5] WIPBuild the digital twin inference pipeline [Module 6] WIPDeploy the digital twin as a REST API [Module 6] WIP  Check out Lesson 1 to better understand the courses goal, technical details and system design.Lets start with Lesson 3  Lesson 3: Change Data Capture: Enabling Event-Driven ArchitecturesWe have changes everywhere. Linkedin, Medium, Github, Substack can be updated everyday.To be able to have or Digital Twin up to date we need synchronized data.What is synchronized data?Synchronized data is data that is consistent and up-to-date across all systems and platforms it resides on or interacts with. It is the result of making sure that any change made in one dataset is immediately reflected in all other datasets that need to share that information.CDCs primary purpose is to identify and capture changes made to database data, such as insertions, updates, and deletions.It then logs these events and sends them to a message queue, like RabbitMQ. This allows other system parts to react to the data changes in real-time by reading from the queue, ensuring that all application parts are up-to-date.Today, we will learn how to syncronize a data pipeline and a feature pipeline by using CDC pattern.Integrating CDC for Enhanced Data Consistency in LLM System Architecture [Image by the Author]Table of ContentsCDC pattern  OverviewCDC pattern Digital Twin Architecture Use CaseCDC with MongoDBRabbitMQ Message BrokerHands-on CDC: Mongo+RabbitMQTest the system -> on local environmentTest the system -> cloud environmentConclusion  Check out the code on GitHub [1] and support us with a 1. CDC pattern-OverviewChange Data Capture, commonly known as CDC, is an efficient way to track changes in a database.The purpose of CDC is to capture insertions, updates, and deletions applied to a database and to make this change data available in a format easily consumable by downstream applications.Why do we need CDC pattern?Real-time Data Syncing: CDC facilitates near-real-time data integration and syncing.Efficient Data Pipelines: It allows incremental data loading, which is more efficient than bulk load operations.Minimized System Impact: CDC minimizes the impact on the source system by reducing the need for performance-intensive queries.Event-Driven Architectures: It enables event-driven architectures by streaming database events.What problem does CDC solve?Change Data Capture (CDC) is particularly adept at solving consistency issues in distributed systems.Consider a common scenario where an application is required to perform a sequence of actions in response to a trigger  such as a REST call or an event receipt.These actions usually involve making a change to the database and sending a message through a messaging service like Kafka.However, theres an inherent risk: if the application encounters a failure or loses its connection to the messaging service after the database transaction but before the message dispatch, the database will reflect the change, but the corresponding message will never be sent. This discrepancy leads to an inconsistent state within the system.CDC solve this challenge by decoupling the database update from the messaging.It works by treating the database as a reliable source of events. Any committed change in the database is automatically captured by the CDC mechanism, which then ensures the corresponding message is sent to the messaging queue.This separation of concerns provided by CDC means that the database update and the message dispatch are no longer directly dependent on the applications stability or network reliability.By employing CDC, we can maintain consistency across distributed components of a system, even in the face of application failures or network issues, thereby solving a critical problem in maintaining the integrity of distributed systems.Another advantage of using change streams is that they read from this Oplog, not directly from the database.This method significantly reduces the load on the database, avoiding the common pitfall of throttling database performance with frequent direct queries.By tapping into the Oplog, CDC can efficiently identify and capture change events (such as insertions, updates, or deletions) without adding undue stress on the database itself. You can learn more about it here [2], [3] and [4]The problem that CDC solves in distributed systems [Generated by ChatGpt]Summary of diagram:Application Triggered: The diagram begins with an application that is triggered by a REST call or an event.Update Database: The application first updates the database. This is shown as a communication from the Application to the Database.Database Acknowledges: The database acknowledges the update back to the application.Send Message Attempt: Next, the application tries to send a message through the messaging service (like Kafka). This is where the risk of failure is highlighted  if the application fails after updating the database but before successfully sending the message, it results in inconsistency.CDC Mechanism: To resolve this, the CDC mechanism comes into play. It decouples the database update from the messaging.Database Commit Triggering CDC: Any committed change in the database is automatically captured by the CDC mechanism.CDC Dispatches Message: Finally, the CDC mechanism ensures that the corresponding message is sent to the messaging service. This maintains consistency across the system, even if the application encounters issues after updating the database.2. CDC pattern  Digital Twin Architecture Use CaseThe Digital Twin Architecture is respecting the 3-pipeline architecture pattern:the feature pipelinethe training pipelinethe inference pipelineBut one of the most important component in our architecture is the entrypoint of the system: the data pipelineTo ensure our feature store stays up-to-date with the data pipeline, we need a mechanism that detects changes at the pipelines entry point. This way, we can avoid discrepancies like having 100 entries deleted from our RAW Database while the Vector Database lags behind without these updates.In the Data Collection Pipeline, data from various digital platforms like Medium, Substack, LinkedIn, and GitHub is extracted, transformed, and loaded (ETL) into a NoSQL database. Once this raw data is stored, the CDC pattern comes into play.The CDC pattern comes into action after data storage, meticulously monitoring and capturing any changes  insertions, updates, or deletions within the NoSQL database.These changes then trigger events that the CDC system captures and pushes onto a queue, managed by RabbitMQ (message broker).On the other side of the CDC pattern is the Feature Pipeline, where the data continue to flow.Here, a streaming ingestion pipeline (Bytewax and Superlinked) takes the queues data and processes it in real-time. The processed data includes articles, posts, and code which are then transformed into features  actionable insights or inputs for machine learning models.The processed data is then loaded into a Vector DB (Qdrant), where its organized and indexed for efficient retrieval.The Vector DB Retrieval Clients serve as the access points for querying and extracting these processed data features, now ready to be used in various applications, including training machine learning models or powering search algorithms.3. CDC with MongoDBIn the world of data-driven applications, timing is everything.The swifter a system can respond to data changes, the more agile and user-friendly it becomes. Lets dive into this concept, especially in the context of MongoDBs change streams, a feature that fundamentally transforms how applications interact with data.Immediate Response to Data ChangesConsider a scenario where LinkedIn posts are regularly updated in our MongoDB database. Each post might undergo changes  perhaps an edit to the content, a new comment, or an update in user engagement metrics.In a traditional setup, reflecting these updates into our feature store, specifically Qdrant, could involve significant delays and manual intervention.However, with MongoDBs change streams, we implement a observer within our database. This feature is detecting changes in real-time. When a LinkedIn post is edited, MongoDB instantly captures this event and relays it to our data pipeline.Our data pipeline, upon receiving a notification of the change, springs into action. The updated LinkedIn post is then processed  perhaps analyzed for new keywords, sentiments, or user interactions  and updated in Qdrant.The sweet spot of MongoDBs change streams is in their ability to streamline this process. They provide a direct line from the occurrence of a change in MongoDB to its reflection in Qdrant, ensuring our feature store is always in sync with the latest data.This capability is crucial for maintaining an up-to-date and accurate data landscape, which in turn, powers more relevant and dynamic analytics for the LLM twin.Before change streams, applications that needed to know about the addition of new data in real-time had to continuously poll data or rely on other update mechanisms.One common, if complex, technique for monitoring changes was tailing MongoDBs Operation Log (Oplog). The Oplog is part of the replication system of MongoDB and as such already tracks modifications to the database but is not easy to use for business logic.Change streams are built on top of the Oplog but they provide a native API that improves efficiency and usability.Example of Change Streams flow [Image by the Author]Note that you cannot open a change stream against a collection in a standalone MongoDB server because the feature relies on the Oplog which is only used on replica sets.When registering a change stream you need to specify the collection and what types of changes you want to listen to. You can do this by using the $match and a few other aggregation pipeline stages which limit the amount of data you will receive.4. RabbitMQ Message BrokerRabbitMQ is a reliable and mature messaging and streaming broker, which is easy to deploy on cloud environments, on-premises, and on your local machine. It is currently used by millions worldwide.Why do we need a message broker?Reliability: RabbitMQ guarantees reliable message delivery, ensuring that change events are conveyed to the Feature Pipeline, even in the face of temporary outages.Decoupling: This enables loose coupling between services, promoting autonomous operation and mitigating the propagation of failures across the system.Load Management: It evenly distributes the data load across multiple consumers, enhancing system efficiency.Asynchronous Processing: The system benefits from asynchronous processing, with RabbitMQ queuing change events for processing without delay.Scalability: RabbitMQs scalability features accommodate growing data volumes by facilitating easy addition of consumers and horizontal scaling.Data Integrity: It ensures messages are processed in the order theyre received, which is critical for data integrity.Recovery Mechanisms: RabbitMQ offers message acknowledgment and redelivery features, vital for recovery from failures without data loss.5. Hands-on  CDC: Mongo+RabbitMQ5.1 Hands-on CDC: Mongo+RabbitMQWe are building the RabbitMQConnection class, a singleton structure, for establishing and managing connections to the RabbitMQ server. This class is robustly designed to handle connection parameters like username, password, queue name, host, port, and virtual_host, which can be customized or defaulted from settings.Utilizing the pika Python library, RabbitMQConnection provides methods to connect, check connection status, retrieve channels, and close the connection. This improved approach encapsulates connection management within a singleton instance, ensuring efficient handling of RabbitMQ connections throughout the system lifecycle, from initialization to closure.class RabbitMQConnection:    \"\"\"Singleton class to manage RabbitMQ connection.\"\"\"    _instance = None    def __new__(        cls, host: str = None, port: int = None, username: str = None, password: str = None, virtual_host: str = \"/\"    ):        if not cls._instance:            cls._instance = super().__new__(cls)        return cls._instance    def __init__(        self,        host: str = None,        port: int = None,        username: str = None,        password: str = None,        virtual_host: str = \"/\",        fail_silently: bool = False,        **kwargs,    ):        self.host = host or settings.RABBITMQ_HOST        self.port = port or settings.RABBITMQ_PORT        self.username = username or settings.RABBITMQ_DEFAULT_USERNAME        self.password = password or settings.RABBITMQ_DEFAULT_PASSWORD        self.virtual_host = virtual_host        self.fail_silently = fail_silently        self._connection = None    def __enter__(self):        self.connect()        return self    def __exit__(self, exc_type, exc_val, exc_tb):        self.close()    def connect(self):        try:            credentials = pika.PlainCredentials(self.username, self.password)            self._connection = pika.BlockingConnection(                pika.ConnectionParameters(                    host=self.host, port=self.port, virtual_host=self.virtual_host, credentials=credentials                )            )        except pika.exceptions.AMQPConnectionError as e:            print(\"Failed to connect to RabbitMQ:\", e)            if not self.fail_silently:                raise e    def is_connected(self) -> bool:        return self._connection is not None and self._connection.is_open    def get_channel(self):        if self.is_connected():            return self._connection.channel()    def close(self):        if self.is_connected():            self._connection.close()            self._connection = None            print(\"Closed RabbitMQ connection\")Publishing to RabbitMQ: The publish_to_rabbitmq function is where the magic happens. It connects to RabbitMQ , ensures that the message delivery is confirmed for reliability, and then publishes the data. The data variable, which is expected to be a JSON string, represents the changes captured by MongoDB's CDC mechanism.def publish_to_rabbitmq(queue_name: str, data: str):    \"\"\"Publish data to a RabbitMQ queue.\"\"\"    try:        # Create an instance of RabbitMQConnection        rabbitmq_conn = RabbitMQConnection()        # Establish connection        with rabbitmq_conn:            channel = rabbitmq_conn.get_channel()            # Ensure the queue exists            channel.queue_declare(queue=queue_name, durable=True)            # Delivery confirmation            channel.confirm_delivery()            # Send data to the queue            channel.basic_publish(                exchange=\"\",                routing_key=queue_name,                body=data,                properties=pika.BasicProperties(                    delivery_mode=2,  # make message persistent                ),            )            print(\"Sent data to RabbitMQ:\", data)    except pika.exceptions.UnroutableError:        print(\"Message could not be routed\")    except Exception as e:        print(f\"Error publishing to RabbitMQ: {e}\")5.2 CDC Pattern in MongoDBSetting Up MongoDB Connection: The script begins by establishing a connection to a MongoDB database using MongoDatabaseConnectorclass. This connection targets a specific database named scrabble.mongo.pyfrom pymongo import MongoClientfrom pymongo.errors import ConnectionFailurefrom rag.settings import settingsclass MongoDatabaseConnector:    _instance: MongoClient = None    def __new__(cls, *args, **kwargs):        if cls._instance is None:            try:                cls._instance = MongoClient(settings.MONGO_DATABASE_HOST)            except ConnectionFailure as e:                print(f\"Couldn't connect to the database: {str(e)}\")                raise        print(f\"Connection to database with uri: {settings.MONGO_DATABASE_HOST} successful\")        return cls._instance    def get_database(self):        return self._instance[settings.MONGO_DATABASE_NAME]    def close(self):        if self._instance:            self._instance.close()            print(\"Connected to database has been closed.\")connection = MongoDatabaseConnector()Monitoring Changes with watch: The core of the CDC pattern in MongoDB is realized through the watch method. Here, the script sets up a change stream to monitor for specific types of changes in the database. In this case, it's configured to listen for insert operations in any collection within the scrabble database.changes = db.watch([{'$match': {'operationType': {'$in': ['insert']}}}])Processing Each Change: As changes occur in the database, the script iterates through each change event. For each event, the script extracts important metadata like the data type (collection name) and the entry ID. It also reformats the document by removing the MongoDB-specific _id and appending the data type and entry ID. This formatting makes the data more usable for downstream processes.    for change in changes:        data_type = change['ns']['coll']        entry_id = change['fullDocument']['_id']        change['fullDocument'].pop('_id')        change['fullDocument']['type'] = data_type        change['fullDocument']['entry_id'] = entry_idConversion to JSON and Publishing to RabbitMQ: The transformed document is then converted to a JSON string. This serialized data is ready to be sent to a messaging system, RabbitMQ, in this instance. This is where publish_to_rabbitmq comes into play, sending the JSON data to a specified RabbitMQ queue.data = json.dumps(change['fullDocument'])publish_to_rabbitmq(data=data)5.3 The full system docker-composeThis docker-compose configuration outlines the setup for a system comprising a MongoDB database and a RabbitMQ message broker. The setup is designed to facilitate a development or testing environment using Docker containers. Lets walk through the key components of this configuration:This docker-compose configuration outlines the setup for a system comprising a MongoDB database and a RabbitMQ message broker. The setup is designed to facilitate a development or testing environment using Docker containers.In section 6 we will show you how to start this docker-compose and test the system.Lets walk through the key components of this configuration:MongoDB Service SetupImage: Each MongoDB instance uses the mongo:5 image, which is the official MongoDB image at version 5.2. Container Names: Individually named (mongo1, mongo2, mongo3) for easy identification.3. Commands: Each instance is started with specific commands:--replSet \"my-replica-set\" to set up a replica set named 'my-replica-set'.--bind_ip_all to bind MongoDB to all IP addresses.--port 3000X (where X is 1, 2, or 3) to define distinct ports for each instance.Using three replicas in a MongoDB replica set is a common practice primarily for achieving high availability, data redundancy, and fault tolerance. Heres why having three replicas is beneficial:High Availability: In a replica set, one node is the primary node that handles all write operations, while the others are secondary nodes that replicate the data from the primary. If the primary node fails, one of the secondary nodes is automatically elected as the new primary. With three nodes, you ensure that theres always a secondary node available to take over if the primary fails, minimizing downtime.Data Redundancy: Multiple copies of the data are maintained across different nodes. This redundancy safeguards against data loss in case of a hardware failure or corruption on one of the nodes.4.Volumes: Maps a local directory (e.g., ./data/mongo-1) to the container's data directory (/data/db). This ensures data persistence across container restarts.5. Ports: Exposes each MongoDB instance on a unique port on the host machine (30001, 30002, 30003).6. Healthcheck (only for mongo1): Regularly checks the health of the first MongoDB instance, ensuring the replica set is correctly initiated and operational.RabbitMQ Service SetupImage and Container: Uses RabbitMQ 3 with management plugin based on Alpine Linux. The container is named scrabble_mq.Ports: Exposes RabbitMQ on port 5673 for message queue communication and 15673 for management console access.Volumes: Maps local directories for RabbitMQ data and log storage, ensuring persistence and easy access to logs.Restart Policy: Like MongoDB, its configured to always restart if it stops.version: '3.8'services:  mongo1:    image: mongo:5    container_name: mongo1    command: [\"--replSet\", \"my-replica-set\", \"--bind_ip_all\", \"--port\", \"30001\"]    volumes:      - ./data/mongo-1:/data/db    ports:      - 30001:30001    healthcheck:      test: test $$(echo \"rs.initiate({_id:'my-replica-set',members:[{_id:0,host:\\\"mongo1:30001\\\"},{_id:1,host:\\\"mongo2:30002\\\"},{_id:2,host:\\\"mongo3:30003\\\"}]}).ok || rs.status().ok\" | mongo --port 30001 --quiet) -eq 1      interval: 10s      start_period: 30s  mongo2:    image: mongo:5    container_name: mongo2    command: [\"--replSet\", \"my-replica-set\", \"--bind_ip_all\", \"--port\", \"30002\"]    volumes:      - ./data/mongo-2:/data/db    ports:      - 30002:30002  mongo3:    image: mongo:5    container_name: mongo3    command: [\"--replSet\", \"my-replica-set\", \"--bind_ip_all\", \"--port\", \"30003\"]    volumes:      - ./data/mongo-3:/data/db    ports:      - 30003:30003  mq:    image: rabbitmq:3-management-alpine    container_name: scrabble_mq    ports:      - \"5673:5672\"      - \"15673:15672\"    volumes:      - ~/rabbitmq/data/:/var/lib/rabbitmq/      - ~/rabbitmq/log/:/var/log/rabbitmq    restart: always6. Test the system -> on local environmentIn order to test the entire system on your local environment we created a .Makefile where 3 steps are defined:help: @grep -E '^[a-zA-Z0-9 -]+:.*#'  Makefile | sort | while read -r l; do printf \"\\033[1;32m$$(echo $$l | cut -f 1 -d':')\\033[00m:$$(echo $$l | cut -f 2- -d'#')\\n\"; donelocal-start: # Buil and start mongodb and mq. docker-compose -f docker-compose.yml up --build -dlocal-start-cdc: # Start CDC system python cdc.pylocal-test-cdc: #Test CDC system by inserting data to mongodb python test_cdc.py1. Build and run docker-composeCommand: make local-startPurpose: This step involves building and starting the MongoDB and RabbitMQ services using Docker Compose.If everything went well, type docker ps and see if services are up and running: docker psCONTAINER ID   IMAGE                          COMMAND                  CREATED         STATUS                   PORTS                                                                                                         NAMES8193d0af74b6   mongo:5                        \"docker-entrypoint.s\"   6 minutes ago   Up 6 minutes (healthy)   27017/tcp, 0.0.0.0:30001->30001/tcp                                                                           mongo12d82263e5780   mongo:5                        \"docker-entrypoint.s\"   6 minutes ago   Up 6 minutes             27017/tcp, 0.0.0.0:30003->30003/tcp                                                                           mongo3d1cb8d96dba0   rabbitmq:3-management-alpine   \"docker-entrypoint.s\"   6 minutes ago   Up 6 minutes             4369/tcp, 5671/tcp, 15671/tcp, 15691-15692/tcp, 25672/tcp, 0.0.0.0:5673->5672/tcp, 0.0.0.0:15673->15672/tcp   scrabble_mq7a213f0a22a6   mongo:5                        \"docker-entrypoint.s\"   6 minutes ago   Up 6 minutes             27017/tcp, 0.0.0.0:30002->30002/tcp                                                                           mongo22. Start the CDC watcherCommand: make local-start-cdcPurpose: This step starts the Change Data Capture (CDC) system, which monitors and captures changes in the MongoDB database.Action: Execute make local-start-cdc in your terminal. It triggers the python cdc.py script, activating the CDC watcher. This watcher will monitor for changes in the database and send them to the message queue.After you run this command you must see the following logs:Connection to database with uri: mongodb://localhost:30001,localhost:30002,localhost:30003/?replicaSet=my-replica-set successfulConnection to database with uri: mongodb://localhost:30001,localhost:30002,localhost:30003/?replicaSet=my-replica-set successful2024-04-05 04:43:34,661 - INFO - Connected to MongoDB.3. Insert dummy data into MongoDBCommand: make local-test-cdcPurpose: To test if the CDC system is functioning correctly, you need to insert test data into MongoDB.Action: Use make local-test-cdc to run the command python test_cdc.py. This script inserts dummy data into MongoDB, which should trigger the CDC system. Watch for the CDC system to capture these changes and relay them to RabbitMQ, verifying the whole setup is working correctly.After you run this command, you must observe in the logs that CDC observed that a change was made, and published it to the RabbitMQ .024-04-05 04:43:51,510 - INFO - Change detected and serialized: {\"name\": \"LLM TWIN\", \"type\": \"test\", \"entry_id\": \"660f5757b3153bbb219fd901\"}2024-04-05 04:43:51,513 - INFO - Pika version 1.3.2 connecting to ('127.0.0.1', 5673)2024-04-05 04:43:51,513 - INFO - Socket connected: <socket.socket fd=15, family=2, type=1, proto=6, laddr=('127.0.0.1', 63697), raddr=('127.0.0.1', 5673)>2024-04-05 04:43:51,514 - INFO - Streaming transport linked up: (<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x105c05b80>, _StreamingProtocolShim: <SelectConnection PROTOCOL transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x105c05b80> params=<ConnectionParameters host=localhost port=5673 virtual_host=/ ssl=False>>).2024-04-05 04:43:51,521 - INFO - AMQPConnector - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x105c05b80> params=<ConnectionParameters host=localhost port=5673 virtual_host=/ ssl=False>>2024-04-05 04:43:51,521 - INFO - AMQPConnectionWorkflow - reporting success: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x105c05b80> params=<ConnectionParameters host=localhost port=5673 virtual_host=/ ssl=False>>2024-04-05 04:43:51,521 - INFO - Connection workflow succeeded: <SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x105c05b80> params=<ConnectionParameters host=localhost port=5673 virtual_host=/ ssl=False>>2024-04-05 04:43:51,522 - INFO - Created channel=1Sent data to RabbitMQ: {\"name\": \"LLM TWIN\", \"type\": \"test\", \"entry_id\": \"660f5757b3153bbb219fd901\"}2024-04-05 04:43:51,534 - INFO - Closing connection (200): Normal shutdown2024-04-05 04:43:51,534 - INFO - Closing channel (200): 'Normal shutdown' on <Channel number=1 OPEN conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x105c05b80> params=<ConnectionParameters host=localhost port=5673 virtual_host=/ ssl=False>>>2024-04-05 04:43:51,535 - INFO - Received <Channel.CloseOk> on <Channel number=1 CLOSING conn=<SelectConnection OPEN transport=<pika.adapters.utils.io_services_utils._AsyncPlaintextTransport object at 0x105c05b80> params=<ConnectionParameters host=localhost port=5673 virtual_host=/ ssl=False>>>2024-04-05 04:43:51,535 - INFO - Closing connection (200): 'Normal shutdown'2024-04-05 04:43:51,536 - INFO - Aborting transport connection: state=1; <socket.socket fd=15, family=2, type=1, proto=6, laddr=('127.0.0.1', 63697), raddr=('127.0.0.1', 5673)>2024-04-05 04:43:51,536 - INFO - _AsyncTransportBase._initate_abort(): Initiating abrupt asynchronous transport shutdown: state=1; error=None; <socket.socket fd=15, family=2, type=1, proto=6, laddr=('127.0.0.1', 63697), raddr=('127.0.0.1', 5673)>2024-04-05 04:43:51,536 - INFO - Deactivating transport: state=1; <socket.socket fd=15, family=2, type=1, proto=6, laddr=('127.0.0.1', 63697), raddr=('127.0.0.1', 5673)>2024-04-05 04:43:51,536 - INFO - AMQP stack terminated, failed to connect, or aborted: opened=True, error-arg=None; pending-error=ConnectionClosedByClient: (200) 'Normal shutdown'2024-04-05 04:43:51,536 - INFO - Stack terminated due to ConnectionClosedByClient: (200) 'Normal shutdown'2024-04-05 04:43:51,536 - INFO - Closing transport socket and unlinking: state=3; <socket.socket fd=15, family=2, type=1, proto=6, laddr=('127.0.0.1', 63697), raddr=('127.0.0.1', 5673)>2024-04-05 04:43:51,536 - INFO - User-initiated close: result=BlockingConnection__OnClosedArgs(connection=<SelectConnection CLOSED transport=None params=<ConnectionParameters host=localhost port=5673 virtual_host=/ ssl=False>>, error=ConnectionClosedByClient: (200) 'Normal shutdown')Closed RabbitMQ connection2024-04-05 04:43:51,536 - INFO - Data published to RabbitMQ.7. Test the system -> cloud environment.The flow suggests a system where content from various platforms is crawled, processed, and stored in MongoDB. A CDC system running on Fargate captures any changes in the database and publishes messages about these changes to RabbitMQ.Architecture OverviewMedium/Substack/Linkedin/Github URL Link: These are the sources of content. The system starts with URLs from these platforms.Lambda Handler: This includes a Python Dispatcher and a Lambda Crawler which contains all types of crawlers. The Python Dispatcher is a component that decides which crawler to invoke based on the URL, while the Lambda Crawler is responsible for extracting the content from the provided URLs.MongoDB: A NoSQL database used to store the crawled content.CDC Fargate: This is a Change Data Capture (CDC) process running on AWS Fargate, which is a serverless compute engine for containers. CDC is used to capture and monitor changes in the database (like new articles added, or existing articles updated or deleted).RabbitMQ: This is a message-broker software that receives messages about the changes from the CDC process and likely forwards these messages to other components in the system for further processing or notifying subscribers of the changes.AWS Infrastructure for MongoDB CDC [Image by the Author]Cloud Deployment with GitHub Actions and AWSIn this final phase, weve established a streamlined deployment process using GitHub Actions. This setup automates the build and deployment of our entire system into AWS.Its a hands-off, efficient approach ensuring that every push to our .github folder triggers the necessary actions to maintain your system in the cloud.In our GitHub repository it will be a .Readme file in which we will explain everything you need to setup your credentials and run everything.You can delve into the specifics of our infrastructure-as-code (IaC) practices, particularly our use of Pulumi, in the ops folder within our GitHub repository.This is a real-world example of modern DevOps practices, offering a peek into industry-standard methods for deploying and managing cloud infrastructure.ConclusionThis is the 3rd article of the LLM Twin: Building Your Production-Ready AI Replica free course.In this lesson, we presented Change Data Capture (CDC) as a key component for synchronizing data across various platforms, crucial for maintaining real-time data consistency in event-driven systems:Integration with MongoDB and RabbitMQ: The lesson demonstrates how CDC, combined with MongoDB for data management and RabbitMQ for message brokering, creates a robust framework for real-time data processing.Role of CDC in LLM Twin Architecture: It emphasizes CDCs importance in the construction of an LLM Twin, ensuring data remains synchronized across the system, from data collection to feature extraction.Practical Application and Implementation: Detailed instructions are provided on setting up and testing CDC in both local and cloud environments, offering hands-on experience in implementing these technologies.In Lesson 4, we will dive deeper into the Streaming ingestion pipeline and explain why its a crucial component in any machine learning project, where data is involved.  Check out the code on GitHub [1] and support us with a Have you enjoyed this article? Then Join other engineers in the Decoding ML Newsletter for battle-tested content on production-grade ML. Every week:  [URL] Your LLM Twin Course  GitHub Repository (2024), Decoding ML GitHub Organization[2] Change Streams, MongoDB Documentation[3]Shantanu Bansal, Demystifying MongoDB Oplog: A Comprehensive Guide with Oplog Entry Examples, 2023, Medium[4] How Do Change Streams Work in MongoDB?, MongoDB DocumentationSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipAccess the best member-only stories.Support independent authors.Listen to audio narrations.Read offline.Join the Partner Program and earn for your writing.Try for $5/monthData PipelineData EngineeringMongodbLlmMl System Design416416FollowWritten by Vesa Alexandru54 FollowersEditor for Decoding MLI am an ambitious Senior AI Engineer, with 10 years of experience in the Software Development industry.FollowMore from Vesa Alexandru and Decoding MLVesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable data17 min readMar 23, 20244343Razvant AlexandruinDecoding MLHow to build a Real-Time News Search Engine using Serverless Upstash Kafka and Vector DBA hands-on guide to implementing a live news aggregating streaming pipeline with Apache Kafka, Bytewax, and Upstash Vector Database.19 min readApr 13, 20247075Paul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.16 min readMar 16, 20241.5K9Razvant AlexandruinDecoding MLHow to ensure your deep learning stack is fail-safe in production?Build an end-to-end monitoring dashboard using Prometheus, Triton, and Grafana.10 min readApr 11, 20241692See all from Vesa AlexandruSee all from Decoding MLRecommended from MediumPaul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streamingLesson 1: LLM architecture system design using the 3-pipeline pattern12 min readJan 5, 2024386Alex HoncharinTowards Data ScienceIntro to LLM Agents with Langchain: When RAG is Not EnoughFirst-order principles of brain structure for AI assistants7 min readMar 15, 20242.1K10ListsNatural Language Processing1409 stories905 savesChatGPT prompts 47 stories1476 savesStaff Picks630 stories922 savesFabio MatricardiinGenerative AILlama3 is out and you can run it on your Computer!After only 1 day from the release, here is how you can run even on your Laptop with CPU only the latest Meta-AI model.8 min readApr 20, 20241.4K16Somnath SinghinLevel Up CodingThe Era of High-Paying Tech Jobs is OverThe Death of Tech Jobs.14 min readApr 1, 20249.3K256Vu TrinhinData Engineer ThingsHow does Uber build real-time infrastructure to handle petabytes of data every day?All insights from the paper: Real-time data infrastructure at Uber19 min readMar 23, 20241.1K11Gavin LiinAI AdvancesRun the strongest open-source LLM model: Llama3 70B with just a single 4GB GPU!The strongest open source LLM model Llama3 has been released, Here is how you can run Llama3 70B locally with just 4GB GPU, even on Macbook4 min readApr 21, 20241K2See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."}, {"instruction": "Explain the 3-pipeline architecture, its components, and benefits in transitioning from prototype to production in ML systems.", "content": "An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.End-to-End Framework for Production-Ready LLMs | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonLLM Twin Course: Building Your Production-Ready AI ReplicaAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul IusztinFollowPublished inDecoding ML16 min readMar 16, 20241.5K10ListenShare  the 1st out of 11 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the LLM Twin: Building Your Production-Ready AI Replica free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care?  No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish  from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet MLs experiment tracker to monitor the experiments. Evaluate and save the best model to Comets model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comets model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comets prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 11 hands-on written lessons and the open-source code you can access on GitHub.You can read everything at your own pace.  To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps EngineerLessonsThe course is split into 11 lessons. Every Medium article will be its own lesson.An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG  in Real-Time!Vector DB retrieval clients [Module 2] WIPTraining data preparation [Module 3] WIPFine-tuning LLM [Module 3] WIPLLM evaluation [Module 4] WIPQuantization [Module 5] WIPBuild the digital twin inference pipeline [Module 6] WIPDeploy the digital twin as a REST API [Module 6] WIP  Check out the code on GitHub [1] and support us with a Lets start with Lesson 1  Lesson 1: End-to-end framework for production-ready LLM systemsIn the first lesson, we will present the project you will build during the course: your production-ready LLM Twin/AI replica.Afterward, we will explain what the 3-pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3-pipeline architecture to our LLM microservices.In the following lessons, we will examine each components code and learn how to implement and deploy it to AWS and Qwak.LLM twin system architecture [Image by the Author]Table of ContentsWhat are you going to build? The LLM twin conceptThe 3-pipeline architectureLLM twin system design  Check out the code on GitHub [1] and support us with a 1. What are you going to build? The LLM twin conceptThe outcome of this course is to learn to build your own AI replica. We will use an LLM to do that, hence the name of the course: LLM Twin: Building Your Production-Ready AI Replica.But what is an LLM twin?Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality.It will not be you. It will be your writing copycat.More concretely, you will build an AI replica that writes social media posts or technical articles (like this one) using your own voice.Why not directly use ChatGPT? You may askWhen trying to generate an article or post using an LLM, the results tend to:be very generic and unarticulated,contain misinformation (due to hallucination),require tedious prompting to achieve the desired result.But here is what we are going to do to fix that  First, we will fine-tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub.By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself.Have you seen the universe of AI characters Meta released in 2024 in the Messenger app? If not, you can learn more about it here [2].To some extent, that is what we are going to build.But in our use case, we will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice.For example, we can ask your LLM twin to write a LinkedIn post about LLMs. Instead of writing some generic and unarticulated post about LLMs (e.g., what ChatGPT will do), it will use your voice and style.Secondly, we will give the LLM access to a vector DB to access external information to avoid hallucinating. Thus, we will force the LLM to write only based on concrete data.Ultimately, in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process.For example, we can modify the example above to: Write me a 1000-word LinkedIn post about LLMs based on the article from this link: [URL].Excited? Lets get started  2. The 3-pipeline architectureWe all know how messy ML systems can get. That is where the 3-pipeline architecture kicks in.The 3-pipeline design brings structure and modularity to your ML system while improving your MLOps processes.ProblemDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.In 2022, only 54% of the models get into production. Auch.So what happens?Maybe the first things that come to your mind are:the model is not mature enoughsecurity risks (e.g., data privacy)not enough dataTo some extent, these are true.But the reality is that in many scenariosthe architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.So, good SWE processes and a well-defined architecture are as crucial as using suitable tools and models with high accuracy.Solution  The 3-pipeline architectureLets understand what the 3-pipeline design is.It is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:1. the feature pipeline2. the training pipeline3. the inference pipelinealso known as the Feature/Training/Inference (FTI) architecture.#1. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store. The feature store will act as the central repository of your features. That means that features can be accessed and shared only through the feature store.#2. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained model weights, which are stored and versioned inside a model registry. The models will be accessed and shared only through the model registry.#3. The inference pipeline uses a given version of the features from the feature store and downloads a specific version of the model from the model registry. Its final goal is to output the predictions to a client.The 3-pipeline architecture [Image by the Author].This is why the 3-pipeline design is so beautiful:- it is intuitive- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate- the ML system has been built with modularity in mind since the beginning- the 3 components can easily be divided between multiple teams (if necessary)- every component can use the best stack of technologies available for the job- every component can be deployed, scaled, and monitored independently- the feature pipeline can easily be either batch, streaming or bothBut the most important benefit is thatby following this pattern, you know 100% that your ML model will move out of your Notebooks into production.  If you want to learn more about the 3-pipeline design, I recommend this excellent article [3] written by Jim Dowling, one of the creators of the FTI architecture.3. LLM Twin System designLets understand how to apply the 3-pipeline architecture to our LLM system.The architecture of the LLM twin is split into 4 Python microservices:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineLLM twin system architecture [Image by the Author]As you can see, the data collection pipeline doesnt follow the 3-pipeline design. Which is true.It represents the data pipeline that sits before the ML system.The data engineering team usually implements it, and its scope is to gather, clean, normalize and store the data required to build dashboards or ML models.But lets say you are part of a small team and have to build everything yourself, from data gathering to model deployment.Thus, we will show you how the data pipeline nicely fits and interacts with the FTI architecture.Now, lets zoom in on each component to understand how they work individually and interact with each other.  3.1. The data collection pipelineIts scope is to crawl data for a given user from:Medium (articles)Substack (articles)LinkedIn (posts)GitHub (code)As every platform is unique, we implemented a different Extract Transform Load (ETL) pipeline for each website.  1-min read on ETL pipelines [4]However, the baseline steps are the same for each platform.Thus, for each ETL pipeline, we can abstract away the following baseline steps:log in using your credentialsuse selenium to crawl your profileuse BeatifulSoup to parse the HTMLclean & normalize the extracted HTMLsave the normalized (but still raw) data to Mongo DBImportant note: We are crawling only our data, as most platforms do not allow us to access other peoples data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data.Why Mongo DB?We wanted a NoSQL database that quickly allows us to store unstructured data (aka text).How will the data pipeline communicate with the feature pipeline?We will use the Change Data Capture (CDC) pattern to inform the feature pipeline of any change on our Mongo DB.  1-min read on the CDC pattern [5]To explain the CDC briefly, a watcher listens 24/7 for any CRUD operation that happens to the Mongo DB.The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue.The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB.For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue; ultimately, the feature pipeline consumes and processes it.Doing this ensures that the Mongo DB and vector DB are constantly in sync.With the CDC technique, we transition from a batch ETL pipeline (our data pipeline) to a streaming pipeline (our feature pipeline).Using the CDC pattern, we avoid implementing a complex batch pipeline to compute the difference between the Mongo DB and vector DB. This approach can quickly get very slow when working with big data.Where will the data pipeline be deployed?The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB.3.2. The feature pipelineThe feature pipeline is implemented using Bytewax (a Rust streaming engine with a Python interface). Thus, in our specific use case, we will also refer to it as a streaming ingestion pipeline.It is an entirely different service than the data collection pipeline.How does it communicate with the data pipeline?As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue.Currently, the streaming pipeline doesnt care how the data is generated or where it comes from.It knows it has to listen to a given queue, consume messages from there and process them.By doing so, we decouple the two components entirely. In the future, we can easily add messages from multiple sources to the queue, and the streaming pipeline will know how to process them. The only rule is that the messages in the queue should always respect the same structure/interface.What is the scope of the feature pipeline?It represents the ingestion component of the RAG system.It will take the raw data passed through the queue and:clean the data;chunk it;embed it using the embedding models from Superlinked;load it to the Qdrant vector DB.Every type of data (post, article, code) will be processed independently through its own set of classes.Even though all of them are text-based, we must clean, chunk and embed them using different strategies, as every type of data has its own particularities.What data will be stored?The training pipeline will have access only to the feature store, which, in our case, is represented by the Qdrant vector DB.Note that a vector DB can also be used as a NoSQL DB.With these 2 things in mind, we will store in Qdrant 2 snapshots of our data:1. The cleaned data (without using vectors as indexes  store them in a NoSQL fashion).2. The cleaned, chunked, and embedded data (leveraging the vector indexes of Qdrant)The training pipeline needs access to the data in both formats as we want to fine-tune the LLM on standard and augmented prompts.With the cleaned data, we will create the prompts and answers.With the chunked data, we will augment the prompts (aka RAG).Why implement a streaming pipeline instead of a batch pipeline?There are 2 main reasons.The first one is that, coupled with the CDC pattern, it is the most efficient way to sync two DBs between each other. Otherwise, you would have to implement batch polling or pushing techniques that arent scalable when working with big data.Using CDC + a streaming pipeline, you process only the changes to the source DB without any overhead.The second reason is that by doing so, your source and vector DB will always be in sync. Thus, you will always have access to the latest data when doing RAG.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rusts impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer.Where will the feature pipeline be deployed?The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant.3.3. The training pipelineHow do we have access to the training features?As highlighted in section 3.2, all the training data will be accessed from the feature store. In our case, the feature store is the Qdrant vector DB that contains:the cleaned digital data from which we will create prompts & answers;we will use the chunked & embedded data for RAG to augment the cleaned data.We will implement a different vector DB retrieval client for each of our main types of data (posts, articles, code).We must do this separation because we must preprocess each type differently before querying the vector DB, as each type has unique properties.Also, we will add custom behavior for each client based on what we want to query from the vector DB. But more on this in its dedicated lesson.What will the training pipeline do?The training pipeline contains a data-to-prompt layer that will preprocess the data retrieved from the vector DB into prompts.It will also contain an LLM fine-tuning module that inputs a HuggingFace dataset and uses QLoRA to fine-tune a given LLM (e.g., Mistral). By using HuggingFace, we can easily switch between different LLMs so we wont focus too much on any specific LLM.All the experiments will be logged into Comet MLs experiment tracker.We will use a bigger LLM (e.g., GPT4) to evaluate the results of our fine-tuned LLM. These results will be logged into Comets experiment tracker.Where will the production candidate LLM be stored?We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry.After, we will inspect the LLM production candidate manually using Comets prompt monitoring dashboard. If this final manual check passes, we will flag the LLM from the model registry as accepted.A CI/CD pipeline will trigger and deploy the new LLM version to the inference pipeline.Where will the training pipeline be deployed?The training pipeline will be deployed to Qwak.Qwak is a serverless solution for training and deploying ML models. It makes scaling your operation easy while you can focus on building.Also, we will use the freemium version of Comet ML for the following:experiment tracker;model registry;prompt monitoring.3.4. The inference pipelineThe inference pipeline is the final component of the LLM system. It is the one the clients will interact with.It will be wrapped under a REST API. The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools.How do we access the features?To access the feature store, we will use the same Qdrant vector DB retrieval clients as in the training pipeline.In this case, we will need the feature store to access the chunked data to do RAG.How do we access the fine-tuned LLM?The fine-tuned LLM will always be downloaded from the model registry based on its tag (e.g., accepted) and version (e.g., v1.0.2, latest, etc.).How will the fine-tuned LLM be loaded?Here we are in the inference world.Thus, we want to optimize the LLM's speed and memory consumption as much as possible. That is why, after downloading the LLM from the model registry, we will quantize it.What are the components of the inference pipeline?The first one is the retrieval client used to access the vector DB to do RAG. This is the same module as the one used in the training pipeline.After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt.After the LLM generates its answer, we will log it to Comets prompt monitoring dashboard and return it to the clients.For example, the client will request the inference pipeline to:Write a 1000-word LinkedIn post about LLMs, and the inference pipeline will go through all the steps above to return the generated post.Where will the inference pipeline be deployed?The inference pipeline will be deployed to Qwak.By default, Qwak also offers autoscaling solutions and a nice dashboard to monitor all the production environment resources.As for the training pipeline, we will use a serverless freemium version of Comet for its prompt monitoring dashboard.ConclusionThis is the 1st article of the LLM Twin: Building Your Production-Ready AI Replica free course.In this lesson, we presented what you will build during the course.After we briefly discussed how to design ML systems using the 3-pipeline design.Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineIn Lesson 2, we will dive deeper into the data collection pipeline, learn how to implement crawlers for various social media platforms, clean the gathered data, store it in a Mongo DB, and finally, show you how to deploy it to AWS.  Check out the code on GitHub [1] and support us with a Have you enjoyed this article? Then Join 5k+ engineers in the Decoding ML Newsletter for battle-tested content on production-grade ML. Every week:Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. Fordecodingml.substack.comReferences[1] Your LLM Twin Course  GitHub Repository (2024), Decoding ML GitHub Organization[2] Introducing new AI experiences from Meta (2023), Meta[3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks[4] Extract Transform Load (ETL), Databricks Glossary[5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), SuperlinkedSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipAccess the best member-only stories.Support independent authors.Listen to audio narrations.Read offline.Join the Partner Program and earn for your writing.Try for $5/monthGenerative AiLarge Language ModelsMlopsArtificial IntelligenceMachine Learning1.5K1.5K10FollowWritten by Paul Iusztin2.4K FollowersEditor for Decoding ML Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Decoding ML Newsletter: [URL] from Paul Iusztin and Decoding MLPaul IusztininDecoding MLA Real-time Retrieval System for RAG on Social Media DataUse a streaming engine to populate a vector DB in real-time. Improve RAG accuracy using rerank & UMAP.12 min readMar 30, 2024305Razvant AlexandruinDecoding MLHow to build a Real-Time News Search Engine using Serverless Upstash Kafka and Vector DBA hands-on guide to implementing a live news aggregating streaming pipeline with Apache Kafka, Bytewax, and Upstash Vector Database.19 min readApr 13, 20246873Vesa AlexandruinDecoding MLThe Importance of Data Pipelines in the Era of Generative AIFrom unstructured data crawling to structured valuable data17 min readMar 23, 20244343Paul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streamingLesson 1: LLM architecture system design using the 3-pipeline pattern12 min readJan 5, 2024383See all from Paul IusztinSee all from Decoding MLRecommended from MediumAlex HoncharinTowards Data ScienceIntro to LLM Agents with Langchain: When RAG is Not EnoughFirst-order principles of brain structure for AI assistants7 min readMar 15, 20242K10Fabio MatricardiinGenerative AILlama3 is out and you can run it on your Computer!After only 1 day from the release, here is how you can run even on your Laptop with CPU only the latest Meta-AI model.8 min read6 days ago1.2K13ListsAI Regulation6 stories427 savesNatural Language Processing1402 stories899 savesPredictive Modeling w/ Python20 stories1126 savesPractical Guides to Machine Learning10 stories1352 savesVipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.49 min readApr 18, 20243741Mandar Karhade, MD. PhD.inTowards AIWhy RAG Applications Fail in ProductionIt worked as a prototype; then all went down!7 min readMar 19, 20241K11Gavin LiinAI AdvancesRun the strongest open-source LLM model: Llama3 70B with just a single 4GB GPU!The strongest open source LLM model Llama3 has been released, Here is how you can run Llama3 70B locally with just 4GB GPU, even on Macbook4 min read4 days ago7982Bijit GhoshRAG Vs VectorDBIntroduction to RAG and VectorDB14 min readJan 28, 20242384See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams To make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."}, {"instruction": "Use a Python streaming engine to populate a feature store from 4+ data sources", "content": "SOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG  in Real-Time!Use a Python streaming engine to populate a feature store from 4+ data sourcesStreaming Pipelines for LLMs and RAG | Decoding MLOpen in appSign upSign inWriteSign upSign inMastodonLLM TWIN COURSE: BUILDING YOUR PRODUCTION-READY AI REPLICASOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG  in Real-Time!Use a Python streaming engine to populate a feature store from 4+ data sourcesPaul IusztinFollowPublished inDecoding ML18 min read6 days ago698ListenShare  the 4th out of 11 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the LLM Twin: Building Your Production-Ready AI Replica free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care?   No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish  from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:the data collection pipeline: crawl your digital data from various social media platforms. Clean, normalize and load the data to a NoSQL DB through a series of ETL pipelines. Send database changes to a queue using the CDC pattern. (deployed on AWS)the feature pipeline: consume messages from a queue through a Bytewax streaming pipeline. Every message will be cleaned, chunked, embedded (using Superlinked), and loaded into a Qdrant vector DB in real-time. (deployed on AWS)the training pipeline: create a custom dataset based on your digital data. Fine-tune an LLM using QLoRA. Use Comet MLs experiment tracker to monitor the experiments. Evaluate and save the best model to Comets model registry. (deployed on Qwak)the inference pipeline: load and quantize the fine-tuned LLM from Comets model registry. Deploy it as a REST API. Enhance the prompts using RAG. Generate content using your LLM twin. Monitor the LLM using Comets prompt monitoring dashboard. (deployed on Qwak)LLM twin system architecture [Image by the Author]Along the 4 microservices, you will learn to integrate 3 serverless tools:Comet ML as your ML Platform;Qdrant as your vector DB;Qwak as your ML infrastructure;Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM systems using LLMOps good principles.Level: intermediatePrerequisites: basic knowledge of Python, ML, and the cloudHow will you learn?The course contains 11 hands-on written lessons and the open-source code you can access on GitHub.You can read everything at your own pace.  To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you have to know that we use several cloud tools that might generate additional costs.The cloud computing platforms (AWS, Qwak) have a pay-as-you-go pricing plan. Qwak offers a few hours of free computing. Thus, we did our best to keep costs to a minimum.For the other serverless tools (Qdrant, Comet), we will stick to their freemium version, which is free of charge.Meet your teachers!The course is created under the Decoding ML umbrella by:Paul Iusztin | Senior ML & MLOps EngineerAlex Vesa | Senior AI EngineerAlex Razvant | Senior ML & MLOps EngineerLessonsThe course is split into 11 lessons. Every Medium article will be its own lesson.An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture: Enabling Event-Driven ArchitecturesSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG  in Real-Time!Vector DB retrieval clients [Module 2] WIPTraining data preparation [Module 3] WIPFine-tuning LLM [Module 3] WIPLLM evaluation [Module 4] WIPQuantization [Module 5] WIPBuild the digital twin inference pipeline [Module 6] WIPDeploy the digital twin as a REST API [Module 6] WIP  Check out the code on GitHub [1] and support us with a Lets start with Lesson 4  Lesson 4: Python Streaming Pipelines for Fine-tuning LLMs and RAG  in Real-Time!In the 4th lesson, we will focus on the feature pipeline.The feature pipeline is the first pipeline presented in the 3 pipeline architecture: feature, training and inference pipelines.A feature pipeline is responsible for taking raw data as input, processing it into features, and storing it in a feature store, from which the training & inference pipelines will use it.The component is completely isolated from the training and inference code. All the communication is done through the feature store.To avoid repeating myself, if you are unfamiliar with the 3 pipeline architecture, check out Lesson 1 for a refresher.By the end of this article, you will learn to design and build a production-ready feature pipeline that:uses Bytewax as a stream engine to process data in real-time;ingests data from a RabbitMQ queue;uses SWE practices to process multiple data types: posts, articles, code;cleans, chunks, and embeds data for LLM fine-tuning and RAG;loads the features to a Qdrant vector DB.Note: In our use case, the feature pipeline is also a streaming pipeline, as we use a Bytewax streaming engine. Thus, we will use these words interchangeably.We will wrap up Lesson 4 by showing you how to deploy the feature pipeline to AWS and integrate it with the components from previous lessons: data collection pipeline, MongoDB, and CDC.In the 5th lesson, we will go through the vector DB retrieval client, where we will teach you how to query the vector DB and improve the accuracy of the results using advanced retrieval techniques.Excited? Lets get started!The architecture of the feature/streaming pipeline.Table of ContentsWhy are we doing this?System design of the feature pipelineThe Bytewax streaming flowPydantic data modelsLoad data to QdrantThe dispatcher layerPreprocessing steps: Clean, chunk, embedThe AWS infrastructureRun the code locallyDeploy the code to AWS & Run it from the cloudConclusion  Check out the code on GitHub [1] and support us with a 1. Why are we doing this?A quick reminder from previous lessonsTo give you some context, in Lesson 2, we crawl data from LinkedIn, Medium, and GitHub, normalize it, and load it to MongoDB.In Lesson 3, we are using CDC to listen to changes to the MongoDB database and emit events in a RabbitMQ queue based on any CRUD operation done on MongoDB.and here we are in Lesson 4, where we are building the feature pipeline that listens 24/7 to the RabbitMQ queue for new events to process and load them to a Qdrant vector DB.The problem we are solvingIn our LLM Twin use case, the feature pipeline constantly syncs the MongoDB warehouse with the Qdrant vector DB while processing the raw data into features.Important: In our use case, the Qdrant vector DB will be our feature store.Why we are solving itThe feature store will be the central point of access for all the features used within the training and inference pipelines.For consistency and simplicity, we will refer to different formats of our text data as features.  The training pipeline will use the feature store to create fine-tuning datasets for your LLM twin.  The inference pipeline will use the feature store for RAG.For reliable results (especially for RAG), the data from the vector DB must always be in sync with the data from the data warehouse.The question is, what is the best way to sync these 2?Other potential solutionsThe most common solution is probably to use a batch pipeline that constantly polls from the warehouse, computes a difference between the 2 databases, and updates the target database.The issue with this technique is that computing the difference between the 2 databases is extremely slow and costly.Another solution is to use a push technique using a webhook. Thus, on any CRUD change in the warehouse, you also update the source DB.The biggest issue here is that if the webhook fails, you have to implement complex recovery logic.Lesson 3 on CDC covers more of this.2. System design of the feature pipeline: our solutionOur solution is based on CDC, a queue, a streaming engine, and a vector DB:  CDC adds any change made to the Mongo DB to the queue (read more in Lesson 3).  the RabbitMQ queue stores all the events until they are processed.  The Bytewax streaming engine cleans, chunks, and embeds the data.  A streaming engine works naturally with a queue-based system.  The data is uploaded to a Qdrant vector DB on the flyWhy is this powerful?Here are 4 core reasons:The data is processed in real-time.Out-of-the-box recovery system: If the streaming pipeline fails to process a message will be added back to the queueLightweight: No need for any diffs between databases or batching too many recordsNo I/O bottlenecks on the source database  It solves all our problems!The architecture of the feature/streaming pipeline.How is the data stored?We store 2 snapshots of our data in the feature store. Here is why  Remember that we said that the training and inference pipeline will access the features only from the feature store, which, in our case, is the Qdrant vector DB?Well, if we had stored only the chunked & embedded version of the data, that would have been useful only for RAG but not for fine-tuning.Thus, we make an additional snapshot of the cleaned data, which will be used by the training pipeline.Afterward, we pass it down the streaming flow for chunking & embedding.How do we process multiple data types?How do you process multiple types of data in a single streaming pipeline without writing spaghetti code?Yes, that is for you, data scientists! Jokingam I?We have 3 data types: posts, articles, and code.Each data type (and its state) will be modeled using Pydantic models.To process them we will write a dispatcher layer, which will use a creational factory pattern [9] to instantiate a handler implemented for that specific data type (post, article, code) and operation (cleaning, chunking, embedding).The handler follows the strategy behavioral pattern [10].Intuitively, you can see the combination between the factory and strategy patterns as follows:Initially, we know we want to clean the data, but as we dont know the data type, we cant know how to do so.What we can do, is write the whole code around the cleaning code and abstract away the login under a Handler() interface (aka the strategy).When we get a data point, the factory class creates the right cleaning handler based on its type.Ultimately the handler is injected into the rest of the system and executed.By doing so, we can easily isolate the logic for a given data type & operation while leveraging polymorphism to avoid filling up the code with 1000x if else statements.We will dig into the implementation in future sections.Streaming over batchYou may ask why we need a streaming engine instead of implementing a batch job that polls the messages at a given frequency.That is a valid question.The thing is thatNowadays, using tools such as Bytewax makes implementing streaming pipelines a lot more frictionless than using their JVM alternatives.The key aspect of choosing a streaming vs. a batch design is real-time synchronization between your source and destination DBs.In our particular case, we will process social media data, which changes fast and irregularly.Also, for our digital twin, it is important to do RAG on up-to-date data. We dont want to have any delay between what happens in the real world and what your LLM twin sees.That being said choosing a streaming architecture seemed natural in our use case.3. The Bytewax streaming flowThe Bytewax flow is the central point of the streaming pipeline. It defines all the required steps, following the next simplified pattern: input -> processing -> output.As I come from the AI world, I like to see it as the graph of the streaming pipeline, where you use the input(), map(), and output() Bytewax functions to define your graph, which in the Bytewax world is called a flow.As you can see in the code snippet below, we ingest posts, articles or code messages from a RabbitMQ queue. After we clean, chunk and embed them. Ultimately, we load the cleaned and embedded data to a Qdrant vector DB, which in our LLM twin use case will represent the feature store of our system.To structure and validate the data, between each Bytewax step, we map and pass a different Pydantic model based on its current state: raw, cleaned, chunked, or embedded.Bytewax flow   GitHub Code We have a single streaming pipeline that processes everything.As we ingest multiple data types (posts, articles, or code snapshots), we have to process them differently.To do this the right way, we implemented a dispatcher layer that knows how to apply data-specific operations based on the type of message.More on this in the next sections  Why Bytewax?Bytewax is an open-source streaming processing framework that:- is built in Rust  for performance- has Python   bindings for leveraging its powerful ML ecosystem so, for all the Python fanatics out there, no more JVM headaches for you.Jokes aside, here is why Bytewax is so powerful  - Bytewax local setup is plug-and-play- can quickly be integrated into any Python project (you can go wild  even use it in Notebooks)- can easily be integrated with other Python packages (NumPy, PyTorch, HuggingFace, OpenCV, SkLearn, you name it)- out-of-the-box connectors for Kafka and local files, or you can quickly implement your ownWe used Bytewax to build the streaming pipeline for the LLM Twin course and loved it.To learn more about Bytewax, go and check them out. They are open source, so no strings attached   Bytewax [2] 4. Pydantic data modelsLets take a look at what our Pydantic models look like.First, we defined a set of base abstract models for using the same parent class across all our components.Pydantic base model structure   GitHub Code Afterward, we defined a hierarchy of Pydantic models for:all our data types: posts, articles, or codeall our states: raw, cleaned, chunked, and embeddedThis is how the set of classes for the posts will look like  Pydantic posts model structure   GitHub Code We repeated the same process for the articles and code model hierarchy.Check out the other data classes on our GitHub.Why is keeping our data in Pydantic models so powerful?There are 4 main criteria:every field has an enforced type: you are ensured the data types are going to be correctthe fields are automatically validated based on their type: for example, if the field is a string and you pass an int, it will through an errorthe data structure is clear and verbose: no more clandestine dicts that you never know what is in themyou make your data the first-class citizen of your program5. Load data to QdrantThe first step is to implement our custom Bytewax DynamicSink class  Qdrant DynamicSink   GitHub Code Next, for every type of operation we need (output cleaned or embedded data ) we have to subclass the StatelessSinkPartition Bytewax class (they also provide a stateful option   more in their docs)An instance of the class will run on every partition defined within the Bytewax deployment.In the course, we are using a single partition per worker. But, by adding more partitions (and workers), you can quickly scale your Bytewax pipeline horizontally.Qdrant worker partitions   GitHub Code Note that we used Qdrants Batch method to upload all the available points at once. By doing so, we reduce the latency on the network I/O side: more on that here [8] The RabbitMQ streaming input follows a similar pattern. Check it out here 6. The dispatcher layerNow that we have the Bytewax flow and all our data models.How do we map a raw data model to a cleaned data model?  All our domain logic is modeled by a set of Handler() classes.For example, this is how the handler used to map a PostsRawModel to a PostCleanedModel looks like  Handler hierarchy of classes   GitHub Code Check out the other handlers on our GitHub:  ChunkingDataHandler and EmbeddingDataHandlerIn the next sections, we will explore the exact cleaning, chunking and embedding logic.Now, to build our dispatcher, we need 2 last components:a factory class: instantiates the right handler based on the type of the eventa dispatcher class: the glue code that calls the factory class and handlerHere is what the cleaning dispatcher and factory look like  The dispatcher and factory classes   GitHub Code Check out the other dispatchers on our GitHub.By repeating the same logic, we will end up with the following set of dispatchers:RawDispatcher (no factory class required as the data is not processed)CleaningDispatcher (with a ChunkingHandlerFactory class)ChunkingDispatcher (with a ChunkingHandlerFactory class)EmbeddingDispatcher (with an EmbeddingHandlerFactory class)7. Preprocessing steps: Clean, chunk, embedHere we will focus on the concrete logic used to clean, chunk, and embed a data point.Note that this logic is wrapped by our handler to be integrated into our dispatcher layer using the Strategy behavioral pattern [10].We already described that in the previous section. Thus, we will directly jump into the actual logic here, which can be found in the utils module of our GitHub repository.Note: These steps are experimental. Thus, what we present here is just the first iteration of the system. In a real-world scenario, you would experiment with different cleaning, chunking or model versions to improve it on your data.CleaningThis is the main utility function used to clean the text for our posts, articles, and code.Out of simplicity, we used the same logic for all the data types, but after more investigation, you would probably need to adapt it to your specific needs.For example, your posts might start containing some weird characters, and you dont want to run the unbold_text() or unitalic_text() functions on your code data point as is completely redundant.Cleaning logic   GitHub Code Most of the functions above are from the unstructured [3] Python package. It is a great tool for quickly finding utilities to clean text data.  More examples of unstructured here [3] One key thing to notice is that at the cleaning step, we just want to remove all the weird, non-interpretable characters from the text.Also, we want to remove redundant data, such as extra whitespace or URLs, as they do not provide much value.These steps are critical for our tokenizer to understand and efficiently transform our string input into numbers that will be fed into the transformer models.Note that when using bigger models (transformers) + modern tokenization techniques, you dont need to standardize your dataset too much.For example, it is redundant to apply lemmatization or stemming, as the tokenizer knows how to split your input into a commonly used sequence of characters efficiently, and the transformers can pick up the nuances of the words.  What is important at the cleaning step is to throw out the noise.ChunkingWe are using Langchain to chunk our text.We use a 2 step strategy using Langchains RecursiveCharacterTextSplitter [4] and SentenceTransformersTokenTextSplitter [5]. As seen below  Chunking logic   GitHub Code Overlapping your chunks is a common pre-indexing RAG technique, which helps to cluster chunks from the same document semantically.Again, we are using the same chunking logic for all of our data types, but to get the most out of it, we would probably need to tweak the separators, chunk_size, and chunk_overlap parameters for our different use cases.But our dispatcher + handler architecture would easily allow us to configure the chunking step in future iterations.EmbeddingThe data preprocessing, aka the hard part is done.Now we just have to call an embedding model to create our vectors.Embedding logic   GitHub Code We used the all-MiniLm-L6-v2 [6] from the sentence-transformers library to embed our articles and posts: a lightweight embedding model that can easily run in real-time on a 2 vCPU machine.As the code data points contain more complex relationships and specific jargon to embed, we used a more powerful embedding model: hkunlp/instructor-xl [7].This embedding model is unique as it can be customized on the fly with instructions based on your particular data. This allows the embedding model to specialize on your data without fine-tuning, which is handy for embedding pieces of code.8. The AWS infrastructureIn Lesson 2, we covered how to deploy the data collection pipeline that is triggered by a link to Medium, Substack, LinkedIn or GitHub   crawls the given link   saves the crawled information to a MongoDB.In Lesson 3, we explained how to deploy the CDC components that emit events to a RabbitMQ queue based on any CRUD operation done to MongoDB.What is left is to deploy the Bytewax streaming pipeline and Qdrant vector DB.We will use Qdrants self-hosted option, which is easy to set up and scale.To test things out, they offer a Free Tier plan for up to a 1GB cluster, which is more than enough for our course.  We explained in our GitHub repository how to configure Qdrant.AWS infrastructure of the feature/streaming pipeline.The last piece of the puzzle is the Bytewax streaming pipeline.As we dont require a GPU and the streaming pipeline needs to run 24/7, we will deploy it to AWS Fargate, a cost-effective serverless solution from AWS.As a serverless solution, Fargate allows us to deploy our code quickly and scale it fast in case of high traffic.How do we deploy the streaming pipeline code to Fargate?Using GitHub Actions, we wrote a CD pipeline that builds a Docker image on every new commit made on the main branch.After, the Docker image is pushed to AWS ECR. Ultimately, Fargate pulls the latest version of the Docker image.This is a common CD pipeline to deploy your code to AWS services.Why not use lambda functions, as we did for the data pipeline?An AWS lambda function executes a function once and then closes down.This worked perfectly for the crawling logic, but it won't work for our streaming pipeline, which has to run 24/7.9. Run the code locallyTo quickly test things up, we wrote a docker-compose.yaml file to spin up the MongoDB, RabbitMQ queue and Qdrant vector db.You can spin up the Docker containers using our Makefile by running the following:make local-start-infraTo fully test the Bytewax streaming pipeline, you have to start the CDC component by running:make local-start-cdcUltimately, you start the streaming pipeline:make local-bytewaxTo simulate the data collection pipeline, mock it as follows:make local-insert-data-mongoThe README of our GitHub repository provides more details on how to run and set up everything.10. Deploy the code to AWS & Run it from the cloudThis article is already too long, so I wont go into the details of how to deploy the AWS infrastructure described above and test it out here.But to give you some insights, we have used Pulumi as our infrastructure as a code (IaC) tool, which will allow you to spin it quickly with a few commands.Also, I wont let you hang on to this one. We made a promise and  We prepared step-by-step instructions in the README of our GitHub repository on how to use Pulumni to spin up the infrastructure and test it out.ConclusionNow you know how to write streaming pipelines like a PRO!In Lesson 4, you learned how to:design a feature pipeline using the 3-pipeline architecturewrite a streaming pipeline using Bytewax as a streaming engineuse a dispatcher layer to write a modular and flexible application to process multiple types of data (posts, articles, code)load the cleaned and embedded data to Qdrantdeploy the streaming pipeline to AWS  This is only the ingestion part used for fine-tuning LLMs and RAG.In Lesson 5, you will learn how to write a retrieval client for the 3 data types using good SWE practices and improve the retrieval accuracy using advanced retrieval & post-retrieval techniques. See you there!  Check out the code on GitHub [1] and support us with a Enjoyed This Article?Join the Decoding ML Newsletter for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For FREE  Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. Fordecodingml.substack.comReferencesLiterature[1] Your LLM Twin Course  GitHub Repository (2024), Decoding ML GitHub Organization[2] Bytewax, Bytewax Landing Page[3] Unstructured Cleaning Examples, Unstructured Documentation[4] Recursively split by character, LangChains Documentation[5] Split by tokens, LangChains Documentation[6] sentence-transformers/all-MiniLM-L6-v2, HuggingFace[7] hkunlp/instructor-xl, HuggingFace[8] Qdrant, Qdrant Documentation[9] Abstract Factory Pattern, Refactoring Guru[10] Strategy Pattern, Refactoring GuruImagesIf not otherwise stated, all images are created by the author.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipAccess the best member-only stories.Support independent authors.Listen to audio narrations.Read offline.Join the Partner Program and earn for your writing.Try for $5/monthMl System DesignMachine LearningArtificial IntelligenceData ScienceSoftware Engineering698698FollowWritten by Paul Iusztin2.4K FollowersEditor for Decoding ML Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Decoding ML Newsletter: [URL] from Paul Iusztin and Decoding MLPaul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.16 min readMar 16, 20241.5K10Razvant AlexandruinDecoding MLHow to build a Real-Time News Search Engine using Serverless Upstash Kafka and Vector DBA hands-on guide to implementing a live news aggregating streaming pipeline with Apache Kafka, Bytewax, and Upstash Vector Database.19 min readApr 13, 20246873Paul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streamingLesson 1: LLM architecture system design using the 3-pipeline pattern12 min readJan 5, 2024383Paul IusztininDecoding MLA Real-time Retrieval System for RAG on Social Media DataUse a streaming engine to populate a vector DB in real-time. Improve RAG accuracy using rerank & UMAP.12 min readMar 30, 2024305See all from Paul IusztinSee all from Decoding MLRecommended from MediumVipra SinghBuilding LLM Applications: Serving LLMs (Part 9)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.49 min readApr 18, 20243741A B Vijay KumarMulti-Agent System  Crew.AIMulti-Agent systems are LLM applications that are changing the automation landscape with intelligent bots.7 min readApr 18, 20241574ListsPredictive Modeling w/ Python20 stories1126 savesNatural Language Processing1402 stories899 savesPractical Guides to Machine Learning10 stories1352 savesdata science and AI40 stories137 savesFabio MatricardiinGenerative AILlama3 is out and you can run it on your Computer!After only 1 day from the release, here is how you can run even on your Laptop with CPU only the latest Meta-AI model.8 min read6 days ago1.2K13Gavin LiinAI AdvancesRun the strongest open-source LLM model: Llama3 70B with just a single 4GB GPU!The strongest open source LLM model Llama3 has been released, Here is how you can run Llama3 70B locally with just 4GB GPU, even on Macbook4 min read4 days ago7982Plaban NayakinThe AI ForumSemantic Chunking for RAGWhat is Chunking ?17 min read4 days ago1651Damian GilinTowards Data ScienceAdvanced Retriever Techniques to Improve Your RAGsMaster Advanced Information Retrieval: Cutting-edge Techniques to Optimize the Selection of Relevant Documents with Langchain to Create18 min readApr 17, 20245393See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."}, {"instruction": "Create battle-tested content on building production-grade ML systems leveraging good SWE and MLOps practices, following the entire lifecycle of an ML system from system design to deploying and monitoring.", "content": "About Decoding MLThe hub for continuous learning on ML system design, ML engineering, MLOps, LLMs and computer vision.About Decoding ML. The hub for continuous learning on ML | by Paul Iusztin | Decoding ML | MediumOpen in appSign upSign inWriteSign upSign inMastodonAbout Decoding MLThe hub for continuous learning on ML system design, ML engineering, MLOps, LLMs and computer vision.Paul IusztinFollowPublished inDecoding ML2 min readFeb 19, 202491ListenShareDecoding ML is a publication that creates battle-tested content on building production-grade ML systems leveraging good SWE and MLOps practices.Our motto is More engineering, less F1 scores.Following Decoding ML, you will learn about the entire lifecycle of an ML system, from system design to deploying and monitoring.Decoding ML is the hub for continuous learning on:ML system designML engineeringMLOpsLarge language modelsComputer visionWe are all about end-to-end ML use cases that can directly be applied in the real world  no stories  just hands-on content.The minds behind Decoding ML  Alex Vesa (left), Paul Iusztin (middle) and Alexandru Razvant (right)The minds behind Decoding ML are Vesa Alexandru, Paul Iusztin and Razvant Alexandru.Our passion for constantly learning and engineering production-ready ML systems inevitably turned into Decoding ML.With our 10+ years of hands-on experience in the industry in:Data ScienceDeep LearningComputer VisionGenerative AIML InfrastructureMLOpsSoftware Engineeringwe decided it is about time to share it with the world!Why follow?Join Decoding ML for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For FREE.No more bedtime stories in Jupyter Notebooks.  DML is all about hands-on advice from our 10+ years of experience in AI.We are also on: Newsletter  GitHubSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipAccess the best member-only stories.Support independent authors.Listen to audio narrations.Read offline.Join the Partner Program and earn for your writing.Try for $5/monthAbout9191FollowWritten by Paul Iusztin2.4K FollowersEditor for Decoding ML Senior ML Engineer | Helping machine learning engineers design and productionize ML systems. | Decoding ML Newsletter: [URL] from Paul Iusztin and Decoding MLPaul IusztininDecoding MLAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.16 min readMar 16, 20241.5K10Razvant AlexandruinDecoding MLHow to build a Real-Time News Search Engine using Serverless Upstash Kafka and Vector DBA hands-on guide to implementing a live news aggregating streaming pipeline with Apache Kafka, Bytewax, and Upstash Vector Database.19 min readApr 13, 20246873Paul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streamingLesson 1: LLM architecture system design using the 3-pipeline pattern12 min readJan 5, 2024383Paul IusztininDecoding MLA Real-time Retrieval System for RAG on Social Media DataUse a streaming engine to populate a vector DB in real-time. Improve RAG accuracy using rerank & UMAP.12 min readMar 30, 2024305See all from Paul IusztinSee all from Decoding MLRecommended from MediumQwakHow to Build an End-to-End ML Pipeline in 2024Learn to build an end-to-end ML pipeline and streamline your ML workflows in 2024, from data ingestion to model deployment and performance24 min readApr 7, 20242071Paul IusztininDecoding MLThe LLMs kit: Build a production-ready real-time financial advisor system using streamingLesson 1: LLM architecture system design using the 3-pipeline pattern12 min readJan 5, 2024383ListsMedium's Huge List of Publications Accepting Submissions285 stories2516 savesYesmine RouisinTheFork Engineering BlogA Guide to MLOps with Airflow and MLflowIntroduction11 min readNov 6, 2023319ManralaiMastering MLOps for FreeYour Go-To GitHub Repositories Collection4 min readJan 28, 20246463Vipra SinghBuilding LLM Applications: Evaluation (Part 8)Learn Large Language Models ( LLM ) through the lens of a Retrieval Augmented Generation ( RAG ) Application.47 min readApr 8, 20241891Ramazan OlmezEnd-to-End Machine Learning Project: Churn PredictionThe main objective of this article is to develop an end-to-end machine learning project. For a model to be truly useful, it needs to be18 min readFeb 22, 202487See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy."}]