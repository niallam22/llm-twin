[
  {
    "instruction": "Introduce a Python package that significantly speeds up text preprocessing pipelines for ML applications.",
    "content": "Using this 𝗣𝘆𝘁𝗵𝗼𝗻 𝗽𝗮𝗰𝗸𝗮𝗴𝗲, you can 𝘅𝟭𝟬 your 𝘁𝗲𝘅𝘁 𝗽𝗿𝗲𝗽𝗿𝗼𝗰𝗲𝘀𝘀𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲𝘀. \n\nAny text preprocessing pipeline has to clean, partition, extract, or chunk text data to feed it into your LLMs. \n\n𝘂𝗻𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲𝗱 offers a 𝗿𝗶𝗰𝗵 and 𝗰𝗹𝗲𝗮𝗻 𝗔𝗣𝗜 that allows you to quickly: \n\n- 𝘱𝘢𝘳𝘵𝘪𝘵𝘪𝘰𝘯 your data into smaller segments from various data sources (e.g., HTML, CSV, PDFs, even images, etc.) \n- 𝘤𝘭𝘦𝘢𝘯𝘪𝘯𝘨 the text of anomalies (e.g., wrong ASCII characters), any irrelevant information (e.g., white spaces, bullets, etc.), and filling missing values \n- 𝘦𝘹𝘵𝘳𝘢𝘤𝘵𝘪𝘯𝘨 information from pieces of text (e.g., datetimes, addresses, IP addresses, etc.) \n- 𝘤𝘩𝘶𝘯𝘬𝘪𝘯𝘨 your text segments into pieces of text that can be inserted into your embedding model \n- 𝘦𝘮𝘣𝘦𝘥𝘥𝘪𝘯𝘨 data (e.g., wrapper over OpenAIEmbeddingEncoder, HuggingFaceEmbeddingEncoders, etc.) \n- 𝘴𝘵𝘢𝘨𝘦 your data to be fed into various tools (e.g., Label Studio, Label Box, etc.) \n\n𝗔𝗹𝗹 𝘁𝗵𝗲𝘀𝗲 𝘀𝘁𝗲𝗽𝘀 𝗮𝗿𝗲 𝗲𝘀𝘀𝗲𝗻𝘁𝗶𝗮𝗹 𝗳𝗼𝗿: \n\n- feeding your data into your LLMs \n- embedding the data and ingesting it into a vector DB \n- doing RAG \n- labeling \n- recommender systems \n\n... basically for any LLM or multimodal applications \n\n. \n\nImplementing all these steps from scratch will take a lot of time. \n\nI know some Python packages already do this, but the functionality is scattered across multiple packages. \n\n𝘂𝗻𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲𝗱 packages everything together under a nice, clean API. \n\n↳ Check it out: 🔗 \nhttps://lnkd.in/dCACT3K2 \n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience \n\n. \n\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Share a personal achievement related to your growth on LinkedIn and reflect on your journey.",
    "content": "Hello there 👋 I recently passed 𝟮𝟱𝗸+ 𝗳𝗼𝗹𝗹𝗼𝘄𝗲𝗿𝘀 on LinkedIn. \n\nThis is not a bragging post. \n\nI want to say a big \"𝘛𝘩𝘢𝘯𝘬 𝘺𝘰𝘶!\" to everyone who followed me and found my content helpful. \n\nI am incredibly grateful for that. \n\nI started posting more consistently ~1 year ago when I knew nothing about writing content or social media in general. \n\nThe heck, I wasn't spending time on social media... \n\nI will be honest with you. I am still shocked that some of my posts reached >100k views. \n\n. \n\nAnyhow, I will continue improving my craft, pretending I know what I do, and providing high-quality content on: \n\n- ML engineering \n- MLOps \n- LLMs \n- Python & Software Engineering applied in the world of ML \n\nI am here for the long run. \n\n. \n\n𝘉𝘢𝘴𝘦𝘥 𝘰𝘯 𝘺𝘰𝘶𝘳 𝘦𝘯𝘨𝘢𝘨𝘦𝘮𝘦𝘯𝘵, 𝘩𝘦𝘳𝘦 𝘢𝘳𝘦 𝘴𝘰𝘮𝘦 𝘰𝘧 𝘮𝘺 𝘣𝘦𝘴𝘵 𝘱𝘰𝘴𝘵𝘴 𝘧𝘳𝘰𝘮 𝘵𝘩𝘦 𝘭𝘢𝘵𝘦𝘴𝘵 𝘺𝘦𝘢𝘳: \n\n- The internet is full of 𝗹𝗲𝗮𝗿𝗻𝗶𝗻𝗴 𝗿𝗲𝘀𝗼𝘂𝗿𝗰𝗲𝘀 about 𝗟𝗟𝗠𝘀 & 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕𝘀. But 𝗺𝗼𝘀𝘁 𝗼𝗳 𝗶𝘁 is 𝘁𝗿𝗮𝘀𝗵: 🔗 https://lnkd.in/dPvmUH6G \n\n- To successfully use 𝗥𝗔𝗚 in your 𝗟𝗟𝗠 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀, your 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕 must constantly be updated with the latest data: 🔗 https://lnkd.in/d7gH_SeX \n\n- Writing your own ML models is history: 🔗 https://lnkd.in/ddhxpDAe \n\n- Want to 𝗹𝗲𝗮𝗿𝗻 𝗠𝗟 & 𝗠𝗟𝗢𝗽𝘀 𝗶𝗻 𝗮 𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲𝗱 𝘄𝗮𝘆?: 🔗 https://lnkd.in/dqv4d3-w \n\n- The whole field of prompt engineering can be reduced to these 6 techniques I use almost daily when using ChatGPT (or other LLMs): 🔗 https://lnkd.in/drr9C5cy \n\n- Want to build your first 𝗟𝗟𝗠 𝗽𝗿𝗼𝗷𝗲𝗰𝘁 but don't know where to start?: 🔗 https://lnkd.in/dhfKcYkh \n\n. \n\nAgain, \"Thank you\" for following me and let's master ML engineering together 🔥"
  },
  {
    "instruction": "Discuss strategies for adding real-time monitoring to ML systems on LinkedIn, focusing on different scenarios based on the availability of ground truth data. Highlight the importance of continuous model performance evaluation for maintaining system integrity.",
    "content": "𝗛𝗼𝘄 𝘁𝗼 𝗮𝗱𝗱 𝗿𝗲𝗮𝗹-𝘁𝗶𝗺𝗲 𝗺𝗼𝗻𝗶𝘁𝗼𝗿𝗶𝗻𝗴 𝗮𝗻𝗱 𝗺𝗲𝘁𝗿𝗶𝗰𝘀 to your ML system.\nYour model is exposed to performance degradation after it is deployed to production.\nThat is why you need to monitor it constantly.\nThe most common way to monitor an ML model is to compute its metrics.\nBut for that, you need the ground truth.\n𝗜𝗻 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻, 𝘆𝗼𝘂 𝗰𝗮𝗻 𝗮𝘂𝘁𝗼𝗺𝗮𝘁𝗶𝗰𝗮𝗹𝗹𝘆 𝗮𝗰𝗰𝗲𝘀𝘀 𝘁𝗵𝗲 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵 𝗶𝗻 𝟯 𝗺𝗮𝗶𝗻 𝘀𝗰𝗲𝗻𝗮𝗿𝗶𝗼𝘀:\n1. near real-time: you can access it quite quickly\n2. delayed: you can access it after a considerable amount of time (e.g., one month)\n3. never: you have to label the data manually\n.\n𝗙𝗼𝗿 𝘂𝘀𝗲 𝗰𝗮𝘀𝗲𝘀 𝟮. 𝗮𝗻𝗱 𝟯. 𝘆𝗼𝘂 𝗰𝗮𝗻 𝗾𝘂𝗶𝗰𝗸𝗹𝘆 𝗰𝗼𝗺𝗽𝘂𝘁𝗲 𝘆𝗼𝘂𝗿 𝗺𝗼𝗻𝗶𝘁𝗼𝗿𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗶𝗻 𝘁𝗵𝗲 𝗳𝗼𝗹𝗹𝗼𝘄𝗶𝗻𝗴 𝘄𝗮𝘆:\n- store the model predictions and GT as soon as they are available (these 2 will be out of sync -> you can't compute the metrics right away)\n- build a DAG (e.g., using Airflow) that extracts the predictions & GT computes the metrics in batch mode and loads them into another storage (e.g., GCS)\n- use an orchestration tool to run the DAG in the following scenarios:\n1. scheduled: if the GT is available in near real-time (e.g., hourly), then it makes sense to run your monitoring pipeline based on the known frequency\n2. triggered: if the GT is delayed and you don't know when it may come up, then you can implement a webhook to trigger your monitoring pipeline\n- attach a consumer to your storage to use and display the metrics (e.g., trigger alarms and display them in a dashboard)\n.\nIf you want to see how to implement a near real-time monitoring pipeline using Airflow and GCS, check out my article: 𝘌𝘯𝘴𝘶𝘳𝘪𝘯𝘨 𝘛𝘳𝘶𝘴𝘵𝘸𝘰𝘳𝘵𝘩𝘺 𝘔𝘓 𝘚𝘺𝘴𝘵𝘦𝘮𝘴 𝘞𝘪𝘵𝘩 𝘋𝘢𝘵𝘢 𝘝𝘢𝘭𝘪𝘥𝘢𝘵𝘪𝘰𝘯 𝘢𝘯𝘥 𝘙𝘦𝘢𝘭-𝘛𝘪𝘮𝘦 𝘔𝘰𝘯𝘪𝘵𝘰𝘳𝘪𝘯𝘨. Link in the comments ↓\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Post a LinkedIn update reflecting on the inherent challenges in programming, with a special emphasis on the complexities encountered in AI. Use the hashtags #machinelearning, #artificialintelligence, and #mlops to engage with relevant communities.",
    "content": "The saddest truth about programming.\nEven amplified if you work in AI.\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelliegence\nhashtag\n#\nmlops"
  },
  {
    "instruction": "Announce on LinkedIn the upcoming release of comprehensive lessons from The Full Stack 7-Steps MLOps Framework. Encourage engagement by using the hashtags #learning, #machinelearning, and #mlops.",
    "content": "Close to releasing all the lessons from The Full Stack 7-Steps MLOps Framework.\nhashtag\n#\nlearning\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops"
  },
  {
    "instruction": "Share on LinkedIn insights from Paul Iusztin's series on a 7-step MLOps framework, focusing on the latest installment about model predictions consumption and visualization. Enhance the post with an image to attract more views.",
    "content": "As\nPaul Iusztin\n's series continues to explore a 7-step MLOps framework, a new installment explains how to consume and visualize your model's predictions."
  },
  {
    "instruction": "Discuss the importance of building a 'second brain' especially in data and AI fields on LinkedIn. Highlight key takeaways from Tiago Forte's book on productivity and organizing information, using hashtags #productivity and #secondbrain.",
    "content": "I am always anxious that I forget what I read.\nThat is why I built my second brain.\nIf you are working in any data/AI-related field, you are constantly bombarded with information.\nMuch of it is noise,  but some will be of great value.\n.\nHere are the main ideas I extracted from the 𝘉𝘶𝘪𝘭𝘥𝘪𝘯𝘨 𝘢 𝘚𝘦𝘤𝘰𝘯𝘥 𝘉𝘳𝘢𝘪𝘯 𝘣𝘺 𝘛𝘪𝘢𝘨𝘰 𝘍𝘰𝘳𝘵𝘦 famous book 👇\n#𝟭. Your brain's job is to plan, create and enjoy life, not to remember things.\n#𝟮. Four steps to follow:\n1. Capture the relevant ideas\n2. Organize by actionability\n3. Distill your notes to prevent from hoarding\n4. Use & express your work\n#𝟯. Follow the PARA system to organize your information:\nP - Projects: bounded responsibilities\nA - Areas: timeless responsibilities\nR - Resources: useful data\nA - Archive: your graveyard\n#𝟰. Main productivity tools:\n- calendar\n- TODOist\n- note-taking app\n- Notion: the central hub\nThe tools are not revolutionary.\nYou need the right mindset to apply them consistently in the right way.\n.\nTo conclude...\nYou must create a robust system to capture & use all the essential information from your environment.\nOtherwise, you will overload your brain, forgetting important information and wasting opportunities.\nDo you have a second brain?\nhashtag\n#\nproductivity\nhashtag\n#\nsecondbrain"
  },
  {
    "instruction": "On LinkedIn, stress the critical nature of validating data at every stage in the ML model lifecycle. Share a detailed post with an image illustrating the concept and use hashtags #data, #machinelearning, and #mlops.",
    "content": "A wise man said: 𝘃𝗮𝗹𝗶𝗱𝗮𝘁𝗲 𝗲𝘃𝗲𝗿𝘆𝘁𝗵𝗶𝗻𝗴!\n100% you heard that data validation is good...\nbut where should we validate the data? Everywhere!\nThat might be an overstatement, but let me explain.\nWhen the outputs of an ML model are poor, there are 1000+ reasons why that happened.\nBut even if you know that the issue is data related...\nNarrowing down to the actual function that messed up everything is extremely hard.\nThus, by adding data validation before & after:\n- the ingestion ETL;\n- the data engineering pipeline;\n- the feature engineering pipeline;\nyou might add some redundancy, but this will make scanning for errors extremely easy.\n.\nImagine that you would have a data validation check only after the FE pipeline. If that fails, you know it failed 𝘣𝘶𝘵 𝘥𝘰𝘯'𝘵 𝘬𝘯𝘰𝘸 𝘸𝘩𝘦𝘳𝘦 𝘪𝘵 𝘧𝘢𝘪𝘭𝘦𝘥.\nIf the system is small, that is not an issue, but imagine you have 100+ transformations spread across multiple teams...\n🥲 Finding the right error might take you hours or even days.\n💛 By adding multiple data validation points in your system, you can quickly answer to: \"where the system failed\".\nThus, by adding data validation in multiple, you automatically slice the pipeline making it easy to diagnose.\n.\nNote that this is just an example. Your data infrastructure might look different.\nBut the fundamental idea remains the same. Add data validation in all the essential points of your data pipelines to quickly slice and dice the upcoming errors.\nIf you want a hands-on example about using GE to validate your data, check out my article 𝘌𝘯𝘴𝘶𝘳𝘪𝘯𝘨 𝘛𝘳𝘶𝘴𝘵𝘸𝘰𝘳𝘵𝘩𝘺 𝘔𝘓 𝘚𝘺𝘴𝘵𝘦𝘮𝘴 𝘞𝘪𝘵𝘩 𝘋𝘢𝘵𝘢 𝘝𝘢𝘭𝘪𝘥𝘢𝘵𝘪𝘰𝘯 𝘢𝘯𝘥 𝘙𝘦𝘢𝘭-𝘛𝘪𝘮𝘦 𝘔𝘰𝘯𝘪𝘵𝘰𝘳𝘪𝘯𝘨. Link in the comments 👇\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n💡 Follow me if you want to level up in designing and productionizing ML systems."
  },
  {
    "instruction": "Promote a weekly newsletter on real-world ML and MLOps by Pau Labarta Bajo on LinkedIn, highlighting its value for ML developers. Include the newsletter's link and use relevant hashtags to attract interested audiences.",
    "content": "Do you want to learn ML & MLOps from real-world experience?\nThen I suggest you join\nPau Labarta Bajo\nReal-World Machine Learning\nweekly newsletter, along with another 7.5 ML developer.\nPau Labarta Bajo\nis a great teacher that makes learning seamless ✌\n→\nhttps://lnkd.in/dXx-GxN8\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nmlops"
  },
  {
    "instruction": "Share a concise guide to data validation in ML systems on LinkedIn, emphasizing the simplicity and effectiveness of using Great Expectations. Encourage followers to learn more through an article linked in the comments.",
    "content":  "𝗗𝗮𝘁𝗮 𝘃𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻 shouldn't be hard.\nHere is your data validation guide in under 2 minutes 👇\nData validation ensures the integrity and quality of your data ingested automatically into your ML system.\nThus, implementing your data validation layer is crucial in any successful ML system.\n.\n🧘🏼‍♂️ 𝘎𝘳𝘦𝘢𝘵 𝘌𝘹𝘱𝘦𝘤𝘵𝘢𝘵𝘪𝘰𝘯𝘴 makes everything straightforward.\nUsing GE, you must stack multiple 𝘌𝘹𝘱𝘦𝘤𝘵𝘢𝘵𝘪𝘰𝘯𝘊𝘰𝘯𝘧𝘪𝘨𝘶𝘳𝘢𝘵𝘪𝘰𝘯 objects, where each object checks a single rule/feature.\nFor example:\n\"\"\"\nExpectationConfiguration(\nexpectation_type=\"expect_column_distinct_values_to_be_in_set\",\nkwargs={\"column\": \"area\", \"value_set\": (0, 1, 2)},\n)\n\"\"\"\n, checks if the \"area\" feature contains only values equal to 0, 1 or 2.\nThe most common checks you have to do are for the following:\n- the schema of the table;\n- the type of each column;\n- the values of each column: an interval for continuous variables or an expected set for discrete variables;\n- null values.\n.\nAfter you run your GE validation suit, you will get a success %.\nBased on the success % you can make various decisions, such as:\n🟢  == 100% - ingest the data without an alert\n🟡  >=90% - ingest the data with an alert\n🔴   <90% - drop the data with an error\nP.S. Using GE +\nHopsworks\nas your Feature Store makes everything even simpler 🔥\n.\nSo remember...\nGE makes implementing your data validation layer straightforward.\nYou have to check every feature for a given set of rules.\nBased on the success % you have to take various actions.\nIf you want a hands-on example about using GE to validate your data, check out my article 𝘌𝘯𝘴𝘶𝘳𝘪𝘯𝘨 𝘛𝘳𝘶𝘴𝘵𝘸𝘰𝘳𝘵𝘩𝘺 𝘔𝘓 𝘚𝘺𝘴𝘵𝘦𝘮𝘴 𝘞𝘪𝘵𝘩 𝘋𝘢𝘵𝘢 𝘝𝘢𝘭𝘪𝘥𝘢𝘵𝘪𝘰𝘯 𝘢𝘯𝘥 𝘙𝘦𝘢𝘭-𝘛𝘪𝘮𝘦 𝘔𝘰𝘯𝘪𝘵𝘰𝘳𝘪𝘯𝘨. Link in the comments 👇\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n💡 Follow me if you want to level up in designing and productionizing ML systems."
  },
  {
    "instruction": "Educate your LinkedIn network on mastering Terraform for clean and modular infrastructure management in ML projects, highlighting the use of variables. Use relevant hashtags to reach a wider audience interested in #machinelearning, #artificialintelligence, and #mlops.",
    "content": "Master writing clean & modular Terraform files.\nUsing this one simple technique:\nVariables\nLet's take a look at how to:\n- define a variable\n- reference a variable\n- assign a value to a variable\n- use files to assign values to variables\n... in Terraform.\nNote that assigning values in Terraform is quite strange, as in your Terraform file, you define the structure and type of the variable. The value is assigned only on runtime.\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelliegence\nhashtag\n#\nmlops\n-----\n💡 Follow me if you want to level up in designing and productionizing ML systems."
  },
  {
    "instruction": "Discuss the common starting point of serving ML models in batch mode on LinkedIn, covering the essential components and transitions to other architectures. Provide insights for ML engineers on optimizing model performance.",
    "content": "90% of the ML models start being served in batch mode.\nThus, as an ML engineer, learning to train and serve a model in batch mode successfully is your first step to success.\nYou need to know how to implement the following:\n- a feature engineering pipeline\n- a training pipeline\n- a batch prediction pipeline\n- a feature store\n- an ML Platform\n- a storage system to store your predictions\nMany ML systems start in batch mode and naturally move toward other architectures.\nThis happens because serving an ML model in batch mode is the fastest without spending too much time on constraints such as low latency and high throughput.\nRemember that while in production, if your model isn't running fast enough, it will probably be useless.\nIf you want to learn how to implement the steps above step-by-step practically,\nI want to let you know that...\nI finally released the first 3 lessons for  \"The Full Stack 7-Steps MLOps Framework\" FREE course that will teach you just that.\nThe lessons are accessible on Towards Data Science Medium's publication.\nCheck the comments for the links...\nAnd start learning about training and serving your ML model in batch mode.\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops"
  },
  {
    "instruction": "On LinkedIn, share insights on the advantages of serving ML models using a batch architecture, focusing on the initial deployment phase. Emphasize the process and its impact on system flexibility and user experience.",
    "content": "Why serving an ML model using a batch architecture is so powerful?\nWhen you first start deploying your ML model, you want an initial end-to-end flow as fast as possible.\nDoing so lets you quickly provide value, get feedback, and even collect data.\nBut here is the catch...\nSuccessfully serving an ML model is tricky as you need many iterations to optimize your model to work in real-time:\n- low latency\n- high throughput\nInitially, serving your model in batch mode is like a hack.\nBy storing the model's predictions in dedicated storage, you automatically move your model from offline mode to a real-time online model.\nThus, you no longer have to care for your model's latency and throughput. The consumer will directly load the predictions from the given storage.\n𝐓𝐡𝐞𝐬𝐞 𝐚𝐫𝐞 𝐭𝐡𝐞 𝐦𝐚𝐢𝐧 𝐬𝐭𝐞𝐩𝐬 𝐨𝐟 𝐚 𝐛𝐚𝐭𝐜𝐡 𝐚𝐫𝐜𝐡𝐢𝐭𝐞𝐜𝐭𝐮𝐫𝐞:\n- extracts raw data from a real data source\n- clean, validate, and aggregate the raw data within a feature pipeline\n- load the cleaned data into a feature store\n- experiment to find the best model + transformations using the data from the feature store\n- upload the best model from the training pipeline into the model registry\n- inside a batch prediction pipeline, use the best model from the model registry to compute the predictions\n- store the predictions in some storage\n- the consumer will download the predictions from the storage\n- repeat the whole process hourly, daily, weekly, etc. (it depends on your context)\nThe main downside of deploying your model in batch mode is that the predictions will have a level of lag.\nFor example, in a recommender system, if you make your predictions daily, it won't capture a user's behavior in real time, and it will update the predictions only at the end of the day.\nThat is why moving to other architectures, such as request-response or streaming, will be natural after your system matures in batch mode.\nSo remember, when you initially deploy your model, using a batch mode architecture will be your best shot for a good user experience.\nLet me know in the comments what your strategy is.\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n💡 My goal is to help machine learning engineers level up in designing and productionizing ML systems.\n👉 Follow me for weekly insights."
  },
  {
    "instruction": "Reveal three lesser-known methods for data transformation using a feature store in ML production systems on LinkedIn. Discuss the trade-offs and benefits of each method, inviting feedback from your network.",
    "content": "These are 3 ways you didn't know about how you can transform your data when using a feature store.\nA feature store helps you quickly solve the training serving skew issue by offering you a consistent way to transform your data into features between the training and inference pipelines.\nThe issue boils down to WHEN you do the transformation.\nWhen using a feature store, there are 3 main ways how you can transform your data:\n𝟏. 𝐁𝐞𝐟𝐨𝐫𝐞 𝐬𝐭𝐨𝐫𝐢𝐧𝐠 𝐭𝐡𝐞 𝐝𝐚𝐭𝐚 𝐢𝐧 𝐭𝐡𝐞 𝐟𝐞𝐚𝐭𝐮𝐫𝐞 𝐬𝐭𝐨𝐫𝐞\nIn the feature engineering pipeline, you do everything: clean, validate, aggregate, reduce, and transform your data.\nEven if this is the most intuitive way of doing things, it is the worse.\n🟢 ultra-low latency\n🔴 hard to do EDA on transformed data\n🔴 store duplicated/redundant data\n𝟐. 𝐒𝐭𝐨𝐫𝐞 𝐭𝐡𝐞 𝐭𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧 𝐢𝐧 𝐲𝐨𝐮𝐫 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 𝐨𝐫 𝐦𝐨𝐝𝐞𝐥 𝐩𝐫𝐞-𝐩𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 𝐥𝐚𝐲𝐞𝐫𝐬\nIn the feature engineering pipeline, you perform only the cleaning, validation, aggregations, and reductions steps.\nLater, by incorporating all your transformations into your pipeline object or pre-processing layers, you automatically save them along your model.\nThus, you can input your cleaned data into your pipeline, and it will know how to handle it.\n🟢 store only cleaned data\n🟢 easily explore your data\n🔴 the transformations are done on the client\n𝟑. 𝐘𝐨𝐮 𝐚𝐭𝐭𝐚𝐜𝐡 𝐭𝐨 𝐞𝐯𝐞𝐫𝐲 𝐜𝐥𝐞𝐚𝐧𝐞𝐝 𝐝𝐚𝐭𝐚 𝐬𝐨𝐮𝐫𝐜𝐞 𝐚 𝐔𝐃𝐅 𝐭𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧\nThis is similar to solution 2., but instead of attaching the transformation directly to your model, you attached them as a UDF to the feature store.\nfeature = cleaned data source + UDF\nSo when you request a feature, the feature store will automatically trigger the UDF on a server and return it.\n🟢 store only cleaned data\n🟢 easily explore your data\n🟢 the transformations are done on the server\n🟢 scalable (using Spark)\n🔴 hard to implement\nAs a recap,\nThere are 3 ways when you can perform your transformations to solve the train serving skew when using a feature store.\nWhat method do you think is the best?\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\nI just started my ML engineering weekly newsletter.\nSubscribe to level up in building ML systems:\nhttps://lnkd.in/dsMR4ivA"
  },
  {
    "instruction": "Announce the launch of a weekly ML engineering newsletter on LinkedIn, emphasizing the commitment to providing value on ML system design and MLOps. Encourage sign-ups through a link and use relevant hashtags to attract interested readers.",
    "content": "After 3 months of overthinking...\nI finally did it.\nAs I value people's time, and I understand that everybody has their way of reading...\nI want to announce,\nThat I will start my free weekly ML engineering newsletter.\n.\nThe mission of my newsletter will be the same as my LinkedIn content:\n\"To help machine learning engineers level up in designing and productionizing ML systems.\"\nI will do my best to provide the best value for your time.\n.\nIf you never want to miss my weekly insights about ML engineering,\nSubscribe to my free newsletter here:\nhttps://lnkd.in/dsMR4ivA\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops"
  },
  {
    "instruction": "Summarize the top three ML monitoring resources discovered after reviewing over 100 articles on LinkedIn. Share links to these resources for followers seeking to deepen their understanding of ML system monitoring.",
    "content": "In the last month, I read 100+ ML monitoring articles.\nI trimmed them for you to 3 key resources:\n1. A series of excellent articles made by\nArize AI\nthat will make you understand what ML monitoring is all about.\nLink:\nhttps://lnkd.in/dDVWRujh\n2. The\nEvidently AI\nBlog, where you can find answers to all your questions regarding ML monitoring.\nLink:\nhttps://lnkd.in/du35hWp2\n3. The monitoring hands-on examples hosted by\nDataTalksClub\nwill teach you how to implement an ML monitoring system.\nLink:\nhttps://lnkd.in/d4ziHhxH\nAfter wasting a lot of time reading other resources...\nUsing these 3 resources is a solid start for learning about monitoring ML systems.\nHave you tried them?\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nmonitoring\n-----\n💡 My goal is to help machine learning engineers level up in designing and productionizing ML systems."
  },
  {
    "instruction": "Celebrate reaching a follower milestone on LinkedIn while sharing personal insights and tips on brand building and engagement. Highlight the importance of consistency and adapting to feedback in content creation.",
    "content": "My dreams came true ✌️\nWhile on vacation, I had the wonderful surprise of passing 5k followers on LinkedIn.\nAs a side note, Amsterdam is a beautiful place, and it 100% deserves to be on most of the go-to tourist destinations.\nFirst of all, I want to thank everybody who is reading my content. I am doing my best to provide you with value about ML engineering, building ML systems and MLOps.\nSecondly, I want to keep the tradition of sharing the key steps that got me here:\n1. Consistency when you have <10 likes\nWhen you are just starting, you probably haven’t built trust around your online persona.\nThus most likely, you will be writing into the void.\nJust don’t get discouraged. At some point, your content will gain traction.\n2. Just start and adapt along the way\nIt is hard to find your best niche, plan, etc., without any real feedback.\nStop overthinking and start posting.\nBut always listen and adapt to see what suits you and your audience better.\nAlways learn ✌️\nSo...\nConsistency, regardless of your engagement and taking action when you think you don’t know what you are doing are my two best pieces of advice for building your brand on LinkedIn.\nWhat do you think about my advice? Would you add something else?\nhashtag\n#\nmachinelearning\nhashtag\n#\nlinkedin\nhashtag\n#\nbrand\n---\nIf you want to learn more about ML engineering, ML systems, and MLOps...\nFollow me on LinkedIn and Medium 👇"
  },
  {
    "instruction": "On LinkedIn, emphasize the critical practice of versioning data in MLOps for managing changes in model embeddings and data processes. Discuss the types of changes and the importance of data versioning for clarity and system integrity.",
    "content": "This is one MLOps practice you 100% have to know.\nMany engineers ignore it,\nbut let me explain why it is so important.\nI showed you one post before about how to extract an embedding from your model.\nI showed you a few examples of extraction methods for various models.\nBut what happens if you want to change the extraction method or model to compare the performance?\nMost probably, it will soon become a mess.\nWe all encountered situations such as: \"final_model,\" \"best_final_model,\" \"best_final_final_model,\" etc. You get the idea... It is tough to keep track of our changes.\n.\n3 types of changes can occur when extracting embeddings:\n#𝟏. 𝐂𝐡𝐚𝐧𝐠𝐞 𝐲𝐨𝐮𝐫 𝐦𝐨𝐝𝐞𝐥 𝐚𝐫𝐜𝐡𝐢𝐭𝐞𝐜𝐭𝐮𝐫𝐞\nThis is considered a major change: O.x.x\nChanging your model architecture might change the semantics of the embeddings and their dimensionality. Also, as a by-product, it changes your extraction method, and you must retrain your model.\n#𝟐. 𝐂𝐡𝐚𝐧𝐠𝐞 𝐲𝐨𝐮𝐫 𝐞𝐱𝐭𝐫𝐚𝐜𝐭𝐢𝐨𝐧 𝐭𝐲𝐩𝐞\nThis is considered a minor change: x.O.x\nAgain this might result in changes in your semantics or dimensionality, but you don't have to retrain your model.\n#𝟑. 𝐑𝐞𝐭𝐫𝐚𝐢𝐧 𝐲𝐨𝐮𝐫 𝐦𝐨𝐝𝐞𝐥\nThis is considered a patch version change: x.x.O\nThis won't change your embedding structure, but by retraining, they won't be compatible with your old set of embeddings as the vector space might change.\n.\nAs you see, your embeddings will change quite often, that is why you need to...\nVersion your data!\nData versioning is one key aspect of a clean ML system.\nEvery change will result in a new data version. Then, when you use a specific set of embeddings, you will know exactly how they were computed.\nYou can easily version your data directly in the feature store for structured data. You can quickly add data versioning for unstructured data using tools such as S3 + DVC/your custom software.\nTo conclude...\nYou should fire up a new version of your data whenever you change your data process.\nHow are you versioning your data?\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n💡 My goal is to help machine learning engineers level up in designing and productionizing ML systems.\n👉 Follow me for weekly insights."
  },
  {
    "instruction": "Announce the completion of the \"Full Stack 7-Steps MLOps Framework\" series on LinkedIn, offering a comprehensive guide on designing, implementing, and deploying ML systems with MLOps best practices. Invite followers to access the course and share their projects.",
    "content": "After 3 months of hard work and no sleep...\nI can finally say that I know everything about MLOps.\nHehe, not really, but I have put in a lot of work and...\nI finally finished my hands-on \"Full Stack 7-Steps MLOps Framework\" series.\nThis a step-by-step course that will explain to you how to design, implement, and deploy an ML system using MLOps good practices.\nAt the end of the 7 lessons course, you will know how to:\n- design a batch-serving architecture\n- use Hopsworks as a feature store\n- design a feature engineering pipeline that reads data from an API\n- build a training pipeline with hyper-parameter tunning\n- use W&B as an ML Platform to track your experiments, models, and metadata\n- implement a batch prediction pipeline\n- use Poetry to build your own Python packages\n- deploy your own private PyPi server\n- orchestrate everything with Airflow\n- use the predictions to code a web app using FastAPI and Streamlit\n- use Docker to containerize your code\n- use Great Expectations to ensure data validation and integrity\n- monitor the performance of the predictions over time\n- deploy everything to GCP\n- build a CI/CD pipeline using GitHub Actions\nIf this sounds interesting to you.\nI want you to know that...\nThe course is free and published on Medium under the\nTowards Data Science\npublication so that anybody can level up their ML engineering journey.\nWant to start building your project?\nJust check out the GitHub repository and the first lesson of the series in the comments.\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nlearning\n-----\nI want to thank @Kurtis Pykes for helping me build this series and being an excellent copilot. It was a great collaboration."
  },
  {
    "instruction": "Highlight the importance of careful data validation and monitoring for maintaining the health of ML models in production on LinkedIn. Share an image summarizing the five main criteria to watch, using relevant hashtags to engage the community.",
    "content": "ML is more than training, evaluating and beating metrics.\nTo keep your model healthy while in production, you must carefully validate and monitor your data.\nThese are the 5 main criteria you have to be careful about.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nmonitoring"
  }
]