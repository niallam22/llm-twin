[
  {
    "instruction": "I need a LinkedIn Post of about 250-300 words about learning how to become an engineer for ML Systems in which you promote decodingml. Write it structured using lists.",
    "content": "Want to 𝗹𝗲𝗮𝗿𝗻 to 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻-𝗿𝗲𝗮𝗱𝘆 𝗠𝗟 𝘀𝘆𝘀𝘁𝗲𝗺𝘀 but don't know where to start?\n\nA lot of content on ML is in a Jupyter Notebook and revolves around building excellent models using some static datasets.\n\nThat is great.\n\n𝘉𝘶𝘵 𝘵𝘩𝘢𝘵 𝘸𝘰𝘯'𝘵 𝘩𝘦𝘭𝘱 𝘺𝘰𝘶 𝘣𝘶𝘪𝘭𝘥 𝘳𝘦𝘢𝘭-𝘸𝘰𝘳𝘭𝘥 𝘔𝘓 𝘴𝘺𝘴𝘵𝘦𝘮𝘴 𝘵𝘩𝘢𝘵 𝘸𝘪𝘭𝘭 𝘱𝘳𝘰𝘷𝘪𝘥𝘦 𝘳𝘦𝘢𝘭 𝘷𝘢𝘭𝘶𝘦 𝘢𝘵 𝘺𝘰𝘶𝘳 𝘫𝘰𝘣.\n\nAs Alex Vesa said in his previous post: \"ML engineering is 10% machine learning and 90% engineering.\"\n\n𝗧𝗵𝗲 𝘀𝗼𝗹𝘂𝘁𝗶𝗼𝗻?\n\n𝘋𝘦𝘤𝘰𝘥𝘪𝘯𝘨 𝘔𝘓\n\n.\n\nAs Alex Vesa and Alexandru Razvant started to write alongside me at Decoding ML, we grew the topics we will talk about in Decoding ML.\n\nThe 𝗽𝗿𝗶𝗺𝗮𝗿𝘆 𝗻𝗶𝗰𝗵𝗲 is the same: \"Battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems.\"\n\nBut with their 10+ years of experience in AI, we started creating content on:\n\n- LLM, RAG, prompt engineering, LLM evaluation\n- Computer vision, 3D object detection & segmentation, tracking\n\nBut with a 𝗳𝗼𝗰𝘂𝘀 on 𝗵𝗼𝘄 to 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿 the 𝘀𝘆𝘀𝘁𝗲𝗺 𝗮𝗿𝗼𝘂𝗻𝗱 the 𝗺𝗼𝗱𝗲𝗹𝘀 by using 𝗠𝗟𝗢𝗽𝘀 and 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴 𝗴𝗼𝗼𝗱 𝗽𝗿𝗮𝗰𝘁𝗶𝗰𝗲𝘀:\n\n- Python\n- ML system design\n- SageMaker\n- TensortRT, ONNX\n- Docker, CI/CD, version control\n- experiment trackers, model registries\n- feature stories, data lineage, vector DBs\n- orchestration tools\n- deployment & autoscaling strategies\n- model training & serving \n- monitoring & observability\n\n...and the best part is that we will do our best to 𝗽𝗿𝗼𝘃𝗶𝗱𝗲 the 𝗰𝗼𝗱𝗲 for all our articles.\n\nSo...\n\n𝘞𝘢𝘯𝘵 𝘵𝘰 𝘭𝘦𝘢𝘳𝘯 𝘵𝘰 𝘣𝘶𝘪𝘭𝘥 𝘱𝘳𝘰𝘥𝘶𝘤𝘵𝘪𝘰𝘯-𝘨𝘳𝘢𝘥𝘦 𝘔𝘓 & 𝘔𝘓𝘖𝘱𝘴 𝘴𝘺𝘴𝘵𝘦𝘮𝘴?\n\n𝗦𝘂𝗯𝘃𝗰𝗿𝗶𝗯𝗲 𝘁𝗼 𝘁𝗵𝗲 𝗗𝗲𝗰𝗼𝗱𝗶𝗻𝗴 𝗠𝗟 𝗡𝗲𝘄𝘀𝗹𝗲𝘁𝘁𝗲𝗿. 𝗙𝗼𝗿 𝗙𝗥𝗘𝗘 ↓\n\n🔗 Decoding ML Newsletter: https://lnkd.in/dQpjSXKc\n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience"
  },
  {
    "instruction": "Please write me a LinkedIn Post of about 400 words about fine tuning open source llms: and about fine-tuning vs prompt engineering. Highlight the need to fine tune, why do we need it and the difference between it and prompt engineering.",
    "content": "𝗪𝗵𝘆 & 𝘄𝗵𝗲𝗻 do you need to 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗲 𝗼𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝗟𝗟𝗠𝘀? What about 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴 𝘃𝘀. 𝗽𝗿𝗼𝗺𝗽𝘁 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴? \n \nFine-tuning is the process of taking a pre-trained model and further refining it on a specific task. \n \n𝗙𝗶𝗿𝘀𝘁, 𝗹𝗲𝘁'𝘀 𝗰𝗹𝗮𝗿𝗶𝗳𝘆 𝘄𝗵𝗮𝘁 𝗺𝗲𝘁𝗵𝗼𝗱𝘀 𝗼𝗳 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴 𝗮𝗻 𝗼𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝗟𝗟𝗠 𝗲𝘅𝗶𝘀t ↓ \n \n- 𝘊𝘰𝘯𝘵𝘪𝘯𝘶𝘦𝘥 𝘱𝘳𝘦-𝘵𝘳𝘢𝘪𝘯𝘪𝘯𝘨: utilize domain-specific data to apply the same pre-training process (next token prediction) on the pre-trained (base) model \n- 𝘐𝘯𝘴𝘵𝘳𝘶𝘤𝘵𝘪𝘰𝘯 𝘧𝘪𝘯𝘦-𝘵𝘶𝘯𝘪𝘯𝘨: the pre-trained (base) model is fine-tuned on a Q&A dataset to learn to answer questions \n- 𝘚𝘪𝘯𝘨𝘭𝘦-𝘵𝘢𝘴𝘬 𝘧𝘪𝘯𝘦-𝘵𝘶𝘯𝘪𝘯𝘨: the pre-trained model is refined for a specific task, such as toxicity detection, coding, medicine advice, etc. \n- 𝘙𝘓𝘏𝘍: It requires collecting human preferences (e.g., pairwise comparisons), which are then used to train a reward model. The reward model is used to fine-tune the LLM via RL techniques such as PPO. \n \nCommon approaches are to take a pre-trained LLM (next-word prediction) and apply instruction & single-task fine-tuning. \n \n𝗪𝗵𝘆 𝗱𝗼 𝘆𝗼𝘂 𝗻𝗲𝗲𝗱 𝘁𝗼 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗲 𝘁𝗵𝗲 𝗟𝗟𝗠? \n \nYou do instruction fine-tuning to make the LLM learn to answer your questions. \n \nThe exciting part is when you want to fine-tune your LLM on a single task. \n \nHere is why ↓ \n \n𝘱𝘦𝘳𝘧𝘰𝘳𝘮𝘢𝘯𝘤𝘦: it will improve your LLM performance on given use cases (e.g., coding, extracting text, etc.). Mainly, the LLM will specialize in a given task (a specialist will always beat a generalist in its domain) \n \n𝘤𝘰𝘯𝘵𝘳𝘰𝘭: you can refine how your model should behave on specific inputs and outputs, resulting in a more robust product \n \n𝘮𝘰𝘥𝘶𝘭𝘢𝘳𝘪𝘻𝘢𝘵𝘪𝘰𝘯: you can create an army of smaller models, where each is specialized on a particular task, increasing the overall system's performance. Usually, when you fine-tune one task, it reduces the performance of the other tasks (known as the alignment tax). Thus, having an expert system of multiple smaller models can improve the overall performance. \n \n𝗪𝗵𝗮𝘁 𝗮𝗯𝗼𝘂𝘁 𝗽𝗿𝗼𝗺𝗽𝘁 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴 𝘃𝘀 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴? \n \n𝘥𝘢𝘵𝘢: use prompting when you don't have data available (~2 examples are enough). Fine-tuning needs at least >=100 examples to work. \n \n𝘤𝘰𝘴𝘵: prompting forces you to write long & detailed prompts to achieve your level of performance. You pay per token (API or compute-wise). Thus, when a prompt gets bigger, your costs increase. But, when fine-tuning an LLM, you incorporate all that knowledge inside the model. Hence, you can use smaller prompts with similar performance. \n \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience"
  },
  {
    "instruction": "Write me a LinkedIn Post of about 250 - 300 words about  learning MLE and mMLOps. In here you are trying to promovate your MLOps course showcasing its pros.",
    "content": "Want to learn 𝗠𝗟𝗘 & 𝗠𝗟𝗢𝗽𝘀 in a 𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲𝗱 𝘄𝗮𝘆, for 𝗳𝗿𝗲𝗲, and with 𝗵𝗮𝗻𝗱𝘀-𝗼𝗻 𝗲𝘅𝗮𝗺𝗽𝗹𝗲𝘀? \n \nThen you should check out my 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 FREE course. \n \n. \n \nIn 𝟮.𝟱 𝗵𝗼𝘂𝗿𝘀 𝗼𝗳 𝗿𝗲𝗮𝗱𝗶𝗻𝗴 & 𝘃𝗶𝗱𝗲𝗼 𝗺𝗮𝘁𝗲𝗿𝗶𝗮𝗹𝘀, you will 𝗹𝗲𝗮𝗿𝗻 𝗵𝗼𝘄 𝘁𝗼: \n \n- design a batch-serving architecture \n- use Hopsworks as a feature store \n- design a feature engineering pipeline that reads data from an API \n- build a training pipeline with hyper-parameter tunning \n- use W&B as an ML Platform to track your experiments, models, and metadata \n- implement a batch prediction pipeline \n- use Poetry to build your own Python packages \n- deploy your own private PyPi server \n- orchestrate everything with Airflow \n- use the predictions to code a web app using FastAPI and Streamlit \n- use Docker to containerize your code \n- use Great Expectations to ensure data validation and integrity \n- monitor the performance of the predictions over time \n- deploy everything to GCP \n- build a CI/CD pipeline using GitHub Actions \n- trade-offs & future improvements discussion \n \n...where all the pieces are integrated into a single end-to-end ML system that forecasts hourly energy levels across Denmark. \n \n𝗬𝗼𝘂 𝗰𝗮𝗻 𝗮𝗰𝗰𝗲𝘀𝘀 𝘁𝗵𝗲 𝗰𝗼𝘂𝗿𝘀𝗲 𝗼𝗻: \n➝ 𝘔𝘦𝘥𝘪𝘶𝘮'𝘴 𝘛𝘋𝘚 𝘱𝘶𝘣𝘭𝘪𝘤𝘢𝘵𝘪𝘰𝘯: text tutorials + videos \n➝ 𝘎𝘪𝘵𝘏𝘶𝘣: open-source code + docs \n \nThe course is on Medium's TDS publication to make it easily accessible to people worldwide. Thus ↓ \n \n... anyone can learn the fundamentals of MLE & MLOps. \n \nSo, no more excuses. Just go and build your own project 🔥 \n \nCheck it out ↓ \n \n🔗 𝘛𝘩𝘦 𝘍𝘶𝘭𝘭 𝘚𝘵𝘢𝘤𝘬 7-𝘚𝘵𝘦𝘱𝘴 𝘔𝘓𝘖𝘱𝘴 𝘍𝘳𝘢𝘮𝘦𝘸𝘰𝘳𝘬: \nhttps://lnkd.in/d_GVpZ9X \n \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience "
  },
  {
    "instruction": "Write a LinkedIn post explaining a 7-step process on how to fine-tune an open-source LLM to create a real-time financial advisor.",
    "content": "𝟳-𝘀𝘁𝗲𝗽𝘀 on 𝗵𝗼𝘄 to 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗲 an open-source 𝗟𝗟𝗠 to create your real-time financial advisor ↓\n\nIn the past weeks, we covered 𝘄𝗵𝘆 you must fine-tune an LLM and 𝘄𝗵𝗮𝘁 resources & tools you need.\n\n.\n\nNow, let's see how you can hook all the pieces together into a single fine-tuning module ↓\n\n𝟭. 𝗟𝗼𝗮𝗱 𝘁𝗵𝗲 𝗤&𝗔 𝗱𝗮𝘁𝗮𝘀𝗲𝘁\n\nOur Q&A samples have the following structure keys: \"about_me,\" \"user_context,\" \"question,\" and \"answer.\"\n\nFor task-specific fine-tuning, you need only 100-1000 samples. Thus, you can directly load the whole JSON in memory.\n\nAfter you map every sample to a list of Python 𝘥𝘢𝘵𝘢𝘤𝘭𝘢𝘴𝘴𝘦𝘴 to validate the structure & type of the ingested instances.\n\n𝟮. 𝗣𝗿𝗲𝗽𝗿𝗼𝗰𝗲𝘀𝘀 𝘁𝗵𝗲 𝗤&𝗔 𝗱𝗮𝘁𝗮𝘀𝗲𝘁 𝗶𝗻𝘁𝗼 𝗽𝗿𝗼𝗺𝗽𝘁𝘀\n\nThe first step is to use 𝘶𝘯𝘴𝘵𝘳𝘶𝘤𝘵𝘶𝘳𝘦𝘥 to clean every sample by removing redundant characters.\n\nAfter, as every sample consists of multiple fields, you must map it to a single piece of text, known as the prompt.\n\nTo do so, you define a 𝘗𝘳𝘰𝘮𝘱𝘵𝘛𝘦𝘮𝘱𝘭𝘢𝘵𝘦 class to manage all your prompts. You will use it to map all the sample keys to a prompt using f-strings.\n\nThe last step is to map the list of Python 𝘥𝘢𝘵𝘢𝘤𝘭𝘢𝘴𝘴𝘦𝘴 to a HuggingFace dataset and map every sample to a prompt, as discussed above.\n\n𝟯. 𝗟𝗼𝗮𝗱 𝘁𝗵𝗲 𝗟𝗟𝗠 𝘂𝘀𝗶𝗻𝗴 𝗤𝗟𝗼𝗥𝗔\n\nLoad a pretrained Falcon 7B LLM by passing a 𝘣𝘪𝘵𝘴𝘢𝘯𝘥𝘣𝘺𝘵𝘦𝘴 quantization configuration that loads all the weights on 4 bits.\n\nAfter using LoRA, you freeze the weights of the original Falcon LLM and attach a set of trainable adapters to it.\n\n𝟰. 𝗙𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴\n\nThe 𝘵𝘳𝘭 package makes this step simple.\n\nYou pass to the 𝘚𝘍𝘛𝘛𝘳𝘢𝘪𝘯𝘦𝘳 class the training arguments, the dataset and the model and call the 𝘵𝘳𝘢𝘪𝘯() method.\n\nOne crucial aspect is configuring an experiment tracker, such as Comet ML, to log all vital metrics & artifacts.\n\n𝟱. 𝗣𝘂𝘀𝗵 𝘁𝗵𝗲 𝗯𝗲𝘀𝘁 𝗺𝗼𝗱𝗲𝗹 𝘁𝗼 𝘁𝗵𝗲 𝗺𝗼𝗱𝗲𝗹 𝗿𝗲𝗴𝗶𝘀𝘁𝗿𝘆\n\nOne of the final steps is to attach a callback to the 𝘚𝘍𝘛𝘛𝘳𝘢𝘪𝘯𝘦𝘳 class that runs when the training ends to push the model with the lowest loss to the model registry as the new production candidate.\n\n𝟲. 𝗘𝘃𝗮𝗹𝘂𝗮𝘁𝗲 𝘁𝗵𝗲 𝗻𝗲𝘄 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻 𝗰𝗮𝗻𝗱𝗶𝗱𝗮𝘁𝗲\n\nEvaluating generative AI models is tricky.\n\nYou can run the LLM on the test set and log the prompts & answers to Comet ML's monitoring system to check them manually.\n\nIf the provided answers are valid, using the model registry dashboard, you will manually release it to replace the old LLM.\n\n𝟳. 𝗗𝗲𝗽𝗹𝗼𝘆 𝘁𝗼 𝗕𝗲𝗮𝗺\n\nIt is as easy as wrapping the training & inference functions (or classes) with a Python \"@𝘢𝘱𝘱.𝘳𝘶𝘯()\" decorator."
  },
  {
    "instruction": "Write a LinkedIn post detailing how to generate a synthetic domain-specific Q&A dataset in under 30 minutes for fine-tuning your open-source LLM.",
    "content": "How do you 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗲 a 𝘀𝘆𝗻𝘁𝗵𝗲𝘁𝗶𝗰 𝗱𝗼𝗺𝗮𝗶𝗻-𝘀𝗽𝗲𝗰𝗶𝗳𝗶𝗰 𝗤&𝗔 𝗱𝗮𝘁𝗮𝘀𝗲𝘁 in <𝟯𝟬 𝗺𝗶𝗻𝘂𝘁𝗲𝘀 to 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗲 your 𝗼𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝗟𝗟𝗠?\n\nThis method is also known as 𝗳𝗶𝗻𝗲𝘁𝘂𝗻𝗶𝗻𝗴 𝘄𝗶𝘁𝗵 𝗱𝗶𝘀𝘁𝗶𝗹𝗹𝗮𝘁𝗶𝗼𝗻. Here are its 3 𝘮𝘢𝘪𝘯 𝘴𝘵𝘦𝘱𝘴 ↓\n\n𝘍𝘰𝘳 𝘦𝘹𝘢𝘮𝘱𝘭𝘦, 𝘭𝘦𝘵'𝘴 𝘨𝘦𝘯𝘦𝘳𝘢𝘵𝘦 𝘢 𝘘&𝘈 𝘧𝘪𝘯𝘦-𝘵𝘶𝘯𝘪𝘯𝘨 𝘥𝘢𝘵𝘢𝘴𝘦𝘵 𝘶𝘴𝘦𝘥 𝘵𝘰 𝘧𝘪𝘯𝘦-𝘵𝘶𝘯𝘦 𝘢 𝘧𝘪𝘯𝘢𝘯𝘤𝘪𝘢𝘭 𝘢𝘥𝘷𝘪𝘴𝘰𝘳 𝘓𝘓𝘔.\n\n𝗦𝘁𝗲𝗽 𝟭: 𝗠𝗮𝗻𝘂𝗮𝗹𝗹𝘆 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗲 𝗮 𝗳𝗲𝘄 𝗶𝗻𝗽𝘂𝘁 𝗲𝘅𝗮𝗺𝗽𝗹𝗲𝘀\n\nGenerate a few input samples (~3) that have the following structure:\n- 𝘶𝘴𝘦𝘳_𝘤𝘰𝘯𝘵𝘦𝘹𝘵: describe the type of investor (e.g., \"I am a 28-year-old marketing professional\")\n- 𝘲𝘶𝘦𝘴𝘵𝘪𝘰𝘯: describe the user's intention (e.g., \"Is Bitcoin a good investment option?\")\n\n𝗦𝘁𝗲𝗽 𝟮: 𝗘𝘅𝗽𝗮𝗻𝗱 𝘁𝗵𝗲 𝗶𝗻𝗽𝘂𝘁 𝗲𝘅𝗮𝗺𝗽𝗹𝗲𝘀 𝘄𝗶𝘁𝗵 𝘁𝗵𝗲 𝗵𝗲𝗹𝗽 𝗼𝗳 𝗮 𝘁𝗲𝗮𝗰𝗵𝗲𝗿 𝗟𝗟𝗠\n\nUse a powerful LLM as a teacher (e.g., GPT4, Falcon 180B, etc.) to generate up to +N similar input examples.\n\nWe generated 100 input examples in our use case, but you can generate more.\n\nYou will use the manually filled input examples to do few-shot prompting.\n\nThis will guide the LLM to give you domain-specific samples.\n\n𝘛𝘩𝘦 𝘱𝘳𝘰𝘮𝘱𝘵 𝘸𝘪𝘭𝘭 𝘭𝘰𝘰𝘬 𝘭𝘪𝘬𝘦 𝘵𝘩𝘪𝘴:\n\"\"\"\n...\nGenerate 100 more examples with the following pattern:\n\n# USER CONTEXT 1\n...\n\n# QUESTION 1\n...\n\n# USER CONTEXT 2\n...\n\"\"\"\n\n𝗦𝘁𝗲𝗽 𝟯: 𝗨𝘀𝗲 𝘁𝗵𝗲 𝘁𝗲𝗮𝗰𝗵𝗲𝗿 𝗟𝗟𝗠 𝘁𝗼 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗲 𝗼𝘂𝘁𝗽𝘂𝘁𝘀 𝗳𝗼𝗿 𝗮𝗹𝗹 𝘁𝗵𝗲 𝗶𝗻𝗽𝘂𝘁 𝗲𝘅𝗮𝗺𝗽𝗹𝗲𝘀\n\nNow, you will have the same powerful LLM as a teacher, but this time, it will answer all your N input examples.\n\nBut first, to introduce more variance, we will use RAG to enrich the input examples with news context.\n\nAfterward, we will use the teacher LLM to answer all N input examples.\n\n...and bam! You generated a domain-specific Q&A dataset with almost 0 manual work.\n\n.\n\nNow, you will use this data to train a smaller LLM (e.g., Falcon 7B) on a niched task, such as financial advising.\n\nThis technique is known as finetuning with distillation because you use a powerful LLM as the teacher (e.g., GPT4, Falcon 180B) to generate the data, which will be used to fine-tune a smaller LLM (e.g., Falcon 7B), which acts as the student.\n\n✒️ 𝘕𝘰𝘵𝘦: To ensure that the generated data is of high quality, you can hire a domain expert to check & refine it.\n\n.\n\n↳ To learn more about this technique, check out this article from a0Pau Labarta Bajo a0's RLML newsletter: 🔗 a0https://lnkd.in/diGud3Yh\n\n#machinelearning a0#mlops a0#datascience\n"
  },
  {
    "instruction": "Create a LinkedIn post about implementing a configuration/settings system in Python for all your applications using pydantic.",
    "content": "This is my favorite way to 𝗶𝗺𝗽𝗹𝗲𝗺𝗲𝗻𝘁 a 𝗰𝗼𝗻𝗳𝗶𝗴𝘂𝗿𝗮𝘁𝗶𝗼𝗻/𝘀𝗲𝘁𝘁𝗶𝗻𝗴𝘀 𝘀𝘆𝘀𝘁𝗲𝗺 in 𝗣𝘆𝘁𝗵𝗼𝗻 for all my apps ↓ \n \nThe core is based on 𝘱𝘺𝘥𝘢𝘯𝘵𝘪𝘤, a data validation library for Python. \n \nMore precisely, on their 𝘉𝘢𝘴𝘦𝘚𝘦𝘵𝘵𝘪𝘯𝘨𝘴 class. \n \n𝗪𝗵𝘆 𝘂𝘀𝗲 𝘁𝗵𝗲 𝗽𝘆𝗱𝗮𝗻𝘁𝗶𝗰 𝗕𝗮𝘀𝗲𝗦𝗲𝘁𝘁𝗶𝗻𝗴𝘀 𝗰𝗹𝗮𝘀𝘀? \n \n- you can quickly load values from .𝘦𝘯𝘷 files (or even 𝘑𝘚𝘖𝘕 or 𝘠𝘈𝘔𝘓) \n- add default values for the configuration of your application \n- the MOST IMPORTANT one → It validates the type of the loaded variables. Thus, you will always be ensured you use the correct variables to configure your system. \n \n𝗛𝗼𝘄 𝗱𝗼 𝘆𝗼𝘂 𝗶𝗺𝗽𝗹𝗲𝗺𝗲𝗻𝘁 𝗶𝘁? \n \nIt is pretty straightforward. \n \nYou subclass the 𝘉𝘢𝘴𝘦𝘚𝘦𝘵𝘵𝘪𝘯𝘨𝘴 class and define all your settings at the class level. \n \nIt is similar to a Python 𝘥𝘢𝘵𝘢𝘤𝘭𝘢𝘴𝘴 but with an extra layer of data validation and factory methods. \n \nIf you assign a value to the variable, it makes it optional. \n \nIf you leave it empty, providing it in your .𝙚𝙣𝙫 file is mandatory. \n \n𝗛𝗼𝘄 𝗱𝗼 𝘆𝗼𝘂 𝗶𝗻𝘁𝗲𝗴𝗿𝗮𝘁𝗲 𝗶𝘁 𝘄𝗶𝘁𝗵 𝘆𝗼𝘂𝗿 𝗠𝗟 𝗰𝗼𝗱𝗲? \n \nYou often have a training configuration file (or inference) into a JSON or YAML file (I prefer YAML files as they are easier to read). \n \nYou shouldn't pollute your 𝘱𝘺𝘥𝘢𝘯𝘵𝘪𝘤 settings class with all the hyperparameters related to the module (as they are a lot, A LOT). \n \nAlso, to isolate the application & ML settings, the easiest way is to add the 𝘵𝘳𝘢𝘪𝘯𝘪𝘯𝘨_𝘤𝘰𝘯𝘧𝘪𝘨_𝘱𝘢𝘵𝘩 in your settings and use a 𝘛𝘳𝘢𝘪𝘯𝘪𝘯𝘨𝘊𝘰𝘯𝘧𝘪𝘨 class to load it independently. \n \nDoing so lets you leverage your favorite way (probably the one you already have in your ML code) of loading a config file for the ML configuration: plain YAML or JSON files, hydra, or other fancier methods. \n \nAnother plus is that you can't hardcode the path anywhere on your system. That is a nightmare when you start using git with multiple people. \n \n. \n \nWhat do you say? Would you start using the 𝘱𝘺𝘥𝘢𝘯𝘵𝘪𝘤 𝘉𝘢𝘴𝘦𝘚𝘦𝘵𝘵𝘪𝘯𝘨𝘴 class in your ML applications? \n \nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience"
  },
  {
    "instruction": "Describe the process of designing and deploying a Mistral LLM to AWS SageMaker.",
    "content": "This is 𝗵𝗼𝘄 you can 𝗱𝗲𝘀𝗶𝗴𝗻 and 𝗱𝗲𝗽𝗹𝗼𝘆 a 𝗠𝗶𝘀𝘁𝗿𝗮𝗹 𝗟𝗟𝗠 to 𝗔𝗪𝗦 𝗦𝗮𝗴𝗲𝗺𝗮𝗸𝗲𝗿 ↓\n\n𝗔𝗹𝘄𝗮𝘆𝘀, the 𝗳𝗶𝗿𝘀𝘁 𝘀𝘁𝗲𝗽 is to 𝘂𝗻𝗱𝗲𝗿𝘀𝘁𝗮𝗻𝗱 the 𝗿𝗲𝗾𝘂𝗶𝗿𝗲𝗺𝗲𝗻𝘁𝘀 of your 𝘀𝘆𝘀𝘁𝗲𝗺. \n\n🤔 Ask yourself:\n\n- What are my 𝘥𝘢𝘵𝘢 𝘱𝘳𝘰𝘤𝘦𝘴𝘴𝘪𝘯𝘨 𝘯𝘦𝘦𝘥𝘴? Consider the diversity of the data formats and complexity.\n\n- How does the 𝘸𝘰𝘳𝘬𝘭𝘰𝘢𝘥 look? Is it real-time or batch processing? What's the concurrency level?\n\n- What about 𝘱𝘳𝘰𝘤𝘦𝘴𝘴𝘪𝘯𝘨 𝘤𝘰𝘯𝘴𝘵𝘳𝘢𝘪𝘯𝘵𝘴? Latency requirements, accuracy versus speed, and, importantly, cost considerations.\n\n- Can it 𝘴𝘤𝘢𝘭𝘦 and 𝘢𝘥𝘢𝘱𝘵? Your strategy must include scalability and flexibility for future expansion.\n\n.\n\nAlex Vesa wrote an article in Decoding ML where he:\n- made an overview of AWS Sagemaker\n- showed how to design the system requirements and deploy a Mistral LLM using AWS Sagemaker\n\n𝘏𝘦𝘳𝘦 𝘪𝘴 𝘸𝘩𝘢𝘵 𝘺𝘰𝘶 𝘸𝘪𝘭𝘭 𝘭𝘦𝘢𝘳𝘯 ↓\n\n𝗦𝗮𝗴𝗲𝗺𝗮𝗸𝗲𝗿 𝗜𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗘𝗻𝗱𝗽𝗼𝗶𝗻𝘁 𝗖𝗼𝗺𝗽𝗼𝗻𝗲𝗻𝘁𝘀\n\n𝗦𝗮𝗴𝗲𝗺𝗮𝗸𝗲𝗿 𝗘𝗻𝗱𝗽𝗼𝗶𝗻𝘁: The scalable and secure API for real-time predictions.\n\n𝗦𝗮𝗴𝗲𝗺𝗮𝗸𝗲𝗿 𝗠𝗼𝗱𝗲𝗹: The artifact from training, ready for predictions.\n\n𝗦𝗮𝗴𝗲𝗺𝗮𝗸𝗲𝗿 𝗖𝗼𝗻𝗳𝗶𝗴𝘂𝗿𝗮𝘁𝗶𝗼𝗻: Specifies the hardware and software setup.\n\n𝗦𝗮𝗴𝗲𝗺𝗮𝗸𝗲𝗿 𝗜𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗖𝗼𝗺𝗽𝗼𝗻𝗲𝗻𝘁: Manages model deployment to endpoints efficiently.\n\n↓↓↓\n\nSageMaker's multi-model endpoints offer a dynamic and efficient way to manage and serve multiple models:\n\n- dynamically manages model lifecycles on multi-model endpoints, loading models on demand\n\n- routes additional requests to other instances if one is overloaded\n\n- manages memory by unloading unused models to free up space for new ones\n\n- models can be added or removed by managing them in the S3 bucket without updating the endpoint\n\nDeploying LLMs via AWS SageMaker is a complex task that demands a deep understanding of system requirements and resource management.\n\n.\n\nIn the 𝗮𝗿𝘁𝗶𝗰𝗹𝗲, you will 𝗹𝗲𝗮𝗿𝗻 how to 𝗱𝗲𝗽𝗹𝗼𝘆 a 𝗠𝗶𝘀𝘁𝗿𝗮𝗹 𝟳𝗕 on 𝗔𝗪𝗦 𝗦𝗮𝗴𝗲𝗺𝗮𝗸𝗲𝗿 using a g5.12xlarge instance.\n\nTo do so, you will 𝗴𝗼 𝘀𝘁𝗲𝗽-𝗯𝘆-𝘀𝘁𝗲𝗽 𝘁𝗵𝗿𝗼𝘂𝗴𝗵 the 𝗰𝗼𝗱𝗲 to 𝗹𝗲𝗮𝗿𝗻 𝗵𝗼𝘄 to:\n\n1. Configure Hardware requirements per replica\n2. Prepare a Deployment Service Class\n3. How to deploy the LLM\n4. Cleaning Sagemaker resources\n\n𝗖𝗵𝗲𝗰𝗸 𝗶𝘁 𝗼𝘂𝘁 𝗼𝗻 𝗗𝗲𝗰𝗼𝗱𝗶𝗻𝗴 𝗠𝗟\n\n↓↓↓\n\n🔗 𝘠𝘰𝘶'𝘳𝘦 𝘯𝘰𝘵 𝘥𝘪𝘨𝘨𝘪𝘯𝘨 𝘥𝘦𝘦𝘱𝘦𝘳 𝘪𝘯𝘵𝘰 𝘤𝘰𝘯𝘤𝘦𝘱𝘵𝘴? 𝘏𝘦𝘳𝘦'𝘴 𝘸𝘩𝘢𝘵 𝘩𝘢𝘱𝘱𝘦𝘯𝘴... : https://lnkd.in/ee8FeMHa\n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience\n"
  },
  {
    "instruction": "Explain the benefits of using the walrus operator in Python for writing cleaner code.",
    "content": "Here is some 𝗣𝘆𝘁𝗵𝗼𝗻 𝘀𝘆𝗻𝘁𝗮𝘅 𝘀𝘂𝗴𝗮𝗿 that will help you 𝘄𝗿𝗶𝘁𝗲 𝗰𝗹𝗲𝗮𝗻𝗲𝗿 𝗰𝗼𝗱𝗲 ↓ \n\nI am talking about the 𝘸𝘢𝘭𝘳𝘶𝘴 𝘰𝘱𝘦𝘳𝘢𝘵𝘰𝘳 denoted by the `:=` symbol. \n\nIt was introduced in Python 3.8, but I rarely see it used. \n\nThus, as a \"clean code\" freak, I wanted to dedicate a post to it. \n\n𝗪𝗵𝗮𝘁 𝗱𝗼𝗲𝘀 𝘁𝗵𝗲 𝘄𝗮𝗹𝗿𝘂𝘀 𝗼𝗽𝗲𝗿𝗮𝘁𝗼𝗿 𝗱𝗼? \n\nIt's an assignment expression that allows you to assign and return a value in the same expression. \n\n𝗪𝗵𝘆 𝘀𝗵𝗼𝘂𝗹𝗱 𝘆𝗼𝘂 𝘂𝘀𝗲 𝗶𝘁? \n\n𝘊𝘰𝘯𝘤𝘪𝘴𝘦𝘯𝘦𝘴𝘴: It reduces the number of lines needed for variable assignment and checking, making code more concise. \n\n𝘙𝘦𝘢𝘥𝘢𝘣𝘪𝘭𝘪𝘵𝘺: It can enhance readability by keeping related logic close, although this depends on the context and the reader's familiarity with exotic Python syntax. \n\n𝙃𝙚𝙧𝙚 𝙖𝙧𝙚 𝙨𝙤𝙢𝙚 𝙚𝙭𝙖𝙢𝙥𝙡𝙚𝙨 \n\n↓↓↓ \n\n1. Using the walrus operator, you can directly assign the result of the 𝘭𝘦𝘯() function inside an if statement. \n\n2. Avoid calling the same function twice in a while loop. The benefit is less code and makes everything more readable. \n\n3. Another use case arises in list comprehensions where a value computed in a filtering condition is also needed in the expression body. Before the 𝘸𝘢𝘭𝘳𝘶𝘴 𝘰𝘱𝘦𝘳𝘢𝘵𝘰𝘳, if you had to apply a function to an item from a list and filter it based on some criteria, you had to refactor it to a standard for loop. \n\n. \n\nWhen writing clean code, the detail matters. \n\nThe details make the difference between a codebase that can be read like a book or one with 10 WTFs / seconds. \n\n. \n\nWhat do you think? Does the walrus operator make the Python code more readable and concise? \n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience \n"
  },
  {
    "instruction": "Detail the three main stages of training an LLM on proprietary data.",
    "content": "You must know these 𝟯 𝗺𝗮𝗶𝗻 𝘀𝘁𝗮𝗴𝗲𝘀 of 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗮𝗻 𝗟𝗟𝗠 to train your own 𝗟𝗟𝗠 on your 𝗽𝗿𝗼𝗽𝗿𝗶𝗲𝘁𝗮𝗿𝘆 𝗱𝗮𝘁𝗮.\n\n# 𝗦𝘁𝗮𝗴𝗲 𝟭: 𝗣𝗿𝗲𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗳𝗼𝗿 𝗰𝗼𝗺𝗽𝗹𝗲𝘁𝗶𝗼𝗻\n\nYou start with a bear foot randomly initialized LLM.\n\nThis stage aims to teach the model to spit out tokens. More concretely, based on previous tokens, the model learns to predict the next token with the highest probability.\n\nFor example, your input to the model is \"The best programming language is ___\", and it will answer, \"The best programming language is Rust.\"\n\nIntuitively, at this stage, the LLM learns to speak.\n\n𝘋𝘢𝘵𝘢: >1 trillion token (~= 15 million books). The data quality doesn't have to be great.\n\n# 𝗦𝘁𝗮𝗴𝗲 𝟮: 𝗦𝘂𝗽𝗲𝗿𝘃𝗶𝘀𝗲𝗱 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴 (𝗦𝗙𝗧) 𝗳𝗼𝗿 𝗱𝗶𝗮𝗹𝗼𝗴𝘂𝗲\n\nYou start with the pretrained model from stage 1.\n\nThis stage teaches the model to respond to the user's questions.\n\nFor example, without this step, when prompted, \"What is the best programming language?\", it has a high probability of creating a series of questions such as: \"What is MLOps? What is MLE? etc.\"\n\nAs the model mimics the training data, you must fine-tune it on Q&A data to align the model to respond to questions instead of predicting the following tokens.\n\nAfter the fine-tuning step, it will respond to \"What is the best programming language?\" with \"Rust\".\n\n𝘋𝘢𝘵𝘢: 10K - 100K Q&A example\n\n𝘕𝘰𝘵𝘦: After aligning the model to respond to questions, you can further single-task fine-tune the model to specialize the LLM on a specific use case.\n\n# 𝗦𝘁𝗮𝗴𝗲 𝟯: 𝗥𝗟𝗛𝗙\n\nDemonstration data tells the model what kind of responses to give but doesn't tell the model how good or bad a response is.\n\nThe goal is to align your model with user feedback (what users liked or didn't like) to increase the probability of generating answers that users find helpful.\n\n𝘙𝘓𝘏𝘍 𝘪𝘴 𝘴𝘱𝘭𝘪𝘵 𝘪𝘯 2:\n\n1. Using the LLM from stage 2, train a reward model to act as a scoring function using (prompt, winning_response, losing_response) samples (= comparison data). \n\nThe model will learn to maximize the difference between these 2. After training, this model outputs rewards for (prompt, response) tuples.\n\n𝘋𝘢𝘵𝘢: 100K - 1M comparisons\n\n2. Use an RL algorithm (e.g., PPO) to fine-tune the LLM from stage 2. Here, you will use the reward model trained above to give a score for every: (prompt, response). \n\nThe RL algorithm will align the LLM to generate prompts with higher rewards, increasing the probability of generating responses that users liked.\n\n𝘋𝘢𝘵𝘢: 10K - 100K prompts"
  },
  {
    "instruction": "Highlight the four pillars necessary to build production ML systems.",
    "content": "The 𝟰 𝗽𝗶𝗹𝗹𝗮𝗿𝘀 to 𝗯𝘂𝗶𝗹𝗱 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻 𝗠𝗟 𝘀𝘆𝘀𝘁𝗲𝗺𝘀 ↓\n\nAs highlighted by Alex Vesa, before building a production-ready system, it is critical to go through a set of questions that will later decide the nature of your ML system architecture.\n\n𝘏𝘦𝘳𝘦 𝘢𝘳𝘦 𝘵𝘩𝘦 4 𝘱𝘪𝘭𝘭𝘢𝘳𝘴 𝘵𝘩𝘢𝘵 𝘺𝘰𝘶 𝘢𝘭𝘸𝘢𝘺𝘴 𝘩𝘢𝘷𝘦 𝘵𝘰 𝘤𝘰𝘯𝘴𝘪𝘥𝘦𝘳 𝘣𝘦𝘧𝘰𝘳𝘦 𝘥𝘦𝘴𝘪𝘨𝘯𝘪𝘯𝘨 𝘢𝘯𝘺 𝘴𝘺𝘴𝘵𝘦𝘮 ↓\n\n➔ 𝗗𝗮𝘁𝗮\n\n- What data types do you have? (e.g., tabular data, images, text, etc.)\n- What does the data look like? (e.g., for text data, is it in a single language or multiple?)\n- How do you collect the data?\n- At what frequency do you have to collect the data?\n- How do you collect labels for the data? (crucial for how you plan to evaluate and monitor the model in production)\n\n➔ 𝗧𝗵𝗿𝗼𝘂𝗴𝗵𝗽𝘂𝘁\n\n- What are the throughput requirements? You must know at least the throughput's minimum, average, and maximum statistics.\n- How many requests the system must handle simultaneously? (1, 10, 1k, 1 million, etc.)\n\n➔ 𝗟𝗮𝘁𝗲𝗻𝗰𝘆\n\n- What are the latency requirements? (1 millisecond, 10 milliseconds, 1 second, etc.)\n- Throughput vs. latency trade-off\n- Accuracy vs. speed trade-off\n\n➔ 𝗜𝗻𝗳𝗿𝗮𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲\n\n- Batch vs. real-time architecture (closely related to the throughput vs. latency trade-off)\n- How should the system scale? (e.g., based on CPU workload, # of requests, queue size, data size, etc.)\n- Cost requirements\n\n.\n\nDo you see how we shifted the focus from model performance towards how it is integrated into a more extensive system?\n\nWhen building production-ready ML, the model's accuracy is no longer the holy grail but a bullet point in a grander scheme.\n\n. \n\n𝗧𝗼 𝘀𝘂𝗺𝗺𝗮𝗿𝗶𝘇𝗲, the 4 pillars to keep in mind before designing an ML architecture are:\n- Data\n- Throughput\n- Latency\n- Infrastructure\n\nLet me know in the comments below if you want to dive deeper into each topic. ↴\n\nhashtag\n#machinelearning \nhashtag\n#mlops \nhashtag\n#datascience\n\n.\n\n💡 Follow me for daily lessons about ML engineering and MLOps.\nVizualizaţi traducerea\n"
  },
  {
    "instruction": "Discuss the essentials of fine-tuning an open-source LLM to create a financial advisor.",
    "content": "𝗪𝗵𝗮𝘁 do you need to 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗲 an open-source 𝗟𝗟𝗠 to create your own 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝗱𝘃𝗶𝘀𝗼𝗿?\n\nThis is the 𝗟𝗟𝗠 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴 𝗸𝗶𝘁 you must know ↓\n\n𝗗𝗮𝘁𝗮𝘀𝗲𝘁\n\nThe key component of any successful ML project is the data.\n\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\n\nThe best approach is to hire experts to create it manually.\n\nBut, for a PoC, that might get expensive & slow.\n\nThe good news is that a method called \"𝘍𝘪𝘯𝘦𝘵𝘶𝘯𝘪𝘯𝘨 𝘸𝘪𝘵𝘩 𝘥𝘪𝘴𝘵𝘪𝘭𝘭𝘢𝘵𝘪𝘰𝘯\" exists.\n\nIn a nutshell, this is how it works: \"Use a big & powerful LLM (e.g., GPT4) to generate your fine-tuning data. After, use this data to fine-tune a smaller model (e.g., Falcon 7B).\"\n\nFor specializing smaller LLMs on specific use cases (e.g., financial advisors), this is an excellent method to kick off your project.\n\n\n𝗣𝗿𝗲-𝘁𝗿𝗮𝗶𝗻𝗲𝗱 𝗼𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝗟𝗟𝗠\n\nYou never want to start training your LLM from scratch (or rarely).\n\nWhy? Because you need trillions of tokens & millions of $$$ in compute power.\n\nYou want to fine-tune your LLM on your specific task.\n\nThe good news is that you can find a plethora of open-source LLMs on HuggingFace (e.g., Falcon, LLaMa, etc.)\n\n\n𝗣𝗮𝗿𝗮𝗺𝗲𝘁𝗲𝗿 𝗲𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴\n\nAs LLMs are big... duh...\n\n... they don't fit on a single GPU.\n\nAs you want only to fine-tune the LLM, the community invented clever techniques that quantize the LLM (to fit on a single GPU) and fine-tune only a set of smaller adapters.\n\nOne popular approach is QLoRA, which can be implemented using HF's `𝘱𝘦𝘧𝘵` Python package.\n\n\n𝗠𝗟𝗢𝗽𝘀\n\nAs you want your project to get to production, you have to integrate the following MLOps components:\n\n- experiment tracker to monitor & compare your experiments\n- model registry to version & share your models between the FTI pipelines\n- prompts monitoring to debug & track complex chains\n\n↳ Available on ML platforms, such as Comet ML.\n\n\n𝗖𝗼𝗺𝗽𝘂𝘁𝗲 𝗽𝗹𝗮𝘁𝗳𝗼𝗿𝗺\n\nThe most common approach is to train your LLM on your on-prem Nivida GPUs cluster or rent them on cloud providers such as AWS, Paperspace, etc.\n\nBut what if I told you that there is an easier way?\n\nThere is! It is called serverless.\n\nFor example, Beam is a GPU serverless provider that makes deploying your training pipeline as easy as decorating your Python function with `@𝘢𝘱𝘱.𝘳𝘶𝘯()`.\n\nAlong with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\n\n.\n\n↳ To see everything in action, check out my FREE 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 𝗰𝗼𝘂𝗿𝘀𝗲 & give it a ⭐: 🔗 https://lnkd.in/dZgqtf8f"
  },
  {
    "instruction": "Write a brief LinkedIn post introducing resources for starting with MLOps, highlighting a website and a GitHub repository as starting points.",
    "content": "Have you ever wondered where to start learning MLOps?\n.\nYou can find lots of valuable information by checking out the MLOps organization site:\nhttps://ml-ops.org/\nAlso, you can find hundreds of essential references by following this GitHub repository:\nhttps://lnkd.in/dxknAmTK\nNow you have no excuse to start learning MLOps!\nhashtag\n#\nmlops\nhashtag\n#\ncontent\nhashtag\n#\nlearning\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Create a light-hearted LinkedIn post with an amusing image, intended to engage the audience humorously.",
    "content": "Who fell for it raise your hand 😂😂😂"
  },
  {
    "instruction": "Write a humorous LinkedIn post sharing an amusing meme related to AWS, crediting the creator.",
    "content": "This is so hilarious 🤭😂😂😂\nCredit\nCorey Quinn\nhashtag\n#\nAWS\nhashtag\n#\nawscloud\nhashtag\n#\nawscloudpractitioner\nhashtag\n#\nmachinelearning\nhashtag\n#\nnew\nhashtag\n#\nbrad"
  },
  {
    "instruction": "Craft a LinkedIn post discussing the importance of feature stores in ML production systems, highlighting three key aspects.",
    "content": "These are 3 characteristics showing that a feature store is a crucial component within any end-to-end ML production system.\n.\n#𝟏 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐦𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭\nFeature management is the ability to discover and share features across teams. This characteristic may save time and computation as multiple features can be shared across models. Also, it helps share knowledge across teams or projects.\n#𝟐 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐜𝐨𝐦𝐩𝐮𝐭𝐚𝐭𝐢𝐨𝐧\nFeature computation means you compute your features only once and store them for future uses. In this case, it acts like a data warehouse, and it may save a lot of time and computation, especially when computing specific features consume a lot of resources (usually the case).\n#𝟑 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐜𝐨𝐧𝐬𝐢𝐬𝐭𝐞𝐧𝐜𝐲\nFeature consistency refers to the problem of having two separate pipelines for the model between training and production (known as the training-serving skew issue). Using a feature store, you unify the logic of feature creation, using the feature store both in training and production.\n.\nI am curious to know what tools you use for your feature store solutions. Please Let me know if the comments below 👇\nhashtag\n#\nfeaturestore\nhashtag\n#\ndatawarehouse\nhashtag\n#\nmlops\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Write a brief LinkedIn post introducing resources for starting with MLOps, highlighting a website and a GitHub repository as starting points.",
    "content": "Have you ever wondered where to start learning MLOps?\n.\nYou can find lots of valuable information by checking out the MLOps organization site:\nhttps://ml-ops.org/\nAlso, you can find hundreds of essential references by following this GitHub repository:\nhttps://lnkd.in/dxknAmTK\nNow you have no excuse to start learning MLOps!\nhashtag\n#\nmlops\nhashtag\n#\ncontent\nhashtag\n#\nlearning\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Create a light-hearted LinkedIn post with an amusing image, intended to engage the audience humorously.",
    "content": "Who fell for it raise your hand 😂😂😂"
  },
  {
    "instruction": "Write a humorous LinkedIn post sharing an amusing meme related to AWS, crediting the creator.",
    "content": "This is so hilarious 🤭😂😂😂\nCredit\nCorey Quinn\nhashtag\n#\nAWS\nhashtag\n#\nawscloud\nhashtag\n#\nawscloudpractitioner\nhashtag\n#\nmachinelearning\nhashtag\n#\nnew\nhashtag\n#\nbrad"
  },
  {
    "instruction": "Craft a post to inform your network about a recent publication or article related to your field. Provide a brief summary of the article's key points and your personal insights or opinions.",
    "content": "Electromyography has the potential to revolutionize how we interact with computers. Imagine typing without a keyboard, controlling visual interfaces without a mouse, and navigating AR/VR environments much more intuitively. The signals in our wrists are so clear that EMG can detect finger motion of just a millimeter. This video highlights some of the groundbreaking research by the incredible CTRL-Labs team who joined us last year. 🤩 Mind-blowing potential. #innovation #technology #facebook"
  },
  {
    "instruction": "Express gratitude in a post to thank a mentor or colleague for their support and guidance in your professional journey. Share a specific example or memory of how their mentorship has positively impacted your career.",
    "content": "I'm incredibly grateful to have [mentor's or colleague's name] as a mentor. Their guidance and support have been invaluable in my professional growth. I still remember the time when [specific example or memory]. Thank you for everything! #gratitude #mentorship #careerdevelopment"
  },
  {
    "instruction": "Craft a LinkedIn post discussing the importance of feature stores in ML production systems, highlighting three key aspects.",
    "content": "These are 3 characteristics showing that a feature store is a crucial component within any end-to-end ML production system.\n.\n#𝟏 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐦𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭\nFeature management is the ability to discover and share features across teams. This characteristic may save time and computation as multiple features can be shared across models. Also, it helps share knowledge across teams or projects.\n#𝟐 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐜𝐨𝐦𝐩𝐮𝐭𝐚𝐭𝐢𝐨𝐧\nFeature computation means you compute your features only once and store them for future uses. In this case, it acts like a data warehouse, and it may save a lot of time and computation, especially when computing specific features consume a lot of resources (usually the case).\n#𝟑 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐜𝐨𝐧𝐬𝐢𝐬𝐭𝐞𝐧𝐜𝐲\nFeature consistency refers to the problem of having two separate pipelines for the model between training and production (known as the training-serving skew issue). Using a feature store, you unify the logic of feature creation, using the feature store both in training and production.\n.\nI am curious to know what tools you use for your feature store solutions. Please Let me know if the comments below 👇\nhashtag\n#\nfeaturestore\nhashtag\n#\ndatawarehouse\nhashtag\n#\nmlops\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Create a promotional post to introduce your network to a new product or service. Describe the key features, benefits, and how it can address a specific need or problem your audience may have. Include a call-to-action encouraging engagement or further exploration.",
    "content": "Come to see how you can be a part of a place of innovation, with projects like: ➡️ lek3.co, charging electric cars anywhere. Quick and easy. ➡️ 2park.io, smart parking at your fingertips and ➡️ SafeFleet, innovating the telematics industry. We're waiting for you! :) ➡️ ➡️ ➡️ https://lnkd.in/g-sEMiW #iot #design #future #hr #team #smartcities #projects #electriccars #telematics #parks #futurism #charging #software #projecting #safefleet #recruiting #lek3 #lek3co"
  },
  {
    "instruction": "Write a LinkedIn post about the distinction between workflow management tools and orchestration tools in MLOps, explaining their uses and differences.",
    "content": "𝐖𝐨𝐫𝐤𝐟𝐥𝐨𝐰 𝐦𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭/𝐬𝐜𝐡𝐞𝐝𝐮𝐥𝐞𝐫𝐬 and 𝐨𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐢𝐨𝐧 𝐭𝐨𝐨𝐥𝐬 are two critical elements of any successful MLOps infrastructure. But what, are they used for, and what are some vital differences between them?\n.\n#𝟏 𝐖𝐨𝐫𝐤𝐟𝐥𝐨𝐰 𝐦𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭 𝐭𝐨𝐨𝐥𝐬 (𝐨𝐫 𝐬𝐜𝐡𝐞𝐝𝐮𝐥𝐞𝐫𝐬)\nWorkflow management tools are concerned with when to run jobs and what resources are needed to run those jobs.\nSchedulers are based on job-type abstractions: DAGs or priority queues. The dependencies between multiple jobs are defined within a YAML or Python file. For complex applications, this is essential because, for example, if job A fails, you don't want to execute job B. Another example would be that if job A fails, retry to run it 5 times until you give up.\nSchedulers are used for periodic jobs.\nPopular tools: Airflow, Argo, Prefect, Kubeflow, Metaflow\n#𝟐 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐢𝐨𝐧 𝐭𝐨𝐨𝐥𝐬\nOrchestration tools concern where to get resources to run the jobs and how to scale the resources to run the jobs efficiently.\nOrchestration tools deal with lower-level attractions: machines, instances, or clusters. They manage the hardware resources on top of which you run your system.\nOrchestration tools are responsible for scaling the computational power size to efficiently support the given jobs (e.g., if the number of jobs is more significant than the number of instances, it will spin up more instances).\nOrchestration tools are used for long-running servers that serve clients' requests.\nPopular tools: Kubernetes (K8s)\n.\nThese tools are usually used interchangeably. Schedulers typically run on top of orchestration tools.\nhashtag\n#\ntools\nhashtag\n#\nschedulers\nhashtag\n#\norchestration\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Craft a LinkedIn post emphasizing the necessity of a feature store in ML systems, sharing a succinct, insightful thought.",
    "content": "You don't need a feature store...\nSaid no one after he understood why using one is so essential for any ML system.\nHere is why 👇\nhashtag\n#\nmlops\nhashtag\n#\nml\nhashtag\n#\nfeaturestore"
  },
  {
    "instruction": "Publish a LinkedIn post listing top Python tools for code quality, including links and a note on automating their use with pre-commit hooks.",
    "content": "Top 3 tools to keep your Python code professional without any additional effort 👇\n.\n#𝟏. 𝐁𝐥𝐚𝐜𝐤\nA tool for automatically formatting your code.\nCheck it out:\nhttps://lnkd.in/dR_RdjT9\n#𝟐. 𝐅𝐥𝐚𝐤𝐞𝟖\nA tool that checks and validates your coding style.\nCheck it out:\nhttps://lnkd.in/dBmyuECV\n#𝟑. 𝐑𝐞𝐟𝐮𝐫𝐛\nA tool for automatically refurbishing and modernizing your Python codebases.\nCheck it out:\nhttps://lnkd.in/d67J9hbt\n.\nAn important note is that you can automate the trigger of these tools using pre-commit. Thus, these tools will be triggered every time you run a commit.\nhashtag\n#\npython\nhashtag\n#\ncleancode\nhashtag\n#\ncodingstyle\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Write a LinkedIn post presenting a list of essential ML monitoring metrics, aiming to educate followers on effective monitoring practices.",
    "content": "I compiled a list of the top 10 metrics your ML monitoring system should track.\n.\n#𝟏 𝐎𝐩𝐞𝐫𝐚𝐭𝐢𝐨𝐧𝐚𝐥 𝐦𝐞𝐭𝐫𝐢𝐜𝐬\nUsed to monitor the health of your overall software system.\n➝ latency\n➝ throughput\n➝ CPU/GPU utilization\n➝ memory utilization\n➝ the number of requests your model receives in the last X minutes/hours/days\n➝ the number of successful requests\n#𝟐 𝐌𝐋-𝐬𝐩𝐞𝐜𝐢𝐟𝐢𝐜 𝐦𝐞𝐭𝐫𝐢𝐜𝐬\nUsed to monitor the performance of your ML system.\n➝ accuracy (extremely useful when you have feedback or natural labels from the user: click rate, upvote, downvote, purchases, bookmarks, views, etc.)\n➝ predictions (as predictions are low dimensional various statistics are easy to compute + the distribution of the prediction represents a proxy for the input distribution)\n➝ features (feature validation + two-sample tests for drift detection)\n➝ raw inputs (these are harder to monitor due to their scattered nature within the infrastructure)\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Compose a LinkedIn post analyzing the most demanding domains in AI/ML, using market size and growth as key metrics.",
    "content": "I compiled a table with the top 8 most demanding domains in AI/ML. I want to understand in which field to specialize further as a freelancer. Thus I wanted to take a data-centric approach in making this decision. I used the market size as the 'demand' metric because where the money flows represents a strong signal of demand in that field.\nNote that the absolute values in $$$ might not be 100% accurate compared as they might be extracted from different years, such as 2020-2021-2022. The most interesting column is 𝐂𝐀𝐆𝐑 % which shows the potential growth in that field. If there is growth, there is new demand for engineers in that field.\nThe market size within the table consists only of the AI/ML part of that domain (i.e., the entire energy domain is way bigger than 3.82$ Billion). Also, some domains might cross each other, so the Global Machine Learning values won't add to the rest of the fields.\n.\nWhat do you think? Do you believe that this table is relevant? Did I miss any domains? What field in AI/ML do you think has the most potential?\nI am trying to figure out the answers to these questions myself, so feel free to start an open discussion in the comments below 👀.\nhashtag\n#\ncarrer\nhashtag\n#\nai\nhashtag\n#\ndemand\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Create a LinkedIn post about evaluating the necessity of additional data for improving a model's performance using learning curves.",
    "content": "Everybody says you can improve your model by training it on more data. Is it true? How can we test this theory before spending money and effort collecting that new data?\n.\nGood news. The solution is quite simple.\nUsing the 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠 𝐂𝐮𝐫𝐯𝐞 graph, you can quickly see the performance evolution of the model with various amounts of data.\nIt is important to validate this because more data is not the answer to all your problems. 😂\n.\nThe main idea of the 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠 𝐂𝐮𝐫𝐯𝐞 method is simple:\n1. You create N subsets from your training dataset. Where the first subset is the smallest. You keep increasing the subset size at a given rate until you reach the full dataset size: 1 < 2 < 3 < ... < N.\n2. You train your model on subsample 1.\n3. You test your, model on the test split (which is not divided; it is not good practice to touch your test split).\n4. Save the results.\n5. Repeat steps 2-4 on all your subsets.\n6. Plot the results.\n.\nUsing this mechanism, you can simulate if adding more data will boost your model's performance.\nAlso, you can quickly compare different models and evaluate which one needs more/fewer data to achieve your desired accuracy.\n.\nCheck out how easy it is to do this with Sklearn:\nhttps://lnkd.in/d358Dhhd\n.\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Draft a LinkedIn post about framing machine learning problems effectively, using a case study from e-commerce for personalized discounts.",
    "content": "How you frame a\nhashtag\n#\nmachinelearning\nproblem is essential for your future self.\nIt can make your life easy or a living hell.\n💡 𝐄𝐱𝐚𝐦𝐩𝐥𝐞\nA user just clicked on our e-commerce site that sells tech. We want to welcome them and give them a personalized discount on a hand-picked item.\nHow do we solve this problem using\nhashtag\n#\nML\n?\nAt first sight, we might start building a multiclass classifier that takes input features about the user and predicts the probability that the user might like a particular item.\nBut this decision will make your future life a living hell if we consider it.\n.\n𝐖𝐡𝐲?\nFor every new item in the store, you need to retrain the model to adapt its output to the new inventory.\n.\n𝐒𝐨𝐥𝐮𝐭𝐢𝐨𝐧\nBuild a binary classifier that takes as input the user's features and information about a specific item. The model's output will reflect only the probability that the user might like the item you used as input.\nIn this scenario, we will have no issue when a new item is added to the store.\n.\n𝐖𝐡𝐲?\nBecause the output is not dependent on the number of items within the store.\nWe will predict the probability for every item and take the most significant out of them.\nIn conclusion, it might be worth taking a deep breath and thinking about the problem before starting to code your solution.\n.\n💡 My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n."
  },
  {
    "instruction": "Write a LinkedIn post recommending an ML course for building end-to-end applications, highlighting the instructor's expertise.",
    "content": "I just started watching an excellent  ML course on building real-world, end-to-end ML applications.\nIf you ever want to level up your ML knowledge and make it production-ready, I recommend this course by\nPau Labarta Bajo\n.\nHe is a DS and ML professional with 8+ years of experience building production-ready ML systems. Thus, you can get real-world guidance on how to build end-to-end ML applications.\nWithin the course, you will get clear explanations of how to build and design an end-to-end ML application.\nI think it's worth it to check it out.\nP.S. Also, you can follow him on LinkedIn and Twitter, where he often posts content about ML in production. I learned a lot from him.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nrealworld\nhashtag\n#\ntutorial\nhashtag\n#\ncourse"
  },
  {
    "instruction": "Create a LinkedIn post celebrating the anniversary of a book on deep learning, offering copies to the audience.",
    "content": "We'll send five of these to lucky people anywhere in the world 🌎\nWe want to celebrate with you - our book just turned 1 - they grow up so fast.\n👉 Simply comment below by adding (@) friends who should get the book. We randomly select your friends and contact them for their address to ship the books.\n📖 About the book:\n\"Why does my neural network not learn?\" by @Frank Hafner and I, will kickstart your friends' deep learning journey!\nWith best practices and extensive experience from applying deep learning in research and development, they will get their own neural networks to train.\n🚀 Check it out in more detail, and get your own copy here (make sure to search on your amazon marketplace, we are basically available everywhere) >>\nhttps://lnkd.in/exfkjY9t\n☕ And as always, feel free to follow here on LinkedIn.\nhashtag\n#\ndatascience\nhashtag\n#\npython\nhashtag\n#\ndeeplearning\nhashtag\n#\nGiveaway\nhashtag\n#\nbooks\nhashtag\n#\nkdp\nhashtag\n#\nmachinelearning"
  },
  {
    "instruction": "Draft a LinkedIn post on the importance of data distribution shifts awareness in real-world ML applications.",
    "content": "These are 3 data distribution shifts you have to know when building real-world ML applications 👇\nhashtag\n#\nml\nhashtag\n#\ndata\nhashtag\n#\ndistributionshift\nhashtag\n#\nmonitoring"
  },
  {
    "instruction": "Promote an upcoming Q&A session with Paul Iusztin, highlighting the opportunity for the audience to engage and ask anything about MLOps.",
    "content": "Next Tuesday we'll have the 2nd edition of 𝗔𝘀𝗸 𝗠𝗮𝗿𝘃𝗲𝗹𝗼𝘂𝘀 𝗠𝗟𝗢𝗽𝘀 𝗔𝗻𝘆𝘁𝗵𝗶𝗻𝗴: A live Q&A sessions where we try to answer all your questions 🤗 This time Paul Iusztin will join us, sign up here: https://lnkd.in/eVk7e8k5"
  },
  {
    "instruction": "Announce a new course on building hands-on LLM systems with a focus on good LLMOps principles, highlighting the content and inviting the audience to enroll for free.",
    "content": "If you want to 𝐥𝐞𝐚𝐫𝐧 for FREE to 𝐛𝐮𝐢𝐥𝐝 𝐡𝐚𝐧𝐝𝐬-𝐨𝐧 𝐋𝐋𝐌 𝐬𝐲𝐬𝐭𝐞𝐦𝐬 using good LLMOps principles, we want to announce that we just 𝐟𝐢𝐧𝐢𝐬𝐡𝐞𝐝 the code & video lessons for the \"𝐇𝐚𝐧𝐝𝐬-𝐨𝐧 𝐋𝐋𝐌𝐬\" 𝐜𝐨𝐮𝐫𝐬𝐞.\nBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.\nWe will primarily focus on the engineering & MLOps aspects.\nThus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks.\n𝐌𝐨𝐫𝐞 𝐩𝐫𝐞𝐜𝐢𝐬𝐞𝐥𝐲, 𝐭𝐡𝐞𝐬𝐞 𝐚𝐫𝐞 𝐭𝐡𝐞 3 𝐜𝐨𝐦𝐩𝐨𝐧𝐞𝐧𝐭𝐬 𝐲𝐨𝐮 𝐰𝐢𝐥𝐥 𝐥𝐞𝐚𝐫𝐧 𝐭𝐨 𝐛𝐮𝐢𝐥𝐝:\n1.  a 𝐫𝐞𝐚𝐥-𝐭𝐢𝐦𝐞 𝐬𝐭𝐫𝐞𝐚𝐦𝐢𝐧𝐠 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DB\n2.  a 𝐟𝐢𝐧𝐞-𝐭𝐮𝐧𝐢𝐧𝐠 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\n3.  an 𝐢𝐧𝐟𝐞𝐫𝐞𝐧𝐜𝐞 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)\nWe will also show you how to integrate various serverless tools, such as:\n• Comet ML as your ML Platform;\n• Qdrant as your vector DB;\n• Beam as your infrastructure.\n𝐖𝐡𝐨 𝐢𝐬 𝐭𝐡𝐢𝐬 𝐟𝐨𝐫?\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM systems using LLMOps good principles.\n𝐇𝐨𝐰 𝐰𝐢𝐥𝐥 𝐲𝐨𝐮 𝐥𝐞𝐚𝐫𝐧?\nThe series contains 4 hands-on video lessons and the open-source code you can access on GitHub.\n𝐂𝐮𝐫𝐢𝐨𝐮𝐬?\n↳ Check it out and support us with a ⭐:  🔗\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Explain the benefits and ease of using serverless technology, particularly Beam, for deploying and managing ML models on GPUs.",
    "content": "𝗗𝗲𝗽𝗹𝗼𝘆𝗶𝗻𝗴 & 𝗺𝗮𝗻𝗮𝗴𝗶𝗻𝗴 ML models is 𝗵𝗮𝗿𝗱, especially when running your models on GPUs.\nBut 𝘀𝗲𝗿𝘃𝗲𝗿𝗹𝗲𝘀𝘀 makes things 𝗲𝗮𝘀𝘆.\nUsing\nBeam\nas your serverless provider, deploying & managing ML models can be as easy as ↓\n𝗗𝗲𝗳𝗶𝗻𝗲 𝘆𝗼𝘂𝗿 𝗶𝗻𝗳𝗿𝗮𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲 & 𝗱𝗲𝗽𝗲𝗻𝗱𝗲𝗻𝗰𝗶𝗲𝘀\nIn a few lines of code, you define the application that contains:\n- the requirements of your infrastructure, such as the CPU, RAM, and GPU\n- the dependencies of your application\n- the volumes from where you can load your data and store your artifacts\n𝗗𝗲𝗽𝗹𝗼𝘆 𝘆𝗼𝘂𝗿 𝗷𝗼𝗯𝘀\nUsing the Beam application, you can quickly decore your Python functions to:\n- run them once on the given serverless application\n- put your task/job in a queue to be processed or even schedule it using a CRON-based syntax\n- even deploy it as a RESTful API endpoint\n.\nAs you can see in the image below, you can have one central function for training or inference, and with minimal effort, you can switch from all these deployment methods.\nAlso, you don't have to bother at all with managing the infrastructure on which your jobs run. You specify what you need, and Beam takes care of the rest.\nBy doing so, you can directly start to focus on your application and stop carrying about the infrastructure.\nThis is the power of serverless!\n↳ Check out\nBeam\nto learn more: 🔗\nhttps://lnkd.in/d4-pkCxc\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Share a method for automating the generation of documentation for Python functions using Copilot, highlighting its efficiency and time-saving benefits.",
    "content": "This is how I 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗲𝗱 𝗣𝘆𝗗𝗼𝗰𝘀 for 𝟭𝟬𝟬 𝗣𝘆𝘁𝗵𝗼𝗻 𝗳𝘂𝗻𝗰𝘁𝗶𝗼𝗻𝘀 in <𝟭 𝗵𝗼𝘂𝗿 ↓\nThe most boring programming part is to write PyDocs, so I usually write clean code and let it speak for itself.\nBut, for open-source projects where you have to generate robust documentation, PyDocs are a must.\nThe good news is that now you can automate this process using Copilot.\nYou can see in the video below an example of how easy it is.\nI tested it on more complex functions/classes, and it works well. I chose this example because it fits nicely on one screen.\nOnce I tested Copilot's experience, I will never go back.\nIt is true that, in some cases, you have to make some minor adjustments. But that is still 10000% more efficient than writing it from scratch.\nIf you want more examples, check out our Hands-on LLMs course, where all the PyDocs are generated 99% using Copilot in <1 hour.\n↳ Check it out: 🔗\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Detail the necessary components and steps for fine-tuning an open-source LLM to create a financial advisor, emphasizing data, pre-trained models, fine-tuning techniques, and MLOps integration.",
    "content": "𝗪𝗵𝗮𝘁 do you need to 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗲 an open-source 𝗟𝗟𝗠 to create your own 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝗱𝘃𝗶𝘀𝗼𝗿?\nThis is the 𝗟𝗟𝗠 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴 𝗸𝗶𝘁 you must know ↓\n𝗗𝗮𝘁𝗮𝘀𝗲𝘁\nThe key component of any successful ML project is the data.\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\nThe best approach is to hire a bunch of experts to create it manually.\nBut, for a PoC, that might get expensive & slow.\nThe good news is that a method called \"𝘍𝘪𝘯𝘦𝘵𝘶𝘯𝘪𝘯𝘨 𝘸𝘪𝘵𝘩 𝘥𝘪𝘴𝘵𝘪𝘭𝘭𝘢𝘵𝘪𝘰𝘯\" exists.\nIn a nutshell, this is how it works: \"Use a big & powerful LLM (e.g., GPT4) to generate your fine-tuning data. After, use this data to fine-tune a smaller model (e.g., Falcon 7B).\"\nFor specializing smaller LLMs on specific use cases (e.g., financial advisors), this is an excellent method to kick off your project.\n𝗣𝗿𝗲-𝘁𝗿𝗮𝗶𝗻𝗲𝗱 𝗼𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝗟𝗟𝗠\nYou never want to start training your LLM from scratch (or rarely).\nWhy? Because you need trillions of tokens & millions of $$$ in compute power.\nYou want to fine-tune your LLM on your specific task.\nThe good news is that you can find a plethora of open-source LLMs on HuggingFace (e.g., Falcon, LLaMa, etc.)\n𝗣𝗮𝗿𝗮𝗺𝗲𝘁𝗲𝗿 𝗲𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴\nAs LLMs are big... duh...\n... they don't fit on a single GPU.\nAs you want only to fine-tune the LLM, the community invented clever techniques that quantize the LLM (to fit on a single GPU) and fine-tune only a set of smaller adapters.\nOne popular approach is QLoRA, which can be implemented using HF's `𝘱𝘦𝘧𝘵` Python package.\n𝗠𝗟𝗢𝗽𝘀\nAs you want your project to get to production, you have to integrate the following MLOps components:\n- experiment tracker to monitor & compare your experiments\n- model registry to version & share your models between the FTI pipelines\n- prompts monitoring to debug & track complex chains\n↳ All of them are available on ML platforms, such as Comet ML 🔗\nhttps://lnkd.in/d7jNQz7m\n𝗖𝗼𝗺𝗽𝘂𝘁𝗲 𝗽𝗹𝗮𝘁𝗳𝗼𝗿𝗺\nThe most common approach is to train your LLM on your on-prem Nivida GPUs cluster or rent them on cloud providers such as AWS, Paperspace, etc.\nBut what if I told you that there is an easier way?\nThere is! It is called serverless.\nFor example,\nBeam\nis a GPU serverless provider that makes deploying your training pipeline as easy as decorating your Python function with `@𝘢𝘱𝘱.𝘳𝘶𝘯()`.\nAlong with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\n↳ Beam: 🔗\nhttps://lnkd.in/dedCaMDh\n.\n↳ To see all these components in action, check out my FREE 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 𝗰𝗼𝘂𝗿𝘀𝗲 & give it a ⭐:  🔗\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"
  },
  {
    "instruction": "Celebrate the milestone of surpassing 400 GitHub stars for the Hands-on LLMs course, expressing gratitude towards collaborators and outlining the course's practical approach to leveraging LLMs.",
    "content": "𝗧𝗵𝗲 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 FREE 𝗰𝗼𝘂𝗿𝘀𝗲 just 𝗽𝗮𝘀𝘀𝗲𝗱 𝟰𝟬𝟬+ 𝗚𝗶𝘁𝗛𝘂𝗯 ⭐🌟⭐\nA big 𝗧𝗵𝗮𝗻𝗸 𝘆𝗼𝘂!, for everyone who supported the GitHub repo. This means a lot to me.\nAlso, I want to thank Pau Labarta and Alexandru Razvant for this fantastic collaboration and for making this course possible.\n.\n𝘍𝘰𝘳 𝘵𝘩𝘦 𝘱𝘦𝘰𝘱𝘭𝘦 𝘸𝘩𝘰 𝘥𝘰𝘯'𝘵 𝘬𝘯𝘰𝘸, 𝘩𝘦𝘳𝘦 𝘪𝘴 𝘴𝘰𝘮𝘦 𝘤𝘰𝘯𝘵𝘦𝘹𝘵 𝘢𝘣𝘰𝘶𝘵 𝘵𝘩𝘦 𝘤𝘰𝘶𝘳𝘴𝘦 ↓\n𝗧𝗵𝗲 𝗛𝗮𝗻𝗱𝘀-𝗢𝗻 𝗟𝗟𝗠𝘀 course is not just another demo of how to make a few predictions in a notebook.\nYou'll walk away with a 𝗳𝘂𝗹𝗹𝘆 𝗼𝗽𝗲𝗿𝗮𝘁𝗶𝗼𝗻𝗮𝗹 𝗽𝗿𝗼𝗱𝘂𝗰𝘁, leveraging Large Language Models (LLMs) to build a chatbot for financial investment advice.\n=== 𝗪𝗵𝗮𝘁 𝗬𝗼𝘂'𝗹𝗹 𝗕𝘂𝗶𝗹𝗱 ===\nWithin the course, you will leverage the 𝟯-𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗮𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲, as follows:\n𝟭. 𝗙𝗲𝗮𝘁𝘂𝗿𝗲 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll create a system to ingest real-time financial news—crucial for up-to-date advice.\n𝟮. 𝗧𝗿𝗮𝗱𝗶𝗻𝗴 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll fine-tune an LLM to specialize the model in making financial decisions.\n𝟯. 𝗜𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll combine all the components and deploy the model as a RESTful API, making your application accessible worldwide.\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\n𝘊𝘩𝘦𝘤𝘬 𝘪𝘵 𝘰𝘶𝘵 & 𝘨𝘪𝘷𝘦 𝘪𝘵 𝘢 ⭐ ↓\n↳🔗 Hands-On LLMs course:\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Introduce the three main stages of training an LLM on proprietary data, focusing on pretraining for completion, supervised fine-tuning for dialogue, and reinforcement learning from human feedback for quality improvement.",
    "content": "You must know these 𝟯 𝗺𝗮𝗶𝗻 𝘀𝘁𝗮𝗴𝗲𝘀 of 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗮𝗻 𝗟𝗟𝗠 to train your own 𝗟𝗟𝗠 on your 𝗽𝗿𝗼𝗽𝗿𝗶𝗲𝘁𝗮𝗿𝘆 𝗱𝗮𝘁𝗮.\n# 𝗦𝘁𝗮𝗴𝗲 𝟭: 𝗣𝗿𝗲𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗳𝗼𝗿 𝗰𝗼𝗺𝗽𝗹𝗲𝘁𝗶𝗼𝗻\nYou start with a bear foot randomly initialized LLM.\nThis stage aims to teach the model to spit out tokens. More concretely, based on previous tokens, the model learns to predict the next token with the highest probability.\nFor example, your input to the model is \"The best programming language is ___\", and it will answer, \"The best programming language is Rust.\"\nIntuitively, at this stage, the LLM learns to speak.\n𝘋𝘢𝘵𝘢:  >1 trillion token (~= 15 million books). The data quality doesn't have to be great. Hence, you can scrape data from the internet.\n# 𝗦𝘁𝗮𝗴𝗲 𝟮: 𝗦𝘂𝗽𝗲𝗿𝘃𝗶𝘀𝗲𝗱 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴 (𝗦𝗙𝗧) 𝗳𝗼𝗿 𝗱𝗶𝗮𝗹𝗼𝗴𝘂𝗲\nYou start with the pretrained model from stage 1.\nThis stage aims to teach the model to respond to the user's questions.\nFor example, without this step, when prompting: \"What is the best programming language?\", it has a high probability of creating a series of questions such as: \"What is MLOps? What is MLE? etc.\"\nAs the model mimics the training data, you must fine-tune it on Q&A (questions & answers) data to align the model to respond to questions instead of predicting the following tokens.\nAfter the fine-tuning step, when prompted, \"What is the best programming language?\", it will respond, \"Rust\".\n𝘋𝘢𝘵𝘢: 10K - 100K Q&A example\n𝘕𝘰𝘵𝘦: After aligning the model to respond to questions, you can further single-task fine-tune the model, on Q&A data, on a specific use case to specialize the LLM.\n# 𝗦𝘁𝗮𝗴𝗲 𝟯: 𝗥𝗲𝗶𝗻𝗳𝗼𝗿𝗰𝗲𝗺𝗲𝗻𝘁 𝗹𝗲𝗮𝗿𝗻𝗶𝗻𝗴 𝗳𝗿𝗼𝗺 𝗵𝘂𝗺𝗮𝗻 𝗳𝗲𝗲𝗱𝗯𝗮𝗰𝗸 (𝗥𝗟𝗛𝗙)\nDemonstration data tells the model what kind of responses to give but doesn't tell the model how good or bad a response is.\nThe goal is to align your model with user feedback (what users liked or didn't like) to increase the probability of generating answers that users find helpful.\n𝘙𝘓𝘏𝘍 𝘪𝘴 𝘴𝘱𝘭𝘪𝘵 𝘪𝘯 2:\n1. Using the LLM from stage 2, train a reward model to act as a scoring function using (prompt, winning_response, losing_response) samples (= comparison data). The model will learn to maximize the difference between these 2. After training, this model outputs rewards for (prompt, response) tuples.\n𝘋𝘢𝘵𝘢: 100K - 1M comparisons\n2. Use an RL algorithm (e.g., PPO) to fine-tune the LLM from stage 2. Here, you will use the reward model trained above to give a score for every: (prompt, response). The RL algorithm will align the LLM to generate prompts with higher rewards, increasing the probability of generating responses that users liked.\n𝘋𝘢𝘵𝘢: 10K - 100K prompts\n.\nNote: Post inspired by Chip Huyen's \"RLHF: Reinforcement Learning from Human Feedback\" article: 🔗\nhttps://lnkd.in/dRTFHeFZ\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"
  },
  {
    "instruction": "Summarize the 8 essential categories of MLOps tools every MLOps engineer should be familiar with, emphasizing the importance of mastering tools from each category for success.",
    "content": "These are the 𝟴 𝘁𝘆𝗽𝗲𝘀 of 𝗠𝗟𝗢𝗽𝘀 𝘁𝗼𝗼𝗹𝘀 that must be in your toolbelt to be a 𝘀𝘂𝗰𝗰𝗲𝘀𝘀𝗳𝘂𝗹 𝗠𝗟𝗢𝗽𝘀 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿 ↓\nIf you are into MLOps, you are aware of the 1000+ tools in the space and think you have to know.\nThe reality is that all of these tools can be boiled down to 8 main categories.\nIf you learn the fundamentals and master one tool from each category, you will be fine.\n.\nBaşak Tuğçe Eskili\nand\nMaria Vechtomova\nfrom\nMarvelous MLOps\nwrote an excellent summary highlighting these 8 categories:\n1. 𝙑𝙚𝙧𝙨𝙞𝙤𝙣 𝙘𝙤𝙣𝙩𝙧𝙤𝙡: crucial for the traceability and reproducibility of an ML model deployment or run. Without a version control system, it is difficult to find out what exact code version was responsible for specific runs or errors you might have in production. (🔧 GitHub, GitLab, etc.)\n2. 𝘾𝙄/𝘾𝘿: automated tests are triggered upon pull request creation & deployment to production should only occur through the CD pipeline (🔧 GitHub Actions, GitLab CI/CD, Jenkins, etc.)\n3. 𝙒𝙤𝙧𝙠𝙛𝙡𝙤𝙬 𝙤𝙧𝙘𝙝𝙚𝙨𝙩𝙧𝙖𝙩𝙞𝙤𝙣: manage complex dependencies between different tasks, such as data preprocessing, feature engineering, ML model training (🔧  Airflow, ZenML, AWS Step Functions, etc.)\n4. 𝙈𝙤𝙙𝙚𝙡 𝙧𝙚𝙜𝙞𝙨𝙩𝙧𝙮: store, version, and share trained ML model artifacts, together with additional metadata (🔧  Comet ML, W&B, MLFlow, etc.)\n5. 𝘿𝙤𝙘𝙠𝙚𝙧 𝙧𝙚𝙜𝙞𝙨𝙩𝙧𝙮: store, version, and share Docker images. Basically, all your code will be wrapped up in Docker images and shared through this registry (🔧 Docker Hub, ECR, etc.)\n6 & 7. 𝙈𝙤𝙙𝙚𝙡 𝙩𝙧𝙖𝙞𝙣𝙞𝙣𝙜 & 𝙨𝙚𝙧𝙫𝙞𝙣𝙜 𝙞𝙣𝙛𝙧𝙖𝙨𝙩𝙧𝙪𝙘𝙩𝙪𝙧𝙚: if on-premise, you will likely have to go with Kubernetes. There are multiple choices if you are on a cloud provider: Azure ML on Azure, Sagemaker on AWS, and Vertex AI on GCP.\n8. 𝙈𝙤𝙣𝙞𝙩𝙤𝙧𝙞𝙣𝙜: Monitoring in ML systems goes beyond what is needed for monitoring regular software applications. The distinction lies in that the model predictions can fail even if all typical health metrics appear in good condition. (🔧  SageMaker, NannyML, Arize, etc.)\nThe secret sauce in MLOps is knowing how to glue all these pieces together while keeping things simple.\n.\n↳ To read more about these components, check out the\nMarvelous MLOps\narticle: 🔗\nhttps://lnkd.in/dUHRBzDt\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Outline the steps for implementing a streaming pipeline to populate a vector database for a financial assistant using LLMs, detailing the process from data extraction to embedding and database insertion.",
    "content": "This is 𝗵𝗼𝘄 you can 𝗶𝗺𝗽𝗹𝗲𝗺𝗲𝗻𝘁 a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 to populate a 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕 to do 𝗥𝗔𝗚 for a 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝘁 powered by 𝗟𝗟𝗠𝘀.\nIn a previous post, I covered 𝘄𝗵𝘆 you need a streaming pipeline over a batch pipeline when implementing RAG.\nNow, we will focus on the 𝗵𝗼𝘄, aka implementation details ↓\n🐝 All the following steps are wrapped in\nBytewax\nfunctions and connected in a single streaming pipeline.\n𝗘𝘅𝘁𝗿𝗮𝗰𝘁 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗻𝗲𝘄𝘀 𝗳𝗿𝗼𝗺 𝗔𝗹𝗽𝗮𝗰𝗮\nYou need 2 types of inputs:\n1. A WebSocket API to listen to financial news in real time. This will be used to listen 24/7 for new data and ingest it as soon as it is available.\n2. A RESTful API to ingest historical data in batch mode. When you deploy a fresh vector DB, you must populate it with data between a given range [date_start; date_end].\nYou wrap the ingested HTML document and its metadata in a `pydantic` NewsArticle model to validate its schema.\nRegardless of the input type, the ingested data is the same. Thus, the following steps are the same for both data inputs ↓\n𝗣𝗮𝗿𝘀𝗲 𝘁𝗵𝗲 𝗛𝗧𝗠𝗟 𝗰𝗼𝗻𝘁𝗲𝗻𝘁\nAs the ingested financial news is in HTML, you must extract the text from particular HTML tags.\n`unstructured` makes it as easy as calling `partition_html(document)`, which will recursively return the text within all essential HTML tags.\nThe parsed NewsArticle model is mapped into another `pydantic` model to validate its new schema.\nThe elements of the news article are the headline, summary and full content.\n𝗖𝗹𝗲𝗮𝗻 𝘁𝗵𝗲 𝘁𝗲𝘅𝘁\nNow we have a bunch of text that has to be cleaned. Again, `unstructured` makes things easy. Calling a few functions we clean:\n- the dashes & bullets\n- extra whitespace & trailing punctuation\n- non ascii chars\n- invalid quotes\nFinally, we standardize everything to lowercase.\n𝗖𝗵𝘂𝗻𝗸 𝘁𝗵𝗲 𝘁𝗲𝘅𝘁\nAs the text can exceed the context window of the embedding model, we have to chunk it.\nYet again, `unstructured` provides a valuable function that splits the text based on the tokenized text and expected input length of the embedding model.\nThis strategy is naive, as it doesn't consider the text's structure, such as chapters, paragraphs, etc. As the news is short, this is not an issue, but LangChain provides a `RecursiveCharacterTextSplitter` class that does that if required.\n𝗘𝗺𝗯𝗲𝗱 𝘁𝗵𝗲 𝗰𝗵𝘂𝗻𝗸𝘀\nYou pass all the chunks through an encoder-only model.\nWe have used `all-MiniLM-L6-v2` from `sentence-transformers`, a small model that can run on a CPU and outputs a 384 embedding.\nBut based on the size and complexity of your data, you might need more complex and bigger models.\n𝗟𝗼𝗮𝗱 𝘁𝗵𝗲 𝗱𝗮𝘁𝗮 𝗶𝗻 𝘁𝗵𝗲 𝗤𝗱𝗿𝗮𝗻𝘁 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕\nFinally, you insert the embedded chunks and their metadata into the\nQdrant\nvector DB.\nThe metadata contains the embedded text, the source_url and the publish date.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
  },
  {
    "instruction": "Announce the achievement of reaching 1400 subscribers for the \"Decoding ML\" newsletter and preview an upcoming series of articles based on the Hands-on LLMs course.",
    "content": "My 𝗻𝗲𝘄𝘀𝗹𝗲𝘁𝘁𝗲𝗿 just 𝗽𝗮𝘀𝘀𝗲𝗱 1400 subscribers 🎉🎉🎉  If you are into 𝗟𝗟𝗠𝘀, 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕𝘀, and 𝗠𝗟𝗢𝗽𝘀, you will like the 𝗳𝘂𝘁𝘂𝗿𝗲 𝘀𝗲𝗿𝗶𝗲𝘀 I am about to 𝘀𝘁𝗮𝗿𝘁 ↓\n.\n✌️ 𝗙𝗶𝗿𝘀𝘁, I want to thank everybody who reads my newsletter: \"Decoding ML.\" As it is completely free, your engagement is the only thing that motivates me.\n.\n🔥 𝗦𝗲𝗰𝗼𝗻𝗱𝗹𝘆, **𝘦𝘹𝘤𝘪𝘵𝘦𝘥 𝘷𝘰𝘪𝘤𝘦** here are my plans for the \"Decoding ML\" newsletter\nUntil now, the weekly articles were randomly picked from various 𝗠𝗟𝗘, 𝗠𝗟𝗢𝗽𝘀, 𝗗𝗟, and 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝘃𝗲 𝗔𝗜 𝘁𝗼𝗽𝗶𝗰𝘀.\n𝗦𝘁𝗮𝗿𝘁𝗶𝗻𝗴 𝘁𝗵𝗶𝘀 𝘄𝗲𝗲𝗸, based on my 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 𝗰𝗼𝘂𝗿𝘀𝗲, I will 𝗹𝗮𝘂𝗻𝗰𝗵 a 𝘀𝗲𝗿𝗶𝗲𝘀 of 𝘀𝗵𝗼𝗿𝘁 𝗮𝗿𝘁𝗶𝗰𝗹𝗲𝘀 that will teach you how to 𝗱𝗲𝘀𝗶𝗴𝗻, 𝗯𝘂𝗶𝗹𝗱, and 𝗱𝗲𝗽𝗹𝗼𝘆 an 𝗲𝗻𝗱-𝘁𝗼-𝗲𝗻𝗱 𝗟𝗟𝗠 𝘀𝘆𝘀𝘁𝗲𝗺 for a 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝘁.\nIt will cover topics such as:\n- the 3-pipeline / FTI architecture\n- building your own QA dataset\n- fine-tuning an LLM using QLoRA\n- building a streaming pipeline\n- using a vector DB for RAG\n- gluing everything together using LangChain\n- deploying the solution\nThis is not the course itself. It is just an overview of the most essential aspects.\nBut, if you are too busy to take the whole course, these weekly FREE 5-minute lessons are a great way to learn how to build an end-to-end LLM product seamlessly.\n.\n👀 𝗦𝘂𝗯𝘀𝗰𝗿𝗶𝗯𝗲 to start receiving them in your mail ↓\n↳🔗\nhttps://lnkd.in/dsMR4ivA\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n----\n💡 Follow me for daily lessons about ML engineering and MLOps"
  },
  {
    "instruction": "Discuss the necessity of a streaming pipeline over a batch pipeline for implementing Retrieval Augmented Generation (RAG) in LLM applications, with a focus on real-time data feeding for financial assistant examples.",
    "content": "𝗪𝗵𝘆 do you need a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗶𝗻𝘀𝘁𝗲𝗮𝗱 of a 𝗯𝗮𝘁𝗰𝗵 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 when implementing 𝗥𝗔𝗚 in your 𝗟𝗟𝗠 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀? 𝗪𝗵𝗮𝘁 do you need to 𝗶𝗺𝗽𝗹𝗲𝗺𝗲𝗻𝘁 a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 for a financial assistant?\n↳ 𝗪𝗵𝘆 𝗱𝗼 𝘆𝗼𝘂 𝗻𝗲𝗲𝗱 𝘁𝗼 𝗯𝘂𝗶𝗹𝗱 𝗮 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗶𝗻𝘀𝘁𝗲𝗮𝗱 𝗼𝗳 𝗮 𝗯𝗮𝘁𝗰𝗵 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲?\nThe quality of your RAG implementation is as good as the quality & freshness of your data.\nThus, depending on your use case, you have to ask:\n\"How fresh does my data from the vector DB have to be to provide accurate answers?\"\nBut for the best user experience, the data has to be as fresh as possible, aka real-time data.\nFor example, when implementing a financial assistant, being aware of the latest financial news is critical. A new piece of information can completely change the course of your strategy.\nHence, when implementing RAG, one critical aspect is to have your vector DB synced with all your external data sources in real time.\nA batch pipeline will work if your use case accepts a particular delay (e.g., one hour, one day, etc.).\nBut with tools like Bytewax 🐝, building streaming applications becomes much more accessible. So why not aim for the best?\n↳ 𝗪𝗵𝗮𝘁 𝗱𝗼 𝘆𝗼𝘂 𝗻𝗲𝗲𝗱 𝘁𝗼 𝗯𝘂𝗶𝗹𝗱 𝗮 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗳𝗼𝗿 𝗮 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝘁?\n- A financial news data source exposed through a web socket (e.g., Alpaca)\n- A Python streaming processing framework. For example,\nBytewax\n🐝 is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need the Java ecosystem to implement real-time pipelines anymore.  ↳🔗\nhttps://lnkd.in/dWJytkZ5\n- A Python package to process, clean, and chunk documents. `unstructured` offers a rich set of features that makes parsing HTML documents extremely convenient.\n- An encoder-only language model that maps your chunked documents into embeddings. `setence-transformers` is well integrated with HuggingFace and has a huge list of models of various sizes.\n- A vector DB, where to insert your embeddings and their metadata (e.g., the embedded text, the source_url, the creation date, etc.). For example,\nQdrant\nprovides a rich set of features and a seamless experience.  ↳🔗\nhttps://lnkd.in/d_FA9Bb3\n- A way to deploy your streaming pipeline. Docker + AWS will never disappoint you.\n- A CI/CD pipeline for continuous tests & deployments. GitHub Actions is a great serverless option with a rich ecosystem.\nThis is what you need to build & deploy a streaming pipeline solely in Python 🔥\n.\nWhat is your experience with batch vs. streaming pipelines? Let me know in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps"
  },
  {
    "instruction": "Describe how to implement real-time monitoring and metrics in ML systems, detailing the scenarios where ground truth data is available in various timelines and the steps to build a monitoring pipeline.",
    "content": "𝗛𝗼𝘄 𝘁𝗼 𝗮𝗱𝗱 𝗿𝗲𝗮𝗹-𝘁𝗶𝗺𝗲 𝗺𝗼𝗻𝗶𝘁𝗼𝗿𝗶𝗻𝗴 𝗮𝗻𝗱 𝗺𝗲𝘁𝗿𝗶𝗰𝘀 to your ML system.\nYour model is exposed to performance degradation after it is deployed to production.\nThat is why you need to monitor it constantly.\nThe most common way to monitor an ML model is to compute its metrics.\nBut for that, you need the ground truth.\n𝗜𝗻 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻, 𝘆𝗼𝘂 𝗰𝗮𝗻 𝗮𝘂𝘁𝗼𝗺𝗮𝘁𝗶𝗰𝗮𝗹𝗹𝘆 𝗮𝗰𝗰𝗲𝘀𝘀 𝘁𝗵𝗲 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵 𝗶𝗻 𝟯 𝗺𝗮𝗶𝗻 𝘀𝗰𝗲𝗻𝗮𝗿𝗶𝗼𝘀:\n1. near real-time: you can access it quite quickly\n2. delayed: you can access it after a considerable amount of time (e.g., one month)\n3. never: you have to label the data manually\n.\n𝗙𝗼𝗿 𝘂𝘀𝗲 𝗰𝗮𝘀𝗲𝘀 𝟮. 𝗮𝗻𝗱 𝟯. 𝘆𝗼𝘂 𝗰𝗮𝗻 𝗾𝘂𝗶𝗰𝗸𝗹𝘆 𝗰𝗼𝗺𝗽𝘂𝘁𝗲 𝘆𝗼𝘂𝗿 𝗺𝗼𝗻𝗶𝘁𝗼𝗿𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗶𝗻 𝘁𝗵𝗲 𝗳𝗼𝗹𝗹𝗼𝘄𝗶𝗻𝗴 𝘄𝗮𝘆:\n- store the model predictions and GT as soon as they are available (these 2 will be out of sync -> you can't compute the metrics right away)\n- build a DAG (e.g., using Airflow) that extracts the predictions & GT computes the metrics in batch mode and loads them into another storage (e.g., GCS)\n- use an orchestration tool to run the DAG in the following scenarios:\n1. scheduled: if the GT is available in near real-time (e.g., hourly), then it makes sense to run your monitoring pipeline based on the known frequency\n2. triggered: if the GT is delayed and you don't know when it may come up, then you can implement a webhook to trigger your monitoring pipeline\n- attach a consumer to your storage to use and display the metrics (e.g., trigger alarms and display them in a dashboard)\n.\nIf you want to see how to implement a near real-time monitoring pipeline using Airflow and GCS, check out my article:\n↳🔗 𝘌𝘯𝘴𝘶𝘳𝘪𝘯𝘨 𝘛𝘳𝘶𝘴𝘵𝘸𝘰𝘳𝘵𝘩𝘺 𝘔𝘓 𝘚𝘺𝘴𝘵𝘦𝘮𝘴 𝘞𝘪𝘵𝘩 𝘋𝘢𝘵𝘢 𝘝𝘢𝘭𝘪𝘥𝘢𝘵𝘪𝘰𝘯 𝘢𝘯𝘥 𝘙𝘦𝘢𝘭-𝘛𝘪𝘮𝘦 𝘔𝘰𝘯𝘪𝘵𝘰𝘳𝘪𝘯𝘨:\nhttps://lnkd.in/dhqCrGkD\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Highlight the top 6 ML Platform features crucial for developing and maintaining ML systems, emphasizing their role in ensuring reproducibility, collaboration, and operational efficiency.",
    "content": "𝗧𝗼𝗽 𝟲 𝗠𝗟 𝗣𝗹𝗮𝘁𝗳𝗼𝗿𝗺 𝗳𝗲𝗮𝘁𝘂𝗿𝗲𝘀 you must know and use in your ML system.\nHere they are ↓\n#𝟭. 𝗘𝘅𝗽𝗲𝗿𝗶𝗺𝗲𝗻𝘁 𝗧𝗿𝗮𝗰𝗸𝗶𝗻𝗴\nIn your ML development phase, you generate lots of experiments.\nTracking and comparing the metrics between them is crucial in finding the optimal model.\n#𝟮. 𝗠𝗲𝘁𝗮𝗱𝗮𝘁𝗮 𝗦𝘁𝗼𝗿𝗲\nIts primary purpose is reproducibility.\nTo know how a model was generated, you need to know:\n- the version of the code\n- the version of the packages\n- hyperparameters/config\n- total compute\n- version of the dataset\n... and more\n#𝟯. 𝗩𝗶𝘀𝘂𝗮𝗹𝗶𝘀𝗮𝘁𝗶𝗼𝗻𝘀\nMost of the time, along with the metrics, you must log a set of visualizations for your experiment.\nSuch as:\n- images\n- videos\n- prompts\n- t-SNE graphs\n- 3D point clouds\n... and more\n#𝟰. 𝗥𝗲𝗽𝗼𝗿𝘁𝘀\nYou don't work in a vacuum.\nYou have to present your work to other colleges or clients.\nA report lets you take the metadata and visualizations from your experiment...\n...and create, deliver and share a targeted presentation for your clients or peers.\n#𝟱. 𝗔𝗿𝘁𝗶𝗳𝗮𝗰𝘁𝘀\nThe most powerful feature out of them all.\nAn artifact is a versioned object that is an input or output for your task.\nEverything can be an artifact, but the most common cases are:\n- data\n- model\n- code\nWrapping your assets around an artifact ensures reproducibility.\nFor example, you wrap your features into an artifact (e.g., features:3.1.2), which you can consume into your ML development step.\nThe ML development step will generate config (e.g., config:1.2.4) and code (e.g., code:1.0.2) artifacts used in the continuous training pipeline.\nDoing so lets you quickly respond to questions such as \"What I used to generate the model?\" and \"What Version?\"\n#𝟲. 𝗠𝗼𝗱𝗲𝗹 𝗥𝗲𝗴𝗶𝘀𝘁𝗿𝘆\nThe model registry is the ultimate way to make your model accessible to your production ecosystem.\nFor example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry (e.g., model:1.2.4).\nYou label this model as \"staging\" under a new version and prepare it for testing. If the tests pass, mark it as \"production\" under a new version and prepare it for deployment (e.g., model:2.1.5).\n.\nAll of these features are used in a mature ML system. What is your favorite one?\nYou can see all these features in action in my 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 FREE course. Link in the comments ↓\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Explain the concept of chaining prompts to improve the cost-efficiency, accuracy, and debuggability of LLM applications, using analogies to illustrate the benefits of modular prompt design.",
    "content": "This is what you need to know about 𝗰𝗵𝗮𝗶𝗻𝗶𝗻𝗴 𝗽𝗿𝗼𝗺𝗽𝘁𝘀 to 𝗿𝗲𝗱𝘂𝗰𝗲 𝗰𝗼𝘀𝘁𝘀, 𝗶𝗻𝗰𝗿𝗲𝗮𝘀𝗲 𝗮𝗰𝗰𝘂𝗿𝗮𝗰𝘆, 𝗮𝗻𝗱 𝗲𝗮𝘀𝗶𝗹𝘆 𝗱𝗲𝗯𝘂𝗴 your 𝗟𝗟𝗠 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀.\nHere it is ↓\n𝗖𝗵𝗮𝗶𝗻𝗶𝗻𝗴 𝗽𝗿𝗼𝗺𝗽𝘁𝘀 is an intuitive technique that states that you must split your prompts into multiple calls.\n𝗪𝗵𝘆? 𝗟𝗲𝘁'𝘀 𝘂𝗻𝗱𝗲𝗿𝘀𝘁𝗮𝗻𝗱 𝘁𝗵𝗶𝘀 𝘄𝗶𝘁𝗵 𝘀𝗼𝗺𝗲 𝗮𝗻𝗮𝗹𝗼𝗴𝗶𝗲𝘀.\nWhen cooking, you are following a recipe split into multiple steps. You want to move to the next step only when you know what you have done so far is correct.\n↳ You want every prompt to be simple & focused.\nAnother analogy is between reading all the code in one monolith/god class and using DRY to separate the logic between multiple modules.\n↳ You want to understand & debug every prompt easily.\n.\nChaining prompts is a 𝗽𝗼𝘄𝗲𝗿𝗳𝘂𝗹 𝘁𝗼𝗼𝗹 𝗳𝗼𝗿 𝗯𝘂𝗶𝗹𝗱𝗶𝗻𝗴 𝗮 𝘀𝘁𝗮𝘁𝗲𝗳𝘂𝗹 𝘀𝘆𝘀𝘁𝗲𝗺 where you must take different actions depending on the current state.\nIn other words, you control what happens between 2 chained prompts.\n𝘉𝘺𝘱𝘳𝘰𝘥𝘶𝘤𝘵𝘴 𝘰𝘧 𝘤𝘩𝘢𝘪𝘯𝘪𝘯𝘨 𝘱𝘳𝘰𝘮𝘱𝘵𝘴:\n- increase in accuracy\n- reduce the number of tokens -> lower costs (skips steps of the workflow when not needed)\n- avoid context limitations\n- easier to include a human-in-the-loop -> easier to control, moderate, test & debug\n- use external tools/plugins (web search, API, databases, calculator, etc.)\n.\n𝗘𝘅𝗮𝗺𝗽𝗹𝗲\nYou want to build a virtual assistant to respond to customer service queries.\nInstead of adding in one single prompt the system message, all the available products, and the user inquiry, you can split it into the following:\n1. Use a prompt to extract the products and categories of interest.\n2. Enrich the context only with the products of interest.\n3. Call the LLM for the final answer.\nYou can evolve this example by adding another prompt that classifies the nature of the user inquiry. Based on that, redirect it to billing, technical support, account management, or a general LLM (similar to the complex system of GPT-4).\n.\n𝗧𝗼 𝘀𝘂𝗺𝗺𝗮𝗿𝗶𝘇𝗲:\nInstead of writing a giant prompt that includes multiple steps:\nSplit the god prompt into multiple modular prompts that let you keep track of the state externally and orchestrate the program.\nIn other words, you want modular prompts that you can combine easily (same as in writing standard functions/classes)\n.\nTo 𝗮𝘃𝗼𝗶𝗱 𝗼𝘃𝗲𝗿𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴, use this technique when your prompt contains >= instruction.\nYou can leverage the DRY principle from software -> one prompt = one instruction.\n↳ Tools to chain prompts: LangChain\n↳ Tools to monitor and debug prompts: Comet LLMOps Tools\nLinks in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\ndeeplearning\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Promote the Hands-on LLMs course, emphasizing its comprehensive approach to building production-ready applications with LLMs and detailing the key components students will learn to develop and deploy.",
    "content": "Want to 𝘀𝘁𝗮𝗿𝘁 𝗹𝗲𝗮𝗿𝗻𝗶𝗻𝗴 to 𝗯𝘂𝗶𝗹𝗱 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻-𝗿𝗲𝗮𝗱𝘆 applications using 𝗟𝗟𝗠𝘀? Then, I want to let you know that Pau and I 𝘀𝘁𝗮𝗿𝘁𝗲𝗱 𝗿𝗲𝗹𝗲𝗮𝘀𝗶𝗻𝗴 𝘁𝗵𝗲 𝘃𝗶𝗱𝗲𝗼 𝗹𝗲𝗰𝘁𝘂𝗿𝗲𝘀 for the FREE 𝗛𝗮𝗻𝗱𝘀-𝗢𝗻 𝗟𝗟𝗠𝘀 𝗰𝗼𝘂𝗿𝘀𝗲.\nThe Hands-On LLMs course is not just another demo of how to make a few predictions in a notebook.\nYou'll walk away with a 𝗳𝘂𝗹𝗹𝘆 𝗼𝗽𝗲𝗿𝗮𝘁𝗶𝗼𝗻𝗮𝗹 𝗽𝗿𝗼𝗱𝘂𝗰𝘁, leveraging Large Language Models (LLMs) to build a chatbot for financial investment advice.\n=== 𝗪𝗵𝗮𝘁 𝗬𝗼𝘂'𝗹𝗹 𝗕𝘂𝗶𝗹𝗱 ===\nWithin the course, you will leverage the 𝟯-𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗮𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲, as follows:\n𝟭. 𝗙𝗲𝗮𝘁𝘂𝗿𝗲 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll create a system to ingest real-time financial news—crucial for up-to-date advice.\n𝟮. 𝗧𝗿𝗮𝗱𝗶𝗻𝗴 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll fine-tune an LLM to specialize the model in making financial decisions.\n𝟯. 𝗜𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll combine all the components and deploy the model as a RESTful API, making your application accessible worldwide.\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\nCheck it out ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Discuss the advantages of Retrieval Augmented Generation (RAG) in LLM-powered applications, including overcoming limitations such as outdated knowledge, hallucinations, and lack of source references.",
    "content": "𝗥𝗔𝗚: 𝘄𝗵𝗮𝘁 problems does it solve, and 𝗵𝗼𝘄 it's integrated into 𝗟𝗟𝗠-𝗽𝗼𝘄𝗲𝗿𝗲𝗱 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀?\nLet's find out ↓\nRAG is a popular strategy when building LLMs to add external data to your prompt.\n=== 𝗣𝗿𝗼𝗯𝗹𝗲𝗺 ===\nWorking with LLMs has 3 main issues:\n1. The world moves fast\nAn LLM learns an internal knowledge base. However, the issue is that its knowledge is limited to its training dataset.\nThe world moves fast. New data flows on the internet every second. Thus, the model's knowledge base can quickly become obsolete.\nOne solution is to fine-tune the model every minute or day...\nIf you have some billions to spend around, go for it.\n2. Hallucinations\nAn LLM  is full of testosterone and likes to be blindly confident.\nEven if the answer looks 100% legit, you can never fully trust it.\n3. Lack of reference links\nIt is hard to trust the response of the LLM if we can't see the source of its decisions.\nEspecially for important decisions (e.g., health, financials)\n=== 𝗦𝗼𝗹𝘂𝘁𝗶𝗼𝗻 ===\n→ Surprize! It is RAG.\n1. Avoid fine-tuning\nUsing RAG, you use the LLM as a reasoning engine and the external knowledge base as the main memory (e.g., vector DB).\nThe memory is volatile, so you can quickly introduce or remove data.\n2. Avoid hallucinations\nBy forcing the LLM to answer solely based on the given context, the LLM will provide an answer as follows:\n-  use the external data to respond to the user's question if it contains the necessary insights\n- \"I don't know\" if not\n3. Add reference links\nUsing RAG, you can easily track the source of the data and highlight it to the user.\n=== 𝗛𝗼𝘄 𝗱𝗼𝗲𝘀 𝗥𝗔𝗚 𝘄𝗼𝗿𝗸? ===\nLet's say we want to use RAG to build a financial assistant.\n𝘞𝘩𝘢𝘵 𝘥𝘰 𝘸𝘦 𝘯𝘦𝘦𝘥?\n- a data source with historical and real-time financial news (e.g. Alpaca)\n- a stream processing engine (e.g., Bytewax - 🔗\nhttps://lnkd.in/dWJytkZ5\n)\n- an encoder-only model for embedding the documents (e.g., pick one from `sentence-transformers`)\n- a vector DB (e.g., Qdrant - 🔗\nhttps://lnkd.in/d_FA9Bb3\n)\n𝘏𝘰𝘸 𝘥𝘰𝘦𝘴 𝘪𝘵 𝘸𝘰𝘳𝘬?\n↳ On the feature pipeline side:\n1. using Bytewax, you ingest the financial news and clean them\n2. you chunk the news documents and embed them\n3. you insert the embedding of the docs along with their metadata (e.g., the initial text, source_url, etc.) to Qdrant\n↳ On the inference pipeline side:\n4. the user question is embedded (using the same embedding model)\n5. using this embedding, you extract the top K most similar news documents from Qdrant\n6. along with the user question, you inject the necessary metadata from the extracted top K documents into the prompt template (e.g., the text of documents & its source_url)\n7. you pass the whole prompt to the LLM for the final answer\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"
  },
  {
    "instruction": "Introduce The Full Stack 7-Steps MLOps Framework course in a LinkedIN post, a comprehensive, free resource for learning MLE and MLOps with hands-on examples, covering the entire lifecycle of ML system development and deployment.",
    "content": "Want to learn 𝗠𝗟𝗘 & 𝗠𝗟𝗢𝗽𝘀 in a 𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲𝗱 𝘄𝗮𝘆, for 𝗳𝗿𝗲𝗲, and with 𝗵𝗮𝗻𝗱𝘀-𝗼𝗻 𝗲𝘅𝗮𝗺𝗽𝗹𝗲𝘀?\nThen you should check out my 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 FREE course.\nIn 𝟮.𝟱 𝗵𝗼𝘂𝗿𝘀 𝗼𝗳 𝗿𝗲𝗮𝗱𝗶𝗻𝗴 & 𝘃𝗶𝗱𝗲𝗼 𝗺𝗮𝘁𝗲𝗿𝗶𝗮𝗹𝘀, you will 𝗹𝗲𝗮𝗿𝗻 𝗵𝗼𝘄 𝘁𝗼:\n- design a batch-serving architecture\n- use Hopsworks as a feature store\n- design a feature engineering pipeline that reads data from an API\n- build a training pipeline with hyper-parameter tunning\n- use W&B as an ML Platform to track your experiments, models, and metadata\n- implement a batch prediction pipeline\n- use Poetry to build your own Python packages\n- deploy your own private PyPi server\n- orchestrate everything with Airflow\n- use the predictions to code a web app using FastAPI and Streamlit\n- use Docker to containerize your code\n- use Great Expectations to ensure data validation and integrity\n- monitor the performance of the predictions over time\n- deploy everything to GCP\n- build a CI/CD pipeline using GitHub Actions\n- trade-offs & future improvements discussion\n…where all the pieces are integrated into a single end-to-end ML system that forecasts hourly energy levels across Denmark.\n𝗬𝗼𝘂 𝗰𝗮𝗻 𝗮𝗰𝗰𝗲𝘀𝘀 𝘁𝗵𝗲 𝗰𝗼𝘂𝗿𝘀𝗲 𝗼𝗻:\n➝ 𝘔𝘦𝘥𝘪𝘶𝘮'𝘴 𝘛𝘋𝘚 𝘱𝘶𝘣𝘭𝘪𝘤𝘢𝘵𝘪𝘰𝘯: text tutorials + videos\n➝ 𝘎𝘪𝘵𝘏𝘶𝘣: open-source code + docs\nI published the course on Medium's TDS publication to make it accessible to as many people as people. Thus ↓\n... anyone can learn the fundamentals of MLE & MLOps.\nSo no more excuses. Just go and build your own project 🔥\nCheck it out ↓\n↳🔗 𝘛𝘩𝘦 𝘍𝘶𝘭𝘭 𝘚𝘵𝘢𝘤𝘬 7-𝘚𝘵𝘦𝘱𝘴 𝘔𝘓𝘖𝘱𝘴 𝘍𝘳𝘢𝘮𝘦𝘸𝘰𝘳𝘬:\nhttps://lnkd.in/daShNdjw\nhashtag\n#\nmachinelearn"
  },
  {
    "instruction": "Explain the 3-pipeline architecture for ML systems in A LinkedIN post, highlighting its benefits for structure, modularity, collaboration, and efficient transition from development to production in MLOps practices.",
    "content": "We all know how 𝗺𝗲𝘀𝘀𝘆 𝗠𝗟 𝘀𝘆𝘀𝘁𝗲𝗺𝘀 can get. That is where the 𝟯-𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗮𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲 𝗸𝗶𝗰𝗸𝘀 𝗶𝗻.\nThe 3-pipeline design is a way to bring structure & modularity to your ML system and improve your MLOps processes.\nThis is how ↓\n=== 𝗣𝗿𝗼𝗯𝗹𝗲𝗺 ===\nDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.\nIn 2022, only 54% of the models get into production. Auch.\nSo what happens?\nSometimes the model is not mature enough, sometimes there are some security risks, but most of the time...\n...the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.\nSo, good processes and a well-defined architecture are as crucial as good tools and models.\n=== 𝗦𝗼𝗹𝘂𝘁𝗶𝗼𝗻 ===\n𝘛𝘩𝘦 3-𝘱𝘪𝘱𝘦𝘭𝘪𝘯𝘦 𝘢𝘳𝘤𝘩𝘪𝘵𝘦𝘤𝘵𝘶𝘳𝘦.\nFirst, let's understand what the 3-pipeline design is.\nIt is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:\n1. the feature pipeline\n2. the training pipeline\n3. the inference pipeline\n...also known as the Feature/Training/Inference (FTI) architecture.\n.\n#𝟭. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store.\n#𝟮. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained models, which are stored and versioned inside a model registry.\n#𝟯. The inference pipeline takes a given version of the features and trained models and outputs the predictions to a client.\n.\nThis is why the 3-pipeline design is so beautiful:\n- it is intuitive\n- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components\n- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate\n- the ML system has been built with modularity in mind since the beginning\n- the 3 components can easily be divided between multiple teams (if necessary)\n- every component can use the best stack of technologies available for the job\n- every component can be deployed, scaled, and monitored independently\n- the feature pipeline can easily be either batch, streaming or both\nBut the most important benefit is that...\n...by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.\n.\nWhat do you think about the 3-pipeline architecture? Have you used it?\nIf you want to know more about the 3-pipeline design, I recommend this awesome article from\nHopsworks\n↓\n↳🔗 From MLOps to ML Systems with Feature/Training/Inference Pipelines:\nhttps://lnkd.in/dRnhHDdg\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about MLE and MLOps."
  },
  {
    "instruction": "For a LinkedIn update, highlight the collaboration with Alexandru Răzvanț in developing the Hands-on LLMs course, focusing on a financial assistant project. Invite feedback and discussions on GitHub, stressing the course's practical approach.",
    "content": "Want to learn how to 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗲 𝗮𝗻 𝗟𝗟𝗠, build a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲, use a 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕, build a 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗯𝗼𝘁 and 𝗱𝗲𝗽𝗹𝗼𝘆 𝗲𝘃𝗲𝗿𝘆𝘁𝗵𝗶𝗻𝗴 using serverless solutions?\nThen maybe you know that I,\nPau Labarta Bajo\nand\nAlexandru Răzvanț 👋\n(one of the best MLEs I know out there in the wild) are working on our 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 𝗳𝗿𝗲𝗲 𝗰𝗼𝘂𝗿𝘀𝗲. If not, now you know.\n→ The course will teach you how to build a 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝘁 𝗽𝗿𝗼𝗱𝘂𝗰𝘁 powered by 𝗟𝗟𝗠𝘀 leveraging the 𝗠𝗟𝗢𝗽𝘀 𝟯-𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗱𝗲𝘀𝗶𝗴𝗻.\nThe 𝗼𝘂𝘁𝗰𝗼𝗺𝗲 of the product is a 𝗱𝗲𝗽𝗹𝗼𝘆𝗲𝗱 𝗽𝗿𝗼𝗱𝘂𝗰𝘁 that you can already show off.\n... and not another Notebook.\n.\nAs the course is still a 𝘄𝗼𝗿𝗸 𝗶𝗻 𝗽𝗿𝗼𝗴𝗿𝗲𝘀𝘀, we want to 𝗸𝗲𝗲𝗽 𝘆𝗼𝘂 𝘂𝗽𝗱𝗮𝘁𝗲𝗱 on our progress ↓\n↳ Thus, we opened up the 𝗱𝗶𝘀𝗰𝘂𝘀𝘀𝗶𝗼𝗻 𝘁𝗮𝗯 under the course's GitHub Repository, where we will 𝗸𝗲𝗲𝗽 𝘆𝗼𝘂 𝘂𝗽𝗱𝗮𝘁𝗲𝗱 with everything is happening.\n.\nAlso, if you have any 𝗶𝗱𝗲𝗮𝘀, 𝘀𝘂𝗴𝗴𝗲𝘀𝘁𝗶𝗼𝗻𝘀, 𝗾𝘂𝗲𝘀𝘁𝗶𝗼𝗻𝘀 or want to 𝗰𝗵𝗮𝘁, we encourage you to 𝗰𝗿𝗲𝗮𝘁𝗲 𝗮 \"𝗻𝗲𝘄 𝗱𝗶𝘀𝗰𝘂𝘀𝘀𝗶𝗼𝗻\".\n↓ We want the course to fill your real needs ↓\n↳ Hence, if your suggestion fits well with our hands-on course direction, we will consider implementing it.\n.\nCheck it out and leave a ⭐ if you like what you see:\n↳🔗 Hands-on LLMs Course:\nhttps://lnkd.in/dKRmRgfZ\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Share insights on LinkedIn about prioritizing ML fundamentals over chasing the latest trends. Emphasize the value of a solid foundation for quickly adapting to new technologies in the ML and MLOps landscape.",
    "content": "If anyone told you that 𝗠𝗟 or 𝗠𝗟𝗢𝗽𝘀 is 𝗲𝗮𝘀𝘆, they were 𝗿𝗶𝗴𝗵𝘁.\nHere is a simple trick that I learned the hard way ↓\nIf you are in this domain, you already know that everything changes fast:\n- a new tool every month\n- a new model every week\n- a new project every day\nYou know what I did? I stopped caring about all these changes and switched my attention to the real gold.\nWhich is → \"𝗙𝗼𝗰𝘂𝘀 𝗼𝗻 𝘁𝗵𝗲 𝗳𝘂𝗻𝗱𝗮𝗺𝗲𝗻𝘁𝗮𝗹𝘀.\"\n.\nLet me explain ↓\nWhen you constantly chase the latest models (aka FOMO),  you will only have a shallow understanding of that new information (except if you are a genius or already deep into that niche).\nBut the joke's on you. In reality, most of what you think you need to know, you don't.\nSo you won't use what you learned and forget most of it after 1-2 months.\nWhat a waste of time, right?\n.\nBut...\nIf you master the fundamentals of the topic, you want to learn.\nFor example, for deep learning, you have to know:\n- how models are built\n- how they are trained\n- groundbreaking architectures (Resnet, UNet, Transformers, etc.)\n- parallel training\n- deploying a model, etc.\n...when in need (e.g., you just moved on to a new project), you can easily pick up the latest research.\nThus, after you have laid the foundation, it is straightforward to learn SoTA approaches when needed (if needed).\nMost importantly, what you learn will stick with you, and you will have the flexibility to jump from one project to another quickly.\n.\nI am also guilty. I used to FOMO into all kinds of topics until I was honest with myself and admitted I am no Leonardo Da Vinci.\nBut here is what I did and worked well:\n- building projects\n- replicating the implementations of famous papers\n- teaching the subject I want to learn\n... and most importantly, take my time to relax and internalize the information.\n.\nTo conclude:\n- learn ahead only the fundamentals\n- learn the latest trend only when needed\nWhat is your learning strategy? Let me know in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Introduce the Composite design pattern on LinkedIn as essential for ML engineers, particularly for structuring efficient ML pipelines. Explain its benefits in simplifying complex projects by combining processing steps.",
    "content": "This is my 𝗳𝗮𝘃𝗼𝗿𝗶𝘁𝗲 𝗱𝗲𝘀𝗶𝗴𝗻 𝗽𝗮𝘁𝘁𝗲𝗿𝗻 that you must know as an ML engineer.\nMost ML engineers completely ignore software design patterns, but let me explain why you should know this one for your machine learning projects ↓\nI am talking about Composite.\nThe Composite pattern is a structural design pattern that helps you compose objects in a tree-like structure.\nLet me explain by starting with the problem.\n𝗣𝗿𝗼𝗯𝗹𝗲𝗺\nLet's say that you want to build an ML pipeline that performs object detection + tracking.\nYou can easily divide it into smaller pipelines, such as:\n1. preprocessing\n2. training | inference\n3. postprocessing\nAlso, these 3 pipelines, in their turn, are split into smaller components.\nLet's say that to speed up the ML pipeline. You want to run everything in parallel if possible.\nThus, depending on the use case, it would be best to have a module to compose components sequentially or in parallel.\n❌ If you don't think this through, your code can quickly transform into spaghetti.\n𝗦𝗼𝗹𝘂𝘁𝗶𝗼𝗻\n✅ Now, the Composite design pattern kicks in.\n-> 𝘛𝘩𝘪𝘴 𝘪𝘴 𝘩𝘰𝘸 𝘺𝘰𝘶 𝘤𝘢𝘯 𝘪𝘮𝘱𝘭𝘦𝘮𝘦𝘯𝘵 𝘵𝘩𝘦 𝘔𝘓 𝘱𝘪𝘱𝘦𝘭𝘪𝘯𝘦 𝘢𝘣𝘰𝘷𝘦 𝘶𝘴𝘪𝘯𝘨 𝘵𝘩𝘦 𝘊𝘰𝘮𝘱𝘰𝘴𝘪𝘵𝘦 𝘱𝘢𝘵𝘵𝘦𝘳𝘯:\n1. Define a standard interface for all the transformations. Let's call it \"Transformation.\"\n2. We create an abstract class called \"AtomicTransformation\" that inherits the \"Transformation\" interface for an atomic transformation.\n3. We implement an abstract class called \"CompositeTransformation\" for running multiple transformations. This class inherits the \"Transformation\" interface but also inputs a list of \"Transformation\" objects as input.\n4. Depending on how you want to call a sequence of transformations, you can inherit the \"CompositeTransformation\" interface and implement classes for:\n- \"SequenceTransformations\"\n- \"ParallelTransformations,\"\n- \"DistributedTransformations,\" etc.\n5. Now, when you want to implement a granular transformation (e.g., normalize the image). You implement the \"AtomicTransformation\" interface.\n6. When you want to glue multiple transformations together, you leverage the \"CompositeTransformation\" classes.\n7. When you call a \"CompositeTransformation\" under the hood, it calls the list of \"Transformation\" objects until it hits an \"AtomicTransformation\" object which will do the actual transformation.\nNote that because both the \"AtomicTransformation\" and \"CompositeTransformation\" inherit the \"Transformation\" interface, you can use them interchangeably, like LEGOs.\nThat is powerful.\nThat is why we all love Sklearn and their \"Pipeline\" interface 🔥\n.\nIf you want to know how to apply other software design patterns in MLE, I left in the comments an interesting article ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\ndesignpattern\nhashtag\n#\nmlops\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Discuss on LinkedIn the importance of maintaining an updated vector database for RAG in LLM applications. Outline the components of a streaming pipeline essential for real-time updates, emphasizing the role of this pipeline in enhancing application performance.",
    "content": "To successfully use 𝗥𝗔𝗚 in your 𝗟𝗟𝗠 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀, your 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕 must constantly be updated with the latest data.\nHere is how you can implement a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 to keep your vector DB in sync with your datasets ↓\n.\n𝗥𝗔𝗚 is a popular strategy when building LLMs to add context to your prompt about your private datasets.\nLeveraging your domain data using RAG provides 2 significant benefits:\n- you don't need to fine-tune your model as often (or at all)\n- avoid hallucinations\n.\nOn the 𝗯𝗼𝘁 𝘀𝗶𝗱𝗲, to implement RAG, you have to:\n3. Embed the user's question using an embedding model (e.g., BERT). Use the embedding to query your vector DB and find the most similar vectors using a distance function (e.g., cos similarity).\n4. Get the top N closest vectors and their metadata.\n5. Attach the extracted top N vectors metadata + the chat history to the input prompt.\n6. Pass the prompt to the LLM.\n7. Insert the user question + assistant answer to the chat history.\n.\nBut the question is, 𝗵𝗼𝘄 do you 𝗸𝗲𝗲𝗽 𝘆𝗼𝘂𝗿 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕 𝘂𝗽 𝘁𝗼 𝗱𝗮𝘁𝗲 𝘄𝗶𝘁𝗵 𝘁𝗵𝗲 𝗹𝗮𝘁𝗲𝘀𝘁 𝗱𝗮𝘁𝗮?\n↳ You need a real-time streaming pipeline.\nHow do you implement it?\nYou need 2 components:\n↳ A streaming processing framework. For example, Bytewax is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need Java to implement real-time pipelines anymore.\n🔗 Bytewax:\nhttps://lnkd.in/dbJDDvKB\n↳ A vector DB. For example, Qdrant provides a rich set of features and a seamless experience.\n🔗 Qdrant:\nhttps://qdrant.tech/\n.\nHere is an example of how to implement a streaming pipeline for financial news ↓\n𝟭. Financial news data source (e.g., Alpaca):\nTo populate your vector DB, you need a historical API (e.g., RESTful API) to add data to your vector DB in batch mode between a desired [start_date, end_date] range. You can tweak the number of workers to parallelize this step as much as possible.\n→ You run this once in the beginning.\nYou need the data exposed under a web socket to ingest news in real time. So, you'll be able to listen to the news and ingest it in your vector DB as soon as they are available.\n→ Listens 24/7 for financial news.\n𝟮. Build the streaming pipeline using Bytewax:\nImplement 2 input connectors for the 2 different types of APIs: RESTful API & web socket.\nThe rest of the steps can be shared between both connectors ↓\n- Clean financial news documents.\n- Chunk the documents.\n- Embed the documents (e.g., using Bert).\n- Insert the embedded documents + their metadata to the vector DB (e.g., Qdrant).\n𝟯-𝟳. When the users ask a financial question, you can leverage RAG with an up-to-date vector DB to search for the latest news in the industry.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
  },
  {
    "instruction": "On LinkedIn, announce the expansion to Twitter (X) for sharing content, reflecting on the platform choice to reach a broader audience. Invite followers to connect on Twitter, highlighting the move as a step towards accessible content distribution.",
    "content": "After 1 year, I finally decided to 𝘀𝘁𝗮𝗿𝘁 𝗽𝗼𝘀𝘁𝗶𝗻𝗴 𝗼𝗻 𝗧𝘄𝗶𝘁𝘁𝗲𝗿 or, as others like to call it, 𝗫.\nI took this decision because everybody has a different way of reading and interacting with their socials.\n...and I want everyone to enjoy my content on their favorite platform.\nIt took me a while to make this decision as I was not a Twitter user, but despite what people say, I started using it lately and enjoyed it.\nThus... It just made sense to start posting there, but I must warn you I don't have any followers 👀\nI even bought that stu*** blue ticker to see that I am serious about this 😂\nSo...\nIf you like my content and you are a Twitter/X person ↓\nFollow me on Twitter/X:\n↳ 🔗\nhttps://lnkd.in/d5ad8YSD\n↳ handler: @𝗶𝘂𝘀𝘇𝘁𝗶𝗻𝗽𝗮𝘂𝗹\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience"
  },
  {
    "instruction": "Explain the integration of GitHub Actions and Docker for CI/CD pipelines in a LinkedIn post. Highlight their benefits for ML/MLOps projects, including automation, cost-effectiveness, and simplicity of setup.",
    "content": "This is how you can build a CI/CD pipeline using GitHub Actions and Docker in just a few lines of code.\nAs an ML/MLOps engineer, you should master serving models by building CI/CD pipelines.\nThe good news is that GitHub Actions + Docker simplifies building a CI/CD pipeline.\n.\n𝗪𝗵𝘆?\n- you can easily trigger jobs when merging various branches\n- the CI/CD jobs run on GitHub's VMs (free)\n- easy to implement: copy & paste pre-made templates + adding credentials\n.\n𝗙𝗼𝗿 𝗲𝘅𝗮𝗺𝗽𝗹𝗲, 𝘁𝗵𝗶𝘀 𝗶𝘀 𝗵𝗼𝘄 𝘆𝗼𝘂 𝗰𝗮𝗻 𝗯𝘂𝗶𝗹𝗱 𝗮 𝗖𝗜 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗶𝗻 𝟯 𝘀𝗶𝗺𝗽𝗹𝗲 𝘀𝘁𝗲𝗽𝘀:\n#1. The CI pipeline is triggered when you merge your new feature branch into the main branch.\n#2. You log into the Docker Registry (or any other compatible registry such as ECR).\n#3. You build the image. Run your tests (if you have any), and if the tests pass, you push the image into the registry.\n.\n𝗧𝗼 𝗶𝗺𝗽𝗹𝗲𝗺𝗲𝗻𝘁 𝘁𝗵𝗲𝗺 𝘂𝘀𝗶𝗻𝗴 𝗚𝗶𝘁𝗛𝘂𝗯 𝗔𝗰𝘁𝗶𝗼𝗻𝘀, 𝘆𝗼𝘂 𝗵𝗮𝘃𝗲 𝘁𝗼:\n- Dockerize your code\n- search \"CI Template GitHub Actions\" on Google\n- copy-paste the template\n- add your Docker Registry credentials\n...and bam... you are done.\nEasy right? The steps are similar when building your CD pipeline (deploying the new image to production).\nIf you want to see how I used GitHub Actions to build & deploy an ML system to GCP, check out my article from the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ninfrastructure\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "On LinkedIn, outline methods for evaluating LLM outputs, focusing on structured answers, using LLM as a classifier, and generating multiple answers for selection. Stress the importance of automating this process for efficiency.",
    "content": "Here are 3 techniques you must know to evaluate your LLMs quickly.\nManually testing the output of your LLMs is a tedious and painful process → you need to automate it.\nIn generative AI, most of the time, you cannot leverage standard metrics.\nThus, the real question is, how do you evaluate the outputs of an LLM?\nDepending on your problem, here is what you can do ↓\n#𝟭. 𝗦𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲𝗱 𝗮𝗻𝘀𝘄𝗲𝗿𝘀 - 𝘆𝗼𝘂 𝗸𝗻𝗼𝘄 𝗲𝘅𝗮𝗰𝘁𝗹𝘆 𝘄𝗵𝗮𝘁 𝘆𝗼𝘂 𝘄𝗮𝗻𝘁 𝘁𝗼 𝗴𝗲𝘁\nEven if you use an LLM to generate text, you can ask it to generate a response in a structured format (e.g., JSON) that can be parsed.\nYou know exactly what you want (e.g., a list of products extracted from the user's question).\nThus, you can easily compare the generated and ideal answers using classic approaches.\nFor example, when extracting the list of products from the user's input, you can do the following:\n- check if the LLM outputs a valid JSON structure\n- use a classic method to compare the generated and real answers\n#𝟮. 𝗡𝗼 \"𝗿𝗶𝗴𝗵𝘁\" 𝗮𝗻𝘀𝘄𝗲𝗿 (𝗲.𝗴., 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗻𝗴 𝗱𝗲𝘀𝗰𝗿𝗶𝗽𝘁𝗶𝗼𝗻𝘀, 𝘀𝘂𝗺𝗺𝗮𝗿𝗶𝗲𝘀, 𝗲𝘁𝗰.)\nWhen generating sentences, the LLM can use different styles, words, etc. Thus, traditional metrics (e.g., BLUE score) are too rigid to be useful.\nYou can leverage another LLM to test the output of our initial LLM. The trick is in what questions to ask.\nWhen testing LLMs, you won't have a big testing split size as you are used to. A set of 10-100 tricky examples usually do the job (it won't be costly).\nHere, we have another 2 sub scenarios:\n↳ 𝟮.𝟭 𝗪𝗵𝗲𝗻 𝘆𝗼𝘂 𝗱𝗼𝗻'𝘁 𝗵𝗮𝘃𝗲 𝗮𝗻 𝗶𝗱𝗲𝗮𝗹 𝗮𝗻𝘀𝘄𝗲𝗿 𝘁𝗼 𝗰𝗼𝗺𝗽𝗮𝗿𝗲 𝘁𝗵𝗲 𝗮𝗻𝘀𝘄𝗲𝗿 𝘁𝗼 (𝘆𝗼𝘂 𝗱𝗼𝗻'𝘁 𝗵𝗮𝘃𝗲 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵)\nYou don't have access to an expert to write an ideal answer for a given question to compare it to.\nBased on the initial prompt and generated answer, you can compile a set of questions and pass them to an LLM. Usually, these are Y/N questions that you can easily quantify and check the validity of the generated answer.\nThis is known as \"Rubric Evaluation\"\nFor example:\n\"\"\"\n- Is there any disagreement between the response and the context? (Y or N)\n- Count how many questions the user asked. (output a number)\n...\n\"\"\"\nThis strategy is intuitive, as you can ask the LLM any question you are interested in as long it can output a quantifiable answer (Y/N or a number).\n↳ 𝟮.𝟮. 𝗪𝗵𝗲𝗻 𝘆𝗼𝘂 𝗱𝗼 𝗵𝗮𝘃𝗲 𝗮𝗻 𝗶𝗱𝗲𝗮𝗹 𝗮𝗻𝘀𝘄𝗲𝗿 𝘁𝗼 𝗰𝗼𝗺𝗽𝗮𝗿𝗲 𝘁𝗵𝗲 𝗿𝗲𝘀𝗽𝗼𝗻𝘀𝗲 𝘁𝗼 (𝘆𝗼𝘂 𝗵𝗮𝘃𝗲 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵)\nWhen you have access to an answer manually created by a group of experts, things are easier.\nYou will use an LLM to compare the generated and ideal answers based on semantics, not structure.\nFor example:\n\"\"\"\n(A) The submitted answer is a subset of the expert answer and entirely consistent.\n...\n(E) The answers differ, but these differences don't matter.\n\"\"\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
  },
  {
    "instruction": "Celebrate reaching 10k+ followers on LinkedIn by expressing gratitude and a commitment to continue sharing valuable ML & MLOps content. Highlight the milestone as an unexpected achievement and thank the community for their support.",
    "content": "I wasn't expecting this, but somehow it happened.\nI recently hit 10k+ followers on LinkedIn.\nI will be honest with you guys. That was my goal for the end of 2023.\nBut I don't mind 😂 This was a great surprise for me.\nI have to thank you guys for following me  🙏\nThis motivates me to create more and better content to help you decode ML & MLOps concepts.\nHave a fantastic day, and see you tomorrow 🔥\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata"
  },
  {
    "instruction": "Discuss the top 6 ML Platform features that enhance a mature ML system in a LinkedIn post. Highlight experiment tracking, metadata storage, visualizations, reports, artifacts, and model registry as essential components.",
    "content": "𝗧𝗼𝗽 𝟲 𝗠𝗟 𝗣𝗹𝗮𝘁𝗳𝗼𝗿𝗺 𝗳𝗲𝗮𝘁𝘂𝗿𝗲𝘀 you must know and use in your ML system.\nHere they are ↓\n#𝟭. 𝗘𝘅𝗽𝗲𝗿𝗶𝗺𝗲𝗻𝘁 𝗧𝗿𝗮𝗰𝗸𝗶𝗻𝗴\nIn your ML development phase, you generate lots of experiments.\nTracking and comparing the metrics between them is crucial in finding the optimal model.\n#𝟮. 𝗠𝗲𝘁𝗮𝗱𝗮𝘁𝗮 𝗦𝘁𝗼𝗿𝗲\nIts primary purpose is reproducibility.\nTo know how a model was generated, you need to know:\n- the version of the code\n- the version of the packages\n- hyperparameters/config\n- total compute\n- version of the dataset\n... and more\n#𝟯. 𝗩𝗶𝘀𝘂𝗮𝗹𝗶𝘀𝗮𝘁𝗶𝗼𝗻𝘀\nMost of the time, along with the metrics, you must log a set of visualizations for your experiment.\nSuch as:\n- images\n- videos\n- prompts\n- t-SNE graphs\n- 3D point clouds\n... and more\n#𝟰. 𝗥𝗲𝗽𝗼𝗿𝘁𝘀\nYou don't work in a vacuum.\nYou have to present your work to other colleges or clients.\nA report lets you take the metadata and visualizations from your experiment...\n...and create, deliver and share a targeted presentation for your clients or peers.\n#𝟱. 𝗔𝗿𝘁𝗶𝗳𝗮𝗰𝘁𝘀\nThe most powerful feature out of them all.\nAn artifact is a versioned object that is an input or output for your task.\nEverything can be an artifact, but the most common cases are:\n- data\n- model\n- code\nWrapping your assets around an artifact ensures reproducibility.\nFor example, you wrap your features into an artifact (e.g., features:3.1.2), which you can consume into your ML development step.\nThe ML development step will generate config (e.g., config:1.2.4) and code (e.g., code:1.0.2) artifacts used in the continuous training pipeline.\nDoing so lets you quickly respond to questions such as \"What I used to generate the model?\" and \"What Version?\"\n#𝟲. 𝗠𝗼𝗱𝗲𝗹 𝗥𝗲𝗴𝗶𝘀𝘁𝗿𝘆\nThe model registry is the ultimate way to make your model accessible to your production ecosystem.\nFor example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry (e.g., model:1.2.4).\nYou label this model as \"staging\" under a new version and prepare it for testing. If the tests pass, mark it as \"production\" under a new version and prepare it for deployment (e.g., model:2.1.5).\n.\nAll of these features are used in a mature ML system. What is your favorite one?\nYou can see all these features in action in my 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 FREE course. Link in the comments ↓\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Introduce the capability of embedding spreadsheets in Streamlit applications using Mito in a LinkedIn post. Highlight its benefits for data exploration and analysis within Streamlit dashboards, simplifying data manipulation.",
    "content": "Here is how you can 𝗲𝗺𝗯𝗲𝗱 𝗮 𝘀𝗽𝗿𝗲𝗮𝗱𝘀𝗵𝗲𝗲𝘁 𝗱𝗶𝗿𝗲𝗰𝘁𝗹𝘆 𝗶𝗻𝘁𝗼 𝘆𝗼𝘂𝗿 𝗦𝘁𝗿𝗲𝗮𝗺𝗹𝗶𝘁 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻 with just 𝗮 𝗳𝗲𝘄 𝗹𝗶𝗻𝗲𝘀 𝗼𝗳 𝗰𝗼𝗱𝗲.\nFor sure, you heard about\nTry Mito\n.\n𝘐𝘧 𝘯𝘰𝘵, 𝘩𝘦𝘳𝘦 𝘪𝘴 𝘢 𝘲𝘶𝘪𝘤𝘬 𝘳𝘦𝘤𝘢𝘱:\n\"Mito is a way to incorporate an Excel-like component into your beloved notebooks to explore data without writing any Python code (it writes it for you based on your interactions).\"\n.\nRecently, they introduced the same experience, but this time for Streamlit.\nNow you can add a spreadsheet in your dashboard with just a few lines of code:\n\"\"\"\nfrom mitosheet.streamlit.v1 import spreadsheet\n# ... rest of your streamlit app\nspreadsheet()\n\"\"\"\n.\n𝗛𝗲𝗿𝗲 𝗶𝘀 𝘄𝗵𝗮𝘁 𝗶𝘁 𝗰𝗮𝗻 𝗱𝗼:\n- Import, clean, and transform datasets into a format required by the rest of the Streamlit app.\n- Do flexible data exploration and analysis.\n- Create Python scripts using Mito's code-gen capabilities.\n.\nAccess the docs in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\nhashtag\n#\ndata\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Announce the launch of a personal site dedicated to MLE & MLOps resources on LinkedIn. Invite feedback on the work-in-progress site, aiming to create a central hub for learning and sharing knowledge in the field.",
    "content": "Looking for a hub where to 𝗹𝗲𝗮𝗿𝗻 𝗮𝗯𝗼𝘂𝘁 𝗠𝗟 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴 𝗮𝗻𝗱 𝗠𝗟𝗢𝗽𝘀 𝗳𝗿𝗼𝗺 𝗿𝗲𝗮𝗹-𝘄𝗼𝗿𝗹𝗱 𝗲𝘅𝗽𝗲𝗿𝗶𝗲𝗻𝗰𝗲?\nThen, I want to let you know that I just launched my personal site, where I will constantly aggregate my:\n- courses\n- articles\n- talks\n...and more\n→ Sweet part: Everything will revolve around MLE & MLOps\nIt is still a work in progress...\nBut please check it out and let me know what you think ↓\nYour opinion is deeply appreciated 🙏\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Discuss the strategic advantage of deploying ML models using a batch architecture initially in a LinkedIn post. Emphasize the approach for quick value delivery and feedback collection, positioning it as a foundation for more complex deployment strategies.",
    "content": "Why serving an ML model using a batch architecture is so powerful?\nWhen you first start deploying your ML model, you want an initial end-to-end flow as fast as possible.\nDoing so lets you quickly provide value, get feedback, and even collect data.\n.\nBut here is the catch...\nSuccessfully serving an ML model is tricky as you need many iterations to optimize your model to work in real-time:\n- low latency\n- high throughput\nInitially, serving your model in batch mode is like a hack.\nBy storing the model's predictions in dedicated storage, you automatically move your model from offline mode to a real-time online model.\nThus, you no longer have to care for your model's latency and throughput. The consumer will directly load the predictions from the given storage.\n𝐓𝐡𝐞𝐬𝐞 𝐚𝐫𝐞 𝐭𝐡𝐞 𝐦𝐚𝐢𝐧 𝐬𝐭𝐞𝐩𝐬 𝐨𝐟 𝐚 𝐛𝐚𝐭𝐜𝐡 𝐚𝐫𝐜𝐡𝐢𝐭𝐞𝐜𝐭𝐮𝐫𝐞:\n- extracts raw data from a real data source\n- clean, validate, and aggregate the raw data within a feature pipeline\n- load the cleaned data into a feature store\n- experiment to find the best model + transformations using the data from the feature store\n- upload the best model from the training pipeline into the model registry\n- inside a batch prediction pipeline, use the best model from the model registry to compute the predictions\n- store the predictions in some storage\n- the consumer will download the predictions from the storage\n- repeat the whole process hourly, daily, weekly, etc. (it depends on your context)\n.\n𝘛𝘩𝘦 𝘮𝘢𝘪𝘯 𝘥𝘰𝘸𝘯𝘴𝘪𝘥𝘦 of deploying your model in batch mode is that the predictions will have a level of lag.\nFor example, in a recommender system, if you make your predictions daily, it won't capture a user's behavior in real-time, and it will update the predictions only at the end of the day.\nMoving to other architectures, such as request-response or streaming, will be natural after your system matures in batch mode.\n.\nSo remember, when you initially deploy your model, using a batch mode architecture will be your best shot for a good user experience.\nLet me know in the comments what your usual strategy to serve models is ↓\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Share on LinkedIn how diffusion models are trained by learning to predict noise added to images. Highlight the process's significance in differentiating between image information and noise, contributing to the model's ability to generate accurate images.",
    "content": "Your quick guide on how diffusion models learn to predict your favorite images.\n.\n𝗤𝘂𝗶𝗰𝗸 𝗿𝗲𝗺𝗶𝗻𝗱𝗲𝗿!\nA diffusion model takes a noisy image as input and outputs the noise level from the image.\nAt inference time, you take the input image and subtract the predicted noise from it.\nAlso, A diffusion model is parameterized by a timestamp T that reflects the diffusion process from T to 0.\nThus, for different timestamps, it predicts different levels of noise.\nWhen the timestamp is near T, the model expects noisier images.\nAs it approaches 0, the expected noise level in the image is reduced.\n.\n𝗛𝗲𝗿𝗲 𝗶𝘀 𝘁𝗵𝗲 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗹𝗼𝗼𝗽 👇\n1. Sample a training image from the dataset.\n1. Sample timestamp t from the interval [0, T], which determines the noise level.\n2. Sample the noise.\n3. Add the noise to the image based on the sample timestamp t.\n4. Pass it through the diffusion model, which predicts the noise from the image.\n5. Use an MSE loss to compare the predicted noise with the true one.\n6. Use backpropagation to update the model.\n6. Repeat!\nFollowing this training strategy, the model learns to differentiate between the actual information from an image (e.g., the features of a cat) and the noise.\n.\nTo summarize...\nTo train a diffusion model you:\n- add noise to an image based on timestamp t\n- the models learn to predict the noise from timestamp t\n- you use MSE as a loss to compare the real noise with the predicted noise\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\nstablediffusion\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "On LinkedIn, share a personal experience with Hopsworks while developing The Full Stack 7-Steps MLOps Framework. Praise its features, documentation, and the support received, underscoring the platform's contribution to the course's success.",
    "content": "Don't know what 𝗙𝗲𝗮𝘁𝘂𝗿𝗲 𝗦𝘁𝗼𝗿𝗲 to use?\nI recommend you check out\nHopsworks\n.\nI had an excellent time using it while implementing 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 free course.\nThey provide:\n- an intuitive Python package to interface with their platform\n- a robust documentation\n- all the features you need from a feature store\nThey are still a fast-growing company, so the tool is evolving, making it more robust & feature-rich.\nWhile developing the course, I didn't know the team or have any connections with them, but they were kind enough to approach me and send me a gift.\nI love the logo & branding on their items. 🔥\nI guess I have no other option but to drink this weekend 😂\nThank you,\nHopsworks\n!\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nfeaturestore\n-----\n💡 Follow me if you want to level up in designing ML systems using MLOps good practices."
  },
  {
    "instruction": "Describe integrating a real-time streaming data source into an ML system on LinkedIn. Detail the infrastructure and processes required, emphasizing the approach's scalability and efficiency in enhancing system capabilities.",
    "content": "𝗨𝗻𝗶𝗳𝘆 𝗯𝗮𝘁𝗰𝗵 𝗮𝗻𝗱 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗠𝗟 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲𝘀\nWhat happens if you want to introduce a real-time/streaming data source into your system?\nYou cry. Just kidding. It is a lot easier than it sounds.\nLet's get some context.\nUntil now, you used only a static data source to train your model & compute your features.\nBut you find out that your business wants to use real-time news feeds as features for your model.\n𝗪𝗵𝗮𝘁 𝗱𝗼 𝘆𝗼𝘂 𝗱𝗼?\nYou have to implement 2 𝘮𝘢𝘪𝘯 𝘱𝘪𝘱𝘦𝘭𝘪𝘯𝘦𝘴 𝘧𝘰𝘳 𝘺𝘰𝘶𝘳 𝘯𝘦𝘸 𝘴𝘵𝘳𝘦𝘢𝘮𝘪𝘯𝘨 𝘪𝘯𝘱𝘶𝘵 𝘴𝘰𝘶𝘳𝘤𝘦:\n#𝟭. One that will quickly transform the raw data into features and make them accessible into the feature store to be used by the production services.\n#𝟮. One that will store the raw data in the static raw data source (e.g., a warehouse) so it will be used later for experimentation and research.\nBefore ingesting into your system, the real-time data source might need an extra processing step to standardize and adapt the data format to your interface.\nA standard strategy for:\n#𝟭. Kafka as your streaming platform\n#𝟮. Flink/Kafka Streams as your streaming processing units\nFor step #2. most of the time, you will have access to out-of-the-box data connectors that quickly load the real-time data into your static data storage (e.g., from Kafka to an S3 bucket or Big Query data warehouse).\nTo conclude...\nTo add a streaming data source to your current infrastructure, you need the following:\n- Kafka\n- Flink/Kafka Streams\n- to move your streaming data source into your static one\n- to quickly compute features and load them into the feature store\nThus, it isn't hard—just a lot of infrastructure to set up.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\n💡 Follow me if you want to level up in designing ML systems using MLOps good practices."
  },
  {
    "instruction": "Reflect on participating in the Let's Talk AI podcast in a LinkedIn post. Share insights from the discussion on ML engineering and MLOps, highlighting topics like AI system design, job roles, and the use of Airflow in automation.",
    "content": "Are you into MLOps and ML Engineering?\nI was honored to pour my MLE & MLOps wisdom into the podcast 𝘓𝘦𝘵'𝘴 𝘛𝘢𝘭𝘬 𝘈𝘐 hosted.\nI had a great time talking with\nThomas Bustos\n, where he had some fantastic questions about:\n- building and engineering AI systems\n- finding your niche in AI\n- different ML job positions\n- Airflow for automating ML\n- deploying multiple versions and communicating effectively\n- explaining technical complexity to customers\n... and more\nIf this sounds like something you are interested in, check it out 👇\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\n💡 Follow me if you want to level up in designing ML systems using MLOps good practices."
  },
  {
    "instruction": "Discuss the importance of a model registry in decoupling experimental and production pipelines of ML systems on LinkedIn. Explain how it facilitates smooth transitions from development to deployment, enhancing system reliability.",
    "content": "𝗦𝘂𝗽𝗲𝗿𝗰𝗵𝗮𝗿𝗴𝗲 𝘆𝗼𝘂𝗿 𝗠𝗟 𝘀𝘆𝘀𝘁𝗲𝗺: 𝘂𝘀𝗲 𝗮 𝗺𝗼𝗱𝗲𝗹 𝗿𝗲𝗴𝗶𝘀𝘁𝗿𝘆\nA model registry is the holy grail of any production-ready ML system.\nThe model registry is the critical component that decouples your offline pipeline (experimental/research phase) from your production pipeline.\n𝗖𝗼𝗺𝗽𝘂𝘁𝗲 𝗢𝗳𝗳𝗹𝗶𝗻𝗲 𝗙𝗲𝗮𝘁𝘂𝗿𝗲𝘀\nUsually, when training your model, you use a static data source.\nUsing a feature engineering pipeline, you compute the necessary features used to train the model.\nThese features will be stored inside a features store.\nAfter processing your data, your training pipeline creates the training & testing splits and starts training the model.\nThe output of your training pipeline is the trained weights, also known as the model artifact.\n𝗛𝗲𝗿𝗲 𝗶𝘀 𝘄𝗵𝗲𝗿𝗲 𝘁𝗵𝗲 𝗺𝗼𝗱𝗲𝗹 𝗿𝗲𝗴𝗶𝘀𝘁𝗿𝘆 𝗸𝗶𝗰𝗸𝘀 𝗶𝗻 👇\nThis artifact will be pushed into the model registry under a new version that can easily be tracked.\nSince this point, the new model artifact version can be pulled by any serving strategy:\n#1. batch\n#2. request-response\n#3. streaming\nYour inference pipeline doesn’t care how the model artifact was generated. It just has to know what model to use and how to transform the data into features.\nNote that this strategy is independent of the type of model & hardware you use:\n- classic model (Sklearn, XGboost),\n- distributed system (Spark),\n- deep learning model (PyTorch)\nTo summarize...\nUsing a model registry is a simple and effective method to:\n-> detach your experimentation from your production environment,\nregardless of what framework or hardware you use.\nTo learn more, check out my practical & detailed example of how to use a model registry in my article: 𝘈 𝘎𝘶𝘪𝘥𝘦 𝘵𝘰 𝘉𝘶𝘪𝘭𝘥𝘪𝘯𝘨 𝘌𝘧𝘧𝘦𝘤𝘵𝘪𝘷𝘦 𝘛𝘳𝘢𝘪𝘯𝘪𝘯𝘨 𝘗𝘪𝘱𝘦𝘭𝘪𝘯𝘦𝘴 𝘧𝘰𝘳 𝘔𝘢𝘹𝘪𝘮𝘶𝘮 𝘙𝘦𝘴𝘶𝘭𝘵𝘴\nLink in the comments 👇\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\n💡 Follow me if you want to level up in designing ML systems using MLOps good practices."
  },
  {
    "instruction": "Express pride in Romania's contributions to mathematics in a LinkedIn post. Share personal reflections on the country's achievements, emphasizing the impact on national pride and personal inspiration.",
    "content": "I usually don’t care about someone's nationality, but damn, I am proud of my country Romania \uD83C\uDF89\\nhashtag\\n#\\nmathematics\\nhashtag\\n#\\nromania"
  },
  {
    "instruction": "On LinkedIn, share a strategy for managing information overload by describing a personalized 'second brain' system. Highlight the process from collection to storage, emphasizing the importance of organizing information for productivity and learning.",
    "content": "I never forget anything. Said no one but only your second brain.\nAfter 6+ months of refinement, this is my second brain strategy 👇\nTiago's Forte book inspired me, but I adapted his system to my needs.\n.\n#𝟬. 𝗖𝗼𝗹𝗹𝗲𝗰𝘁\nThis is where you are bombarded with information from all over the place.\n#𝟭. 𝗧𝗵𝗲 𝗚𝗿𝗮𝘃𝗲𝘆𝗮𝗿𝗱\nThis is where I save everything that looks interesting.\nI won't use 90% of what is here, but it satisfied my urge to save that \"cool article\" I saw on LinkedIn.\nTools: Mostly Browser Bookmarks, but I rarely use GitHub stars, Medium lists, etc.\n#𝟮. 𝗧𝗵𝗲 𝗕𝗼𝗮𝗿𝗱\nHere, I start converging the information and planning what to do next.\nTools: Notion\n#𝟯. 𝗧𝗵𝗲 𝗙𝗶𝗲𝗹𝗱\nHere is where I express myself through learning, coding, writing, etc.\nTools: whatever you need to express yourself.\n2 & 3 are iterative processes. Thus I often bounce between them until the information is distilled.\n#𝟰. 𝗧𝗵𝗲 𝗪𝗮𝗿𝗲𝗵𝗼𝘂𝘀𝗲\nHere is where I take the distilled information and write it down for cold storage.\nTools: Notion, Google Drive\n.\nWhen I want to search for a piece of information, I start from the Warehouse and go backward until I find what I need.\nAs a minimalist, I  kept my tools to a minimum. I primarily use only: Brave, Notion, and Google Drive.\nYou don't need 100+ tools to be productive. They just want to take your money from you.\nSo remember...\nYou have to:\n- collect\n- link\n- plan\n- distill\n- store\nWhat is your second brain strategy? Leave your thoughts in the comments.\nhashtag\n#\nproductivity\nhashtag\n#\nsecondbrain\nhashtag\n#\nmachinelearning\n-----\n💡 Follow me for weekly insights about designing ML systems."
  },
  {
    "instruction": "Invite LinkedIn followers to consider the implications of searching for oneself on ChatGPT. Spark a discussion on the role of generative AI in personal and professional discovery, questioning its potential in hiring and dating contexts.",
    "content": "Have you ever searched for yourself on ChatGPT?\nDo you think this is the next generation of hiring and dating?\nThis is what it returned after prompting: \"You are a private detective. Tell me everything you know about Paul Iusztin.\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nchatgpt\n-----\n💡 Follow me for weekly insights about designing ML systems."
  },
  {
    "instruction": "On LinkedIn, introduce the combination of FastAPI and Streamlit as essential tools for ML engineers. Discuss their synergy in enabling Python-centric development for deploying ML models across various architectures.",
    "content": "The Perfect DUO: FastAPI + Streamlit\n2 tools you should know as an ML Engineer\nHere are 2 reasons why FastAPI & Streamlit should be in your MLE stack 👇\n#𝟭. 𝗣𝘆𝘁𝗵𝗼𝗻, 𝗣𝘆𝘁𝗵𝗼𝗻, 𝗣𝘆𝘁𝗵𝗼𝗻!\nAs an MLE, Python is your magic wand.\nUsing FastAPI & Streamlit, you can build full-stack web apps using solely Python.\n#𝟮. 𝗘𝘅𝘁𝗿𝗲𝗺𝗲𝗹𝘆 𝗳𝗹𝗲𝘅𝗶𝗯𝗹𝗲\nUsing FastAPI & Streamlit, you can deploy an ML model in almost any scenario.\n<< 𝘉𝘢𝘵𝘤𝘩 >>\nExpose the predictions from any storage, such as S3 or Redis, using FastAPI as REST endpoints.\nVisualize the predictions using Streamlit by calling the FastAPI REST endpoints.\n<< 𝘙𝘦𝘲𝘶𝘦𝘴𝘵-𝘙𝘦𝘴𝘱𝘰𝘯𝘴𝘦 >>\nWrap your model using FastAPI and expose its functionalities as REST endpoints.\nYet again... visualize the predictions using Streamlit by calling the FastAPI REST endpoints.\n<< 𝘚𝘵𝘳𝘦𝘢𝘮 >>\nWrap your model using FastAPI and expose it as REST endpoints.\nBut this time, the REST endpoints will be called from a Flink or Kafka Streams microservice.\n.\nUsing this tech stack won't be the most optimal solution in 100% use cases,\n... but in most cases:\n- it will get the job done\n- you can quickly prototype almost any ML application.\n.\nSo remember...\nYou should learn FastAPI & Streamlit because:\n- Python all the way!\n- you can quickly deploy a model in almost any architecture scenario\nDo you use FastAPI & Streamlit?\nTo learn more, check out my Medium article 𝘍𝘢𝘴𝘵𝘈𝘗𝘐 𝘢𝘯𝘥 𝘚𝘵𝘳𝘦𝘢𝘮𝘭𝘪𝘵: 𝘛𝘩𝘦 𝘗𝘺𝘵𝘩𝘰𝘯 𝘋𝘶𝘰 𝘠𝘰𝘶 𝘔𝘶𝘴𝘵 𝘒𝘯𝘰𝘸 𝘈𝘣𝘰𝘶𝘵. Link in the comments 👇\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npython\n-----\n💡 Follow me if you want to level up in designing and productionizing ML systems."
  },
  {
    "instruction": "Craft a LinkedIn post sharing how to efficiently use GitHub Actions and Docker to create CI/CD pipelines for ML/MLOps projects. Highlight the ease of setup and the benefits of automation and scalability offered by these tools.",
    "content": "Writing your own ML models is history.\nThe true value is in your data, how you prepare it, and your computer power.\nTo demonstrate my statement. Here is how you can write a Python script to train your LLM at scale in under 5 minutes ↓\n#𝟭. Load your data in JSON format and convert it into a Hugging Dataset\n#𝟮. Use Huggingface to load the LLM and pass it to the SFTTrainer, along with the tokenizer and training & evaluation datasets.\n#𝟯. Wrap your training script with a serverless solution, such as Beam, which quickly lets you access a cluster of GPUs to train large models.\n🚨 As you can see, the secret ingredients are not the LLM but:\n- the amount of data\n- the quality of data\n- how you process the data\n- $$$ for compute power\n- the ability to scale the system\n.\n💡 My advice\n↳ If you don't plan to become an ML researcher, shift your focus from the latest models to your data and infrastructure.\n.\n𝗡𝗼𝘁𝗲: Integrating serverless services, such as Beam, makes the deployment of your training pipeline fast & seamless, leaving you to focus only on the last piece of the puzzle: your data.\n↳🔗 Check out Beam's docs to find out more:\nhttps://lnkd.in/dtu2MWSp\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Initiate a conversation on LinkedIn about the distinct roles within ML engineering, drawing from personal observations. Encourage sharing of experiences to explore how these roles intersect and contribute to the broader field of AI and ML.",
    "content": "𝗝𝗼𝗯 𝗿𝗼𝗹𝗲𝘀 tell you there is just 𝗼𝗻𝗲 𝘁𝘆𝗽𝗲 𝗼𝗳 𝗠𝗟 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴, but there are 𝗮𝗰𝘁𝘂𝗮𝗹𝗹𝘆 𝟯\nHere they are ↓\nThese are the 3 ML engineering personas I found while working with different teams in the industry:\n#𝟭. 𝗥𝗲𝘀𝗲𝗮𝗿𝗰𝗵𝗲𝗿𝘀 𝘂𝗻𝗱𝗲𝗿𝗰𝗼𝘃𝗲𝗿\nThey like to stay in touch with the latest papers, understand the architecture of models, optimize them, run experiments, etc.\nThey are great at picking the best models but not that great at writing clean code and scaling the solution.\n#𝟮. 𝗦𝗪𝗘 𝘂𝗻𝗱𝗲𝗿𝗰𝗼𝘃𝗲𝗿\nThey pretend they read papers but don't (maybe only when they have to). They are more concerned with writing modular code and data quality than the latest hot models. Usually, these are the \"data-centric\" people.\nThey are great at writing clean code & processing data at scale but lack deep mathematical skills to develop complex DL solutions.\n#𝟯. 𝗠𝗟𝗢𝗽𝘀 𝗳𝗿𝗲𝗮𝗸𝘀\nThey ultimately don't care about the latest research & hot models. They are more into the latest MLOps tools and building ML systems. They love to automate everything and use as many tools as possible.\nGreat at scaling the solution and building ML pipelines, but not great at running experiments & tweaking ML models. They love to treat the ML model as a black box.\n.\nI started as #1. , until I realized I hated it - now I am a mix of:\n→ #𝟭. 20%\n→ #𝟮. 40%\n→ #𝟯. 40%\nBut that doesn't mean one is better - these types are complementary.\nA great ML team should have at least one of each persona.\nWhat do you think? Did I get it right?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Share a comparison between ML development environments and continuous training setups on LinkedIn, focusing on their distinct purposes and designs. Highlight the strategic importance of each in the lifecycle of ML system deployment.",
    "content": "What is the 𝗱𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝗰𝗲 between your 𝗠𝗟 𝗱𝗲𝘃𝗲𝗹𝗼𝗽𝗺𝗲𝗻𝘁 and 𝗰𝗼𝗻𝘁𝗶𝗻𝘂𝗼𝘂𝘀 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗲𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁𝘀?\nThey might do the same thing, but their design is entirely different ↓\n𝗠𝗟 𝗗𝗲𝘃𝗲𝗹𝗼𝗽𝗺𝗲𝗻𝘁 𝗘𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁\nAt this point, your main goal is to ingest the raw and preprocessed data through versioned artifacts (or a feature store), analyze it & generate as many experiments as possible to find the best:\n- model\n- hyperparameters\n- augmentations\nBased on your business requirements, you must maximize some specific metrics, find the best latency-accuracy trade-offs, etc.\nYou will use an experiment tracker to compare all these experiments.\nAfter you settle on the best one, the output of your ML development environment will be:\n- a new version of the code\n- a new version of the configuration artifact\nHere is where the research happens. Thus, you need flexibility.\nThat is why we decouple it from the rest of the ML systems through artifacts (data, config, & code artifacts).\n𝗖𝗼𝗻𝘁𝗶𝗻𝘂𝗼𝘂𝘀 𝗧𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗘𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁\nHere is where you want to take the data, code, and config artifacts and:\n- train the model on all the required data\n- output a staging versioned model artifact\n- test the staging model artifact\n- if the test passes, label it as the new production model artifact\n- deploy it to the inference services\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub Actions):\n- builds a docker image from the code artifact (e.g., triggered manually or when a new artifact version is created)\n- start the training pipeline inside the docker container that pulls the feature and config artifacts and outputs the staging model artifact\n- manually look over the training report -> If everything went fine, manually trigger the testing pipeline\n- manually look over the testing report -> if everything worked fine (e.g., the model is better than the previous one), manually trigger the CD pipeline that deploys the new model to your inference services\nNote how the model registry quickly helps you to decouple all the components.\nAlso, because training and testing metrics are not always black & white, it is tough to 100% automate the CI/CD pipeline.\nThus, you need a human in the loop when deploying ML models.\nTo conclude...\nThe ML development environment is where you do your research to find better models:\n- 𝘪𝘯𝘱𝘶𝘵: data artifact\n- 𝘰𝘶𝘵𝘱𝘶𝘵: code & config artifacts\nThe continuous training environment is used to train & test the production model at scale:\n- 𝘪𝘯𝘱𝘶𝘵: data, code, config artifacts\n- 𝘰𝘶𝘵𝘱𝘶𝘵: model artifact\nThis is not a fixed solution, as ML systems are still an open question. I would love to see your opinion in the comments. ↓\n.\nBut if you want to see this strategy in action,\nCheck out my 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 FREE Course.\nLink in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning"
  },
  {
    "instruction": "Discuss the versatility of transforming data with a feature store in an ML production system on LinkedIn. Highlight three innovative approaches to data transformation that enhance model performance and system efficiency.",
    "content": "These are 3 ways you didn't know about how you can transform your data when using a feature store.\nA feature store helps you quickly solve the training serving skew issue by offering you a consistent way to transform your data into features between the training and inference pipelines.\nThe issue boils down to WHEN you do the transformation.\nWhen using a feature store, there are 3 main ways you can transform your data:\n𝟏. 𝐁𝐞𝐟𝐨𝐫𝐞 𝐬𝐭𝐨𝐫𝐢𝐧𝐠 𝐭𝐡𝐞 𝐝𝐚𝐭𝐚 𝐢𝐧 𝐭𝐡𝐞 𝐟𝐞𝐚𝐭𝐮𝐫𝐞 𝐬𝐭𝐨𝐫𝐞\nIn the feature engineering pipeline, you do everything: clean, validate, aggregate, reduce, and transform your data.\nEven if this is the most intuitive way of doing things, it is the worse.\n🟢 ultra-low latency\n🔴 hard to do EDA on transformed data\n🔴 store duplicated/redundant data\n𝟐. 𝐒𝐭𝐨𝐫𝐞 𝐭𝐡𝐞 𝐭𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧 𝐢𝐧 𝐲𝐨𝐮𝐫 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 𝐨𝐫 𝐦𝐨𝐝𝐞𝐥 𝐩𝐫𝐞-𝐩𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 𝐥𝐚𝐲𝐞𝐫𝐬\nIn the feature engineering pipeline, you perform only the cleaning, validation, aggregations, and reduction steps.\nLater, by incorporating all your transformations into your pipeline object or pre-processing layers, you automatically save them along your model.\nThus, you can input your cleaned data into your pipeline, and it will know how to handle it.\n🟢 store only cleaned data\n🟢 easily explore your data\n🔴 the transformations are done on the client\n𝟑. 𝐘𝐨𝐮 𝐚𝐭𝐭𝐚𝐜𝐡 𝐭𝐨 𝐞𝐯𝐞𝐫𝐲 𝐜𝐥𝐞𝐚𝐧𝐞𝐝 𝐝𝐚𝐭𝐚 𝐬𝐨𝐮𝐫𝐜𝐞 𝐚 𝐔𝐃𝐅 𝐭𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧\nThis is similar to solution 2., but instead of attaching the transformation directly to your model, you attached them as a UDF to the feature store.\nfeature = cleaned data source + UDF\nSo when you request a feature, the feature store will automatically trigger the UDF on a server and return it.\n🟢 store only cleaned data\n🟢 easily explore your data\n🟢 the transformations are done on the server\n🟢 scalable (using Spark)\n🔴 hard to implement\nAs a recap,\nThere are 3 ways you can perform your transformations to solve the train serving skew when using a feature store.\nWhat method do you think is the best?\n.\n↳ To see method #𝟮. in action\nCheck out my 𝘈 𝘎𝘶𝘪𝘥𝘦 𝘵𝘰 𝘉𝘶𝘪𝘭𝘥𝘪𝘯𝘨 𝘌𝘧𝘧𝘦𝘤𝘵𝘪𝘷𝘦 𝘛𝘳𝘢𝘪𝘯𝘪𝘯𝘨 𝘗𝘪𝘱𝘦𝘭𝘪𝘯𝘦𝘴 𝘧𝘰𝘳 𝘔𝘢𝘹𝘪𝘮𝘶𝘮 𝘙𝘦𝘴𝘶𝘭𝘵𝘴 article\nLink in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Outline a LinkedIn post explaining the method of chaining prompts to build a financial assistant using LLMs. Emphasize the systematic approach for enhancing reliability and user experience in LLM-powered applications.",
    "content": "𝟳 𝘀𝘁𝗲𝗽𝘀 on how to 𝗰𝗵𝗮𝗶𝗻 your 𝗽𝗿𝗼𝗺𝗽𝘁𝘀 to build a production-ready 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝘁 using 𝗟𝗟𝗠𝘀 ↓\nWhen building LLM applications, you frequently have to divide your application into multiple steps & prompts, which are known as \"chaining prompts\".\nHere are 7 standard steps when building a financial assistant using LLMs (or any other assistant) ↓\n𝗦𝘁𝗲𝗽 𝟭: Check if the user's question is safe using OpenAI's Moderation API\nIf the user's query is safe, move to 𝗦𝘁𝗲𝗽 𝟮 ↓\n𝗦𝘁𝗲𝗽 𝟮: Query your proprietary data (e.g., financial news) to enrich the prompt with fresh data & additional context.\nTo do so, you have to:\n- use an LM to embed the user's input\n- use the embedding to query your proprietary data stored in a vector DB\n𝘕𝘰𝘵𝘦: You must use the same LM model to embed:\n- the data that will be stored in the vector DB\n- the user's question used to query the vector DB\n𝗦𝘁𝗲𝗽 𝟯: Build the prompt using:\n- a predefined template\n- the user's question\n- extracted financial news as context\n- your conversation history as context\n𝗦𝘁𝗲𝗽 𝟰: Call the LLM\n𝗦𝘁𝗲𝗽 𝟱: Check if the assistant's answer is safe using the OpenAI's Moderation API.\nIf the assistant's answer is safe, move to 𝗦𝘁𝗲𝗽 𝟱 ↓\n𝗦𝘁𝗲𝗽 𝟲: Use an LLM to check if the final answer is satisfactory.\nTo do so, you build a prompt using the following:\n- a validation predefined template\n- the user's initial question\n- the assistants answer\nThe LLM has to give a \"yes\" or \"no\" answer.\nThus, if it answers \"yes,\" we show the final answer to the user. Otherwise, we will return a predefined response, such as:\n\"Sorry, we couldn't answer your question because we don't have enough information.\"\n𝗦𝘁𝗲𝗽 𝟳: Add the user's question and assistant's answer to a history cache. Which will be used to enrich the following prompts with the current conversation.\nJust to remind you, the assistant should support a conversation. Thus, it needs to know what happened in the previous questions.\n→ In practice, you usually keep only the latest N (question, answer) tuples or a conversation summary to keep your context length under control.\n.\n↳ If you want to see this strategy in action, check out our new FREE Hands-on LLMs course (work in progress) & give it a ⭐ to stay updated with its latest progress.\nLink in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\ndeeplearning\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "On LinkedIn, share strategies for ensuring the reliability and accuracy of LLM outputs. Discuss four methods that can be implemented to monitor and improve the quality of LLM responses, enhancing overall system performance.",
    "content": "Here are 4 ways to monitor and check the output prompts of any LLM to increase the reliability and accuracy of your system.\n#𝟭. 𝗢𝗽𝗲𝗻𝗔𝗣𝗜 𝗠𝗼𝗱𝗲𝗿𝗮𝘁𝗶𝗼𝗻 𝗔𝗣𝗜\nYou can check whether the LLM's answer is harmful with a simple API call. It classifies the prompt as hate, harassment, self-harm, sexual, and violence.\nYou don't want your LLM to become a bully without knowing it.\n#𝟮. 𝗟𝗟𝗠𝗢𝗽𝘀: 𝗠𝗼𝗻𝗶𝘁𝗼𝗿 𝘁𝗵𝗲 𝗽𝗿𝗼𝗺𝗽𝘁𝘀\nOne part of LLMOps is to monitor, track, and see the lineage of all the prompts that come into & out of your system.\nYou can easily do that with Comet ML's LLMOps features. Link in the comments ↓\n#𝟯. 𝗨𝘀𝗲 𝘁𝗵𝗲 𝘀𝗮𝗺𝗲 𝗟𝗟𝗠 𝘁𝗼 𝗰𝗹𝗮𝘀𝘀𝗶𝗳𝘆 𝘁𝗵𝗲 𝗼𝘂𝘁𝗽𝘂𝘁 𝗮𝘀 𝘀𝗮𝘁𝗶𝘀𝗳𝘆𝗶𝗻𝗴 𝗼𝗿 𝗻𝗼𝘁\nAlong with generating text, an LLM can also be used as a classifier (without additional training).\nAfter all, outputting a class can still be considered text generation, right?\nTo do so, you have to:\n- write a system prompt: \"You are an assistant that evaluates ... respond with \"Y\" if the output is sufficient and \"N\" otherwise.\n- add the user question\n- add the LLM answer\n- add the additional context used by the LLM to generate the answers (e.g., a set of product information)\n↳ concatenate everything and pass it to the same LLM...\n... and vualá, you've built a monitoring system that constantly classifies the LLM's answers between satisfying or not.\n#𝟰. 𝗚𝗲𝗻𝗲𝗿𝗮𝘁𝗲 𝗺𝗼𝗿𝗲 𝗮𝗻𝘀𝘄𝗲𝗿𝘀 𝗮𝗻𝗱 𝘂𝘀𝗲 𝘁𝗵𝗲 𝘀𝗮𝗺𝗲 𝗟𝗟𝗠 𝘁𝗼 𝗽𝗶𝗰𝗸 𝘁𝗵𝗲 𝗯𝗲𝘀𝘁 𝗮𝗻𝘀𝘄𝗲𝗿\nQuite self-explanatory.\nAnother option is letting the user pick the best option - a popular strategy for generating stuff.\nA big downside to this strategy is that it adds extra costs.\n.\nSo remember...\nThere are 4 ways to parse your LLM's outputs:\n1. use the OpenAI Moderation API\n2. log them to Comet ML\n3. build a Y/N satisfying classifier\n4. generate more options and pick the best\nHave you used any of these options? Let me know ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\n💡  Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Summarize key ML monitoring resources in a LinkedIn post, following an extensive review of articles and materials. Highlight three standout sources that provide deep insights and practical examples for effectively monitoring ML systems.",
    "content": "In the last month, I read 100+ ML monitoring articles.\nI trimmed them for you to 3 key resources:\n1. A series of excellent articles made by\nArize AI\nthat will make you understand what ML monitoring is all about.\n↳🔗\nhttps://lnkd.in/dDVWRujh\n2. The\nEvidently AI\nBlog, where you can find answers to all your questions regarding ML monitoring.\n↳🔗\nhttps://lnkd.in/du35hWp2\n3. The monitoring hands-on examples hosted by\nDataTalksClub\nwill teach you how to implement an ML monitoring system.\n↳🔗\nhttps://lnkd.in/d4ziHhxH\nAfter wasting a lot of time reading other resources...\nUsing these 3 resources is a solid start for learning about monitoring ML systems.\nHave you tried them?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Promote the adoption of Terraform for infrastructure management in ML projects on LinkedIn. Share personal savings and efficiencies gained from using Terraform, positioning it as a crucial tool for cost-effective and scalable ML deployments.",
    "content": "I was uselessly spending 1000$ dollars every month on cloud machines until I started using this tool 👇\nTerraform!\n.\n𝐅𝐢𝐫𝐬𝐭, 𝐥𝐞𝐭'𝐬 𝐮𝐧𝐝𝐞𝐫𝐬𝐭𝐚𝐧𝐝 𝐰𝐡𝐲 𝐰𝐞 𝐧𝐞𝐞𝐝 𝐓𝐞𝐫𝐫𝐚𝐟𝐨𝐫𝐦.\nWhen you want to deploy a software application, there are two main steps:\n1. Provisioning infrastructure\n2. Deploying applications\nA regular workflow would be that before deploying your applications or building your CI/CD pipelines, you manually go and spin up your, let's say, AWS machines.\nInitially, this workflow should be just fine, but there are two scenarios when it could get problematic.\n#1. Your infrastructure gets too big and complicated. Thus, it is cumbersome and might yield bugs in manually replicating it.\n#2. In the world of AI, there are many cases when you want to spin up a GPU machine to train your models, and afterward, you don't need it anymore. Thus, if you forget to close it, you will end up uselessly paying a lot of $$$.\nWith Terraform, you can solve both of these issues.\n.\nSo...\n𝐖𝐡𝐚𝐭 𝐢𝐬 𝐓𝐞𝐫𝐫𝐚𝐟𝐨𝐫𝐦?\nIt sits on the provisioning infrastructure layer as a: \"infrastructure as code\" tool that:\n- is declarative (you focus on the WHAT, not on the HOW)\n- automates and manages your infrastructure\n- is open source\nYeah... yeah... that sounds fancy. But 𝐰𝐡𝐚𝐭 𝐜𝐚𝐧 𝐈 𝐝𝐨 𝐰𝐢𝐭𝐡 𝐢𝐭?\nLet's take AWS as an example, where you have to:\n- create a VPC\n- create AWS users and permissions\n- spin up EC2 machines\n- install programs (e.g., Docker)\n- create a K8s cluster\nUsing Terraform...\nYou can do all that just by providing a configuration file that reflects the state of your infrastructure.\nBasically, it helps you create all the infrastructure you need programmatically. Isn't that awesome?\n.\nIf you want to quickly understand Terraform enough to start using it in your own projects,\n↳ check out my 7-minute read article: 𝘚𝘵𝘰𝘱 𝘔𝘢𝘯𝘶𝘢𝘭𝘭𝘺 𝘊𝘳𝘦𝘢𝘵𝘪𝘯𝘨 𝘠𝘰𝘶𝘳 𝘈𝘞𝘚 𝘐𝘯𝘧𝘳𝘢𝘴𝘵𝘳𝘶𝘤𝘵𝘶𝘳𝘦. 𝘜𝘴𝘦 𝘛𝘦𝘳𝘳𝘢𝘧𝘰𝘳𝘮!\nLink from the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nsoftwareengineering\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  },
  {
    "instruction": "Introduce the concept of retry policies for enhancing code reliability in ML systems on LinkedIn. Highlight the use of the Tenacity Python package as a simple yet powerful solution for implementing customizable retry strategies.",
    "content": "One strategy that makes the 𝗱𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗯𝗲𝘁𝘄𝗲𝗲𝗻 𝗴𝗼𝗼𝗱 𝗰𝗼𝗱𝗲 𝗮𝗻𝗱 𝗴𝗿𝗲𝗮𝘁 𝗰𝗼𝗱𝗲 is adding 𝗿𝗲𝘁𝗿𝘆 𝗽𝗼𝗹𝗶𝗰𝗶𝗲𝘀.\nTo manually implement them can get tedious and complicated.\nRetry policies are a must when you:\n- make calls to an external API\n- read from a queue, etc.\n.\n𝗨𝘀𝗶𝗻𝗴 𝘁𝗵𝗲 𝗧𝗲𝗻𝗮𝗰𝗶𝘁𝘆 𝗣𝘆𝘁𝗵𝗼𝗻 𝗽𝗮𝗰𝗸𝗮𝗴𝗲...\n𝘠𝘰𝘶 𝘤𝘢𝘯 𝘲𝘶𝘪𝘤𝘬𝘭𝘺 𝘥𝘦𝘤𝘰𝘳𝘢𝘵𝘦 𝘺𝘰𝘶𝘳 𝘧𝘶𝘯𝘤𝘵𝘪𝘰𝘯𝘴 𝘢𝘯𝘥 𝘢𝘥𝘥 𝘤𝘶𝘴𝘵𝘰𝘮𝘪𝘻𝘢𝘣𝘭𝘦 𝘳𝘦𝘵𝘳𝘺 𝘱𝘰𝘭𝘪𝘤𝘪𝘦𝘴, 𝘴𝘶𝘤𝘩 𝘢𝘴:\n1. Add fixed and random wait times between multiple retries.\n2. Add a maximum number of attempts or computation time.\n3. Retry only when specific errors are thrown (or not thrown).\n... as you can see, you easily compose these policies between them.\nThe cherry on top is that you can access the statistics of the retries of a specific function:\n\"\nprint(raise_my_exception.retry.statistics)\n\"\n.\nWhat is your current strategy for adding retry policies to your Python code?\nhashtag\n#\nmachinelearning\nhashtag\n#\npython\nhashtag\n#\nsoftwareengineering\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps."
  }
]
